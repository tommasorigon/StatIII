<!DOCTYPE html>
<html lang="en"><head>
<script src="un_A_files/libs/clipboard/clipboard.min.js"></script>
<script src="un_A_files/libs/quarto-html/tabby.min.js"></script>
<script src="un_A_files/libs/quarto-html/popper.min.js"></script>
<script src="un_A_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="un_A_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="un_A_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="un_A_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <meta name="author" content="Tommaso Rigon">
  <title>Linear models and misspecification</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="un_A_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="un_A_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="un_A_files/libs/revealjs/dist/theme/quarto-8fd9a32a7a1f1c24c97cc70db42e1276.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">
  <link href="un_A_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="un_A_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="un_A_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="un_A_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Linear models and misspecification</h1>
  <p class="subtitle">Statistics III - CdL SSE</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<span class="orange">Tommaso Rigon</span> 
</div>
        <p class="quarto-title-affiliation">
            <em>Universit√† degli Studi di Milano-Bicocca</em>
          </p>
    </div>
</div>

</section>
<section id="homepage" class="slide level2 center">
<h2><a href="../index.html">Homepage</a></h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="img/ABC.png"> <em>‚ÄúEverything should be made as simple as possible, but not simpler‚Äù</em></p>
<p>Attributed to <span class="grey">Albert Einstein</span></p>
</div><div class="column" style="width:60%;">
<ul>
<li><p>This unit will cover the following <span class="orange">topics</span>:</p>
<ul>
<li><span class="blue">Recap</span>: linear models and the modeling process</li>
<li>Robustness of OLS estimates, sandwich estimators</li>
<li>Weighted least squares</li>
<li>Box-Cox transform, variance stabilizing transformations</li>
</ul></li>
<li><p>The main theme is: what should we do when the <span class="blue">assumptions</span> of linear models are <span class="orange">violated</span>?</p></li>
<li><p>We will push the linear model to its limit, using it even when is not supposed to work.</p></li>
<li><p>The symbol üìñ means that a few extra steps are discussed in the <span class="blue">handwritten notes</span>.</p></li>
</ul>
</div></div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The content of this Unit is covered in <span class="orange">Chapter 1</span> of <span class="citation" data-cites="Salvan2020">Salvan et al. (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span>. Alternatively, see <span class="blue">Chapter 2</span> of <span class="citation" data-cites="Agresti2015">Agresti (<a href="#/references" role="doc-biblioref" onclick="">2015</a>)</span> or <span class="grey">Chapter 5</span> of <span class="citation" data-cites="Azzalini2008">Azzalini (<a href="#/references" role="doc-biblioref" onclick="">2008</a>)</span>.</p>
</div>
</div>
</div>
</section>
<section>
<section id="the-modeling-process" class="title-slide slide level1 center">
<h1>The modeling process</h1>

</section>
<section id="car-data-diesel-or-gas" class="slide level2 center">
<h2>Car data (<span class="blue">diesel</span> or <span class="orange">gas</span>)</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="un_A_files/figure-revealjs/unnamed-chunk-2-1.png" width="540"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>We consider data for <span class="math inline">n = 203</span> models of cars in circulation in 1985 in the USA.</li>
<li>We want to <span class="blue">predict</span> the distance per unit of fuel as a function of the vehicle features.</li>
<li>We consider the following <span class="orange">variables</span>:
<ul>
<li>The city distance per unit of fuel (km/L, <code>city.distance</code>)</li>
<li>The engine size (L, <code>engine.size</code>)</li>
<li>The number of cylinders (<code>n.cylinders</code>)</li>
<li>The curb weight (kg, <code>curb.weight</code>)</li>
<li>The fuel type (gasoline or diesel, <code>fuel</code>).</li>
</ul></li>
</ul>
</div></div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>We assume you are already familiar with linear models. The following is a brief recap rather than a full discussion.</p>
</div>
</div>
</div>
</section>
<section id="linear-regression" class="slide level2 center">
<h2>Linear regression</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="un_A_files/figure-revealjs/unnamed-chunk-3-1.png" width="360"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li><p>Let us consider the variables <code>city.distance</code> (<span class="math inline">y</span>), <code>engine.size</code> (<span class="math inline">x</span>) and <code>fuel</code> (<span class="math inline">z</span>).</p></li>
<li><p>A <span class="blue">simple linear regression</span> <span class="math display">
Y_i = \beta_1 + \beta_2 x_i + \epsilon_i, \qquad i=1,\dots,n,
</span> could be easily fit by least squares‚Ä¶</p></li>
<li><p>‚Ä¶ but the plot suggests that the relationship between <code>city.distance</code> and <code>engine.size</code> is <span class="orange">not</span> well approximated by a <span class="orange">linear</span> function.</p></li>
<li><p>‚Ä¶ and also that <code>fuel</code> has a non-negligible effect on the response.</p></li>
</ul>
</div></div>
</section>
<section id="regression-models" class="slide level2 center">
<h2>Regression models</h2>
<ul>
<li class="fragment"><p>A <span class="orange">general</span> and <span class="orange">more flexible formulation</span> for modeling the relationship between a vector of <span class="blue">fixed covariates</span> <span class="math inline">\bm{x}_i = (x_{i1},\dots,x_{ip})^T \in \mathbb{R}^p</span> and a random variable <span class="math inline">Y_i \in \mathbb{R}</span> is <span class="math display">
Y_i = f(\bm{x}_i; \beta) + \epsilon_i, \qquad i=1,\dots,n,
</span> where the ‚Äúerrors‚Äù <span class="math inline">\epsilon_i</span> are iid random variables, having zero mean and variance <span class="math inline">\sigma^2</span>.</p></li>
<li class="fragment"><p>To estimate the unknown parameters <span class="math inline">\beta</span>, a possibility is to rely on the <span class="blue">least squares criterion</span>: we seek the <span class="orange">minimum</span> of the objective function <span class="math display">
D(\beta) = \sum_{i=1}^n\{y_i - f(\bm{x}_i; \beta)\}^2,
</span> using <span class="math inline">n</span> pairs of covariates <span class="math inline">\bm{x}_i = (x_{i1},\dots,x_{ip})^T</span> and the observed realizations <span class="math inline">y_i</span> of the random variables <span class="math inline">Y_i</span>, for <span class="math inline">i = 1,\dots,n</span>. The <span class="orange">optimal value</span> is denoted by <span class="math inline">\hat{\beta}</span>.</p></li>
<li class="fragment"><p>The <span class="blue">predicted values</span> are <span class="math inline">\hat{y}_i = \widehat{\mathbb{E}(Y_i)} = f(\bm{x}_i; \hat{\beta})</span>, for <span class="math inline">i=1,\dots,n.</span></p></li>
</ul>
</section>
<section id="linear-models" class="slide level2 center">
<h2>Linear models</h2>
<ul>
<li><p>Let us consider again the variables <code>city.distance</code> (<span class="math inline">y</span>), <code>engine.size</code> (<span class="math inline">x</span>) and <code>fuel</code> (<span class="math inline">z</span>).</p></li>
<li><p>Which function <span class="math inline">f(x,z;\beta)</span> should we choose?</p></li>
</ul>
<div class="fragment">
<ul>
<li>A first attempt is to consider a <span class="orange">polynomial term</span> combined with a <span class="blue">dummy variable</span> <span class="math display">
f(x, z; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \beta_4 x^3 + \beta_5 I(z = \texttt{gas}),
</span> which is a special instance of <span class="orange">linear model</span>.</li>
</ul>
</div>
<div class="fragment">
<div class="callout callout-note no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Linear model</strong></p>
</div>
<div class="callout-content">
<p>In a <span class="blue">linear model</span> the response variable <span class="math inline">Y_i</span> is related to the covariates through the function<span class="math display">
    \mathbb{E}(Y_i) =f(\bm{x}_i; \beta) = \beta_1 x_{i1} + \cdots + \beta_p x_{ip} =\bm{x}_i^T\beta,
    </span> where <span class="math inline">\bm{x}_i = (x_{i1},\dots,x_{ip})^T</span> is a vector of <span class="orange">covariates</span> and <span class="math inline">\beta = (\beta_1,\dots,\beta_p)^T</span> is the corresponding vector of <span class="orange">coefficients</span>.</p>
</div>
</div>
</div>
</div>
</section>
<section id="matrix-notation" class="slide level2 center">
<h2>Matrix notation</h2>
<ul>
<li><p>The <span class="orange">response random variables</span> are collected in the random vector <span class="math inline">\bm{Y} = (Y_1,\dots,Y_n)^T</span>, whose <span class="blue">observed realization</span> is <span class="math inline">\bm{y} = (y_1,\dots,y_n)^T</span>.</p></li>
<li><p>The <span class="blue">design matrix</span> is a <span class="math inline">n \times p</span> matrix, comprising the covariate‚Äôs values, defined by <span class="math display">
\bm{X} =
\begin{bmatrix}
x_{11} &amp; \cdots &amp; x_{1p}\\
\vdots &amp; \ddots &amp; \vdots \\
x_{n1} &amp; \cdots &amp; x_{np}
\end{bmatrix}.
</span></p></li>
</ul>
<div class="fragment">
<ul>
<li>The <span class="math inline">j</span>th variable (column) is denoted with <span class="math inline">\tilde{\bm{x}}_j</span>, whereas the <span class="math inline">i</span>th observation (row) is <span class="math inline">\bm{x}_i</span>: <span class="math display">
\bm{X} = (\tilde{\bm{x}}_1,\dots,\tilde{\bm{x}}_p) = (\bm{x}_1, \dots,\bm{x}_n)^T.
</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Then, a <span class="blue">linear model</span> can be written using the <span class="orange">compact notation</span>: <span class="math display">
\bm{Y} = \bm{X}\beta + \bm{\epsilon},
</span> where <span class="math inline">\bm{\epsilon} = (\epsilon_1,\dots,\epsilon_n)^T</span> is a vector of iid error terms with zero mean and variance <span class="math inline">\sigma^2</span>.</li>
</ul>
</div>
</section>
<section id="linear-regression-estimation-i" class="slide level2 center">
<h2>Linear regression: estimation I</h2>
<ul>
<li>The optimal set of coefficients <span class="math inline">\hat{\beta}</span> is the minimizer of the <span class="orange">least squared criterion</span> <span class="math display">
D(\beta) = (\bm{y} - \bm{X}\beta)^T(\bm{y} - \bm{X}\beta) = ||\bm{y} - \bm{X}\beta||^2,
</span> also known as <span class="orange">residual sum of squares (RSS)</span>, where <span class="math display">
||\bm{y}|| = \sqrt{y_1^2 + \cdots + y_n^2},</span> denotes the <span class="blue">Euclidean norm</span>.</li>
</ul>
<div class="fragment">
<div class="callout callout-note no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Least square estimate (OLS)</strong></p>
</div>
<div class="callout-content">
<p>If the design matrix has <span class="blue">full rank</span>, that is, if <span class="math inline">\text{rk}(\bm{X}^T\bm{X}) = p</span>, then the <span class="orange">least square estimate</span> has an explicit solution: <span class="math display">
    \hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T \bm{y}.
    </span></p>
</div>
</div>
</div>
</div>
</section>
<section id="linear-regression-estimation-ii" class="slide level2 center">
<h2>Linear regression: estimation II</h2>
<ul>
<li>In matrix notation, the predicted values can be obtained as <span class="math display">
\hat{\bm{y}} = \bm{X}\hat{\beta} = \bm{H}\bm{y}, \qquad \bm{H} = \bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T,
</span> where <span class="math inline">\bm{H}</span> is a <span class="math inline">n \times n</span> <span class="orange">projection matrix</span> matrix sometimes called <span class="blue">hat matrix</span>. The matrix is idempotent, meaning that <span class="math inline">\bm{H} = \bm{H}^T</span> and <span class="math inline">\bm{H}^2 = \bm{H}</span>.</li>
</ul>
<div class="fragment">
<ul>
<li><p>The quantity <span class="math inline">D(\hat{\beta})</span> is the so-called <span class="blue">deviance</span>, which is equal to <span class="math display">
D(\hat{\beta}) = ||\bm{y} - \hat{\bm{y}}||^2 = \bm{y}^T(I_n - \bm{H})\bm{y}.
</span></p></li>
<li><p>Moreover, a typical estimate for the <span class="orange">residual variance</span> <span class="math inline">\sigma^2</span> is obtained as follows: <span class="math display">
s^2 = \frac{D(\hat{\beta})}{n - p} = \frac{1}{n-p}\sum_{i=1}^n(y_i - \bm{x}_i^T\hat{\beta})^2.
</span></p></li>
<li><p>To evaluate the goodness of fit, we can calculate the <span class="orange">coefficient of determination</span>: <span class="math display">
R^2 = 1 - \frac{\textsf{(``Residual deviance'')}}{\textsf{(``Total deviance'')}} = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n(y_i - \bar{y})^2}.
</span></p></li>
</ul>
</div>
</section>
<section id="linear-regression-inference" class="slide level2 center">
<h2>Linear regression: inference</h2>
<ul>
<li><p>Recall that the errors <span class="math inline">\bm{\epsilon}</span> have zero mean <span class="math inline">\mathbb{E}(\bm{\epsilon}) = 0</span> and are <span class="orange">uncorrelated</span> <span class="math inline">\text{var}(\bm{\epsilon}) = \sigma^2I_n</span>.</p></li>
<li><p>Then, the estimator <span class="math inline">\hat{\beta}</span> is <span class="orange">unbiased</span> <span class="math inline">\mathbb{E}(\hat{\beta}) = \beta</span> and its <span class="blue">variance</span> is <span class="math inline">\text{var}(\hat{\beta}) = \sigma^2 (\bm{X}^T\bm{X})^{-1}</span>. Since <span class="math inline">\sigma^2</span> is also unknown, we can estimate the variances of <span class="math inline">\hat{\beta}</span> as follows: <span class="math display">
  \widehat{\text{var}}(\hat{\beta}) = s^2 (\bm{X}^T\bm{X})^{-1}.
  </span></p></li>
<li><p>The <span class="orange">standard errors</span> of the components of <span class="math inline">\hat{\beta}</span> correspond to the square root of the diagonal of the above covariance matrix.</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>Let us additionally assume that the errors follow a Gaussian distribution: <span class="math inline">\epsilon_i \overset{\text{iid}}{\sim} \text{N}(0, \sigma^2)</span>.</p></li>
<li><p>This implies that the <span class="orange">distribution</span> of the <span class="orange">estimator</span> <span class="math inline">\hat{\beta}</span> is <span class="math display">
\hat{\beta} \sim \text{N}_p(\beta, \sigma^2 (\bm{X}^T\bm{X})^{-1}).
</span></p></li>
<li><p>Confidence interval and Wald‚Äôs tests can be obtained through classical inferential theory.</p></li>
</ul>
</div>
</section>
<section id="linear-regression-diagnostic" class="slide level2 center">
<h2>Linear regression: diagnostic</h2>
<ul>
<li><p>The diagonal elements <span class="math inline">h_i \in [0, 1]</span> of the matrix <span class="math inline">\bm{H}</span> are called <span class="blue">leverages</span> and it holds <span class="math display">
\text{var}(\hat{Y}_i) = \sigma^2 h_i, \qquad \text{var}(Y_i - \hat{Y}_i) = \sigma^2 (1 - h_i), \qquad \text{cor}(Y_i, \hat{Y}_i) = \sqrt{h_i}.
</span> The leverage <span class="math inline">h_i</span> determines the <span class="orange">precision</span> with which <span class="math inline">\hat{Y}_i</span> predicts <span class="math inline">Y_i</span>. For large <span class="math inline">h_i</span> close to <span class="math inline">1</span>, <span class="math inline">\text{cor}(Y_i, \hat{Y}_i) \approx 1</span>, therefore changes of a single point <span class="math inline">Y_i</span> leads to significant changes in <span class="math inline">\hat{Y}_i</span>.</p></li>
<li><p>Leverages also appear in the definition of <span class="blue">standardized residuals</span>: <span class="math display">
\tilde{r}_i = \frac{r_i}{\sqrt{s^2(1 - h_i)}} = \frac{y_i - \bm{x}_i^T\hat{\beta}}{\sqrt{s^2(1 - h_i)}},
</span> where <span class="math inline">r_i = y_i - \bm{x}_i^T\hat{\beta}</span> are the (raw) <span class="orange">residuals</span>.</p></li>
</ul>
<div class="fragment">
<ul>
<li>An observation is <span class="blue">influent</span> if it has high leverage and high squared residual. <span class="orange">Cook‚Äôs distance</span> <span class="math inline">c_i</span> is based on the change in <span class="math inline">\hat{\beta}</span> when the observation is removed: <span class="math display">
p \cdot c_i = (\hat{\beta} - \hat{\beta}_{-i})^T\widehat{\text{var}}(\hat{\beta})^{-1}(\hat{\beta} - \hat{\beta}_{-i}) = \tilde{r}_i^2 \frac{h_i}{p(1 - h_i)}.
</span> Cook‚Äôs distance is considered relatively large when <span class="math inline">c_i \ge 1</span>.</li>
</ul>
</div>
</section>
<section id="leverages-outliers-and-influence-points" class="slide level2 center">
<h2>Leverages, outliers and influence points</h2>

<img data-src="un_A_files/figure-revealjs/unnamed-chunk-4-1.png" class="quarto-figure quarto-figure-center r-stretch" width="990"><ul>
<li><span class="orange">Left plot</span>: leverage, not outlier. <span class="blue">Central plot</span>: outlier, not leverage. <span class="grey">Right plot</span>: influence point = leverage + outlier.</li>
</ul>
</section>
<section id="a-first-model-estimated-coefficients" class="slide level2 center">
<h2>A first model: estimated coefficients</h2>
<ul>
<li><p>Our first attempt for predicting <code>city.distance</code> (<span class="math inline">y</span>) via <code>engine.size</code> (<span class="math inline">x</span>) and <code>fuel</code> (<span class="math inline">z</span>) is: <span class="math display">
  f(x, z; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \beta_4 x^3 + \beta_5 I(z = \texttt{gas}).
  </span></p></li>
<li><p>We obtain the following <span class="orange">summary</span> for the regression coefficients <span class="math inline">\hat{\beta}</span>.</p></li>
</ul>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>(Intercept)</code></td>
<td style="text-align: right;">28.045</td>
<td style="text-align: right;">3.076</td>
<td style="text-align: right;">9.119</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>engine.size</code></td>
<td style="text-align: right;">-10.980</td>
<td style="text-align: right;">3.531</td>
<td style="text-align: right;">-3.109</td>
<td style="text-align: right;">0.002</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>engine.size^2</code></td>
<td style="text-align: right;">2.098</td>
<td style="text-align: right;">1.271</td>
<td style="text-align: right;">1.651</td>
<td style="text-align: right;">0.100</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>engine.size^3</code></td>
<td style="text-align: right;">-0.131</td>
<td style="text-align: right;">0.139</td>
<td style="text-align: right;">-0.939</td>
<td style="text-align: right;">0.349</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>fuel_gas</code></td>
<td style="text-align: right;">-3.214</td>
<td style="text-align: right;">0.427</td>
<td style="text-align: right;">-7.523</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
<ul>
<li>Moreover, the coefficient <span class="math inline">R^2</span> and the residual standard deviation <span class="math inline">s</span> are:</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: right;">r.squared</th>
<th style="text-align: right;">sigma</th>
<th style="text-align: right;">deviance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.5973454</td>
<td style="text-align: right;">1.790362</td>
<td style="text-align: right;">634.6687</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="a-first-model-fitted-values" class="slide level2 center">
<h2>A first model: fitted values</h2>

<img data-src="un_A_files/figure-revealjs/unnamed-chunk-7-1.png" class="quarto-figure quarto-figure-center r-stretch" width="702"></section>
<section id="a-first-model-graphical-diagnostics" class="slide level2 center">
<h2>A first model: graphical diagnostics</h2>

<img data-src="un_A_files/figure-revealjs/unnamed-chunk-8-1.png" class="quarto-figure quarto-figure-center r-stretch" width="702"></section>
<section id="comments-and-criticisms" class="slide level2 center">
<h2>Comments and criticisms</h2>
<ul>
<li><p>Is this a good model?</p></li>
<li><p>The overall fit <span class="blue">seems satisfactory</span> at first glance, especially if we aim at predicting the urban distance of cars when average engine size (i.e., between <span class="math inline">1.5L</span> and <span class="math inline">3L</span>).</p></li>
</ul>
<div class="fragment">
<ul>
<li>However, the plot of the <span class="orange">residuals</span> <span class="math inline">r_i = y_i - \hat{y}_i</span> suggests that the homoschedasticity assumption, i.e.&nbsp;<span class="math inline">\text{var}(\epsilon_i) = \sigma^2</span>, might be violated.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>Also, this model is unsuitable for <span class="orange">extrapolation</span>. Indeed:</p>
<ul>
<li>It has no grounding in physics or engineering, leading to difficulties when interpreting the trend and to paradoxical situations.</li>
<li>For example, the curve of the set of gasoline cars shows a local minimum around <span class="math inline">4.6 L</span> and then rises again!</li>
</ul></li>
<li><p>It is plausible that we can find a better one, so what‚Äôs next?</p></li>
</ul>
</div>
</section>
<section id="linear-models-and-non-linear-patterns" class="slide level2 center">
<h2>Linear models and non-linear patterns</h2>
<ul>
<li>A significant advantage of linear models is that they can describe non-linear relationships via <span class="blue">variable transformations</span> such as polynomials, logarithms, etc.</li>
</ul>
<div class="fragment">
<ul>
<li>This gives the statistician a lot of modeling flexibility. For instance, we could let: <span class="math display">
\log{Y_i} = \beta_1 + \beta_2 \log{x_i} + \beta_3 I(z_i = \texttt{gas}) + \epsilon_i, \qquad i=1,\dots,n.
</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>This specification is <span class="orange">linear in the parameters</span>, it fixes the domain issues, and it imposes a monotone relationship between engine size and consumption.</li>
</ul>
</div>
<div class="fragment">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>(Intercept)</code></td>
<td style="text-align: right;">3.060</td>
<td style="text-align: right;">0.047</td>
<td style="text-align: right;">64.865</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>log(engine.size)</code></td>
<td style="text-align: right;">-0.682</td>
<td style="text-align: right;">0.040</td>
<td style="text-align: right;">-17.129</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>fuel_gas</code></td>
<td style="text-align: right;">-0.278</td>
<td style="text-align: right;">0.038</td>
<td style="text-align: right;">-7.344</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="second-model-fitted-values" class="slide level2 center">
<h2>Second model: fitted values</h2>

<img data-src="un_A_files/figure-revealjs/unnamed-chunk-10-1.png" class="quarto-figure quarto-figure-center r-stretch" width="702"></section>
<section id="second-model-graphical-diagnostics" class="slide level2 center">
<h2>Second model: graphical diagnostics</h2>

<img data-src="un_A_files/figure-revealjs/unnamed-chunk-11-1.png" class="quarto-figure quarto-figure-center r-stretch" width="702"></section>
<section id="comments-and-criticisms-1" class="slide level2 center">
<h2>Comments and criticisms</h2>
<ul>
<li>The <span class="blue">goodness of fit</span> indices are the following:</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: right;">r.squared.original</th>
<th style="text-align: right;">r.squared</th>
<th style="text-align: right;">sigma</th>
<th style="text-align: right;">deviance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.5847555</td>
<td style="text-align: right;">0.6196093</td>
<td style="text-align: right;">0.1600278</td>
<td style="text-align: right;">5.121777</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li>Do not mix <span class="blue">apple</span> and <span class="orange">oranges</span>! Compare <span class="math inline">R^2</span>s only if they refer to the same scale!</li>
</ul>
<div class="fragment">
<ul>
<li><p>This second model is <span class="blue">more parsimonious</span>, and yet it reaches satisfactory predictive performance.</p></li>
<li><p>It is also more coherent with the nature of the data: the predictions cannot be negative, and the relationship between engine size and the consumption is monotone.</p></li>
<li><p>Yet, there is still some heteroscedasticity in the residuals ‚Äî is this is due to a missing covariate that has not been included in the model?</p></li>
</ul>
</div>
</section>
<section id="a-third-model-additional-variables" class="slide level2 center">
<h2>A third model: additional variables</h2>
<ul>
<li><p>Let us consider <span class="blue">two additional variables</span>: <code>curb.weight</code> (<span class="math inline">w</span>) and <code>n.cylinders</code> (<span class="math inline">v</span>).</p></li>
<li><p>A richer model, therefore, could be: <span class="math display">
\log{Y_i} = \beta_1 + \beta_2 \log{x_i} +  \beta_3 \log{w_i} + \beta_4 I(z_i = \texttt{gas}) + \beta_5 I(v_i = 2) + \epsilon_i,
  </span> for <span class="math inline">i=1,\dots,n</span>. The estimates are:</p></li>
</ul>
<div class="fragment">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>(Intercept)</code></td>
<td style="text-align: right;">9.423</td>
<td style="text-align: right;">0.482</td>
<td style="text-align: right;">19.549</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>log(engine.size)</code></td>
<td style="text-align: right;">-0.180</td>
<td style="text-align: right;">0.051</td>
<td style="text-align: right;">-3.504</td>
<td style="text-align: right;">0.001</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>log(curb.weight)</code></td>
<td style="text-align: right;">-0.943</td>
<td style="text-align: right;">0.072</td>
<td style="text-align: right;">-13.066</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>fuel_gas</code></td>
<td style="text-align: right;">-0.353</td>
<td style="text-align: right;">0.022</td>
<td style="text-align: right;">-15.934</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>cylinders2_TRUE</code></td>
<td style="text-align: right;">-0.481</td>
<td style="text-align: right;">0.052</td>
<td style="text-align: right;">-9.301</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="a-third-model-graphical-diagnostics" class="slide level2 center">
<h2>A third model: graphical diagnostics</h2>

<img data-src="un_A_files/figure-revealjs/unnamed-chunk-14-1.png" class="quarto-figure quarto-figure-center r-stretch" width="702"></section>
<section id="comments-and-criticisms-2" class="slide level2 center">
<h2>Comments and criticisms</h2>
<ul>
<li>The goodness of fit greatly <span class="blue">improved</span>:</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: right;">r.squared.original</th>
<th style="text-align: right;">r.squared</th>
<th style="text-align: right;">sigma</th>
<th style="text-align: right;">deviance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.869048</td>
<td style="text-align: right;">0.8819199</td>
<td style="text-align: right;">0.0896089</td>
<td style="text-align: right;">1.589891</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li><p>In this third model, we handled the <span class="orange">outliers</span> appearing in the residual plots, which it turns out are identified by the group of cars having 2 cylinders.</p></li>
<li><p>The diagnostic plots are also very much improved, although still not perfect.</p></li>
<li><p>The estimates are coherent with our expectations, based on common knowledge. Have a look at the book (<span class="citation" data-cites="Azzalini2012">Azzalini and Scarpa (<a href="#/references" role="doc-biblioref" onclick="">2012</a>)</span>) for a detailed explanation of <span class="math inline">\beta_4</span>!</p></li>
<li><p>The car dataset is available from the textbook (A&amp;S) website:</p>
<ul>
<li>Dataset <a href="http://azzalini.stat.unipd.it/Book-DM/auto.dat" class="uri">http://azzalini.stat.unipd.it/Book-DM/auto.dat</a></li>
<li>Variable description <a href="http://azzalini.stat.unipd.it/Book-DM/auto.names" class="uri">http://azzalini.stat.unipd.it/Book-DM/auto.names</a></li>
</ul></li>
</ul>
</section></section>
<section>
<section id="misspecification-and-remedies" class="title-slide slide level1 center">
<h1>Misspecification and remedies</h1>

</section>
<section id="assumptions-and-misspecification" class="slide level2 center">
<h2>Assumptions and misspecification</h2>
<div class="callout callout-note no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Classical assumptions of linear models</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p><strong>(A.1)</strong> <span class="blue">Linear structure</span>, namely <span class="math inline">\bm{Y} = \bm{X}\beta + \bm{\epsilon}</span> with <span class="math inline">\mathbb{E}(\bm{\epsilon}) = 0</span>, implying <span class="math inline">\mathbb{E}(\bm{Y}) = \bm{X}\beta</span>. <sup>1</sup></p></li>
<li><p><strong>(A.2)</strong> <span class="orange">Homoschedasticity</span> and <span class="orange">uncorrelation</span> of the errors, namely <span class="math inline">\text{var}(\bm{\epsilon}) = \sigma^2 I_n</span>.</p></li>
<li><p><strong>(A.3)</strong> <span class="grey">Gaussianity</span>, namely <span class="math inline">\bm{\epsilon} \sim \text{N}_n(0, \sigma^2 I_n)</span>. In other words, the errors <span class="math inline">\epsilon_i \overset{\text{iid}}{\sim}N(0, \sigma^2)</span> are iid Gaussian random variables with zero mean and variance <span class="math inline">\sigma^2</span>.</p></li>
</ul>
<p>It is also commonly asked that <span class="math inline">\text{rk}(\bm{X}) = p</span>, otherwise the model is not identifiable.</p>
</div>
</div>
</div>
<ul>
<li>If one of the above assumptions is violated, it is not necessarily a huge problem, because
<ul>
<li>the OLS estimator <span class="math inline">\hat{\beta}</span> is fairly <span class="orange">robust</span> to misspecification;</li>
<li>simple <span class="blue">fixes</span> (variable transformations, standard error corrections) are available.</li>
</ul></li>
</ul>
<!-- - Here we review the [implications]{.blue} of each [assumption]{.orange}, and a few  [solutions]{.blue}. -->
<!-- - Even if the data are [heroschedastic]{.orange} the OLS estimator is still a very reasonable choice, but we need to "correct" the standard errors to account for that.  -->
<aside><ol class="aside-footnotes"><li id="fn1"><p>If the intercept is included in <span class="math inline">\bm{X}</span>, the errors automatically satisfy the property <span class="math inline">\mathbb{E}(\bm{\epsilon}) = 0</span>.</p></li></ol></aside></section>
<section id="robust-estimation-and-assumptions" class="slide level2 center">
<h2>Robust estimation and assumptions</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/plane.png" class="quarto-figure quarto-figure-center" width="600"></p>
</figure>
</div>
<ul>
<li><p>A plane can still fly with one of its <span class="orange">engines on fire</span>, but this is hardly an appealing situation.</p></li>
<li><p>Similarly, robust estimators may work under <span class="orange">model misspecification</span>, but this does not mean we should neglect <span class="blue">checking</span> whether the original <span class="blue">assumptions</span> hold.</p></li>
</ul>
</section>
<section id="non-normality-of-the-errors-i" class="slide level2 center">
<h2>Non-normality of the errors I üìñ</h2>
<ul>
<li><p>Let us consider the case in which assumptions <strong>(A.1)</strong>-<strong>(A.2)</strong> are <span class="blue">valid</span> but <strong>(A.3)</strong> <span class="orange">is not</span>, that is <span class="math inline">\mathbb{E}(\bm{\epsilon}) = 0</span> and <span class="math inline">\text{var}(\bm{\epsilon}) = \sigma^2 I_n</span>, but <span class="math inline">\epsilon</span> does <span class="orange">not</span> follow <span class="orange">a Gaussian</span> distribution.</p></li>
<li><p>For example, <span class="math inline">\epsilon_i</span> may follow a Laplace distribution, a skew-Normal, a logistic distribution, a Student‚Äôs t distribution, etc.</p></li>
</ul>
<div class="fragment">
<ul>
<li>The OLS estimate <span class="math inline">\hat{\beta}</span> is <span class="orange">not</span> anymore the <span class="orange">maximum likelihood</span> estimator, but it <span class="blue">preserves</span> most of its <span class="blue">properties</span> and a <span class="blue">geometric interpretation</span>.</li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Under <strong>(A.1)</strong>-<strong>(A.2)</strong>, even without requiring normality of the errors <strong>(A.3)</strong>, we obtain the usual formulas: <span class="math display">
\mathbb{E}(\hat{\beta}) = \beta, \qquad \text{var}(\hat{\beta}) = \sigma^2(\bm{X}^T\bm{X})^{-1}.
</span> Moreover, because of <span class="blue">Gauss-Markov</span> theorem, the OLS estimator <span class="math inline">\hat{\beta}</span> is the most <span class="orange">efficient</span> within the class of linear and unbiased estimators (<span class="blue">BLUE</span>) for any distribution of the errors <span class="math inline">\bm{\epsilon}</span>.</p>
</div>
</div>
</div>
<ul>
<li>In fact, note that the <span class="orange">proof</span> of the Gauss-Markov theorem requires <strong>(A.1)</strong>-<strong>(A.2)</strong> but <span class="orange">not</span> <strong>(A.3)</strong>.</li>
</ul>
</div>
</section>
<section id="non-normality-of-the-errors-ii" class="slide level2 center">
<h2>Non-normality of the errors II</h2>
<ul>
<li><p>When the errors are non Gaussian the <span class="blue">exact inferential results</span> are not valid. In particular <span class="math inline">\hat{\beta}</span> does not follow anymore a Gaussian distribution.</p></li>
<li><p>However, a <span class="orange">central limit theorem</span> can be invoked under very mild conditions on the design matrix <span class="math inline">\bm{X}</span>.</p></li>
<li><p>Thus, when the sample size <span class="math inline">n</span> is large enough, then the following <span class="orange">approximation</span> holds <span class="math display">
\hat{\beta} \:\dot{\sim}\: \text{N}_p(\beta, \sigma^2(\bm{X}^T\bm{X})^{-1}),
</span> from which <span class="blue">confidence intervals</span> and <span class="blue">test statistics</span> can be obtained as usual. The approximation is <span class="orange">excellent</span> if the errors are <span class="blue">symmetric</span> around <span class="math inline">0</span>.</p></li>
</ul>
<div class="fragment">
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p><span class="blue">Non-normality</span> of the errors is <span class="orange">not a major concern</span>: the OLS estimator preserves most of its properties, including approximate normality for sufficiently large <span class="math inline">n</span>.</p>
<p>There is often an <span class="orange">over-emphasis</span> on testing whether the residuals are Gaussian. However, even if normality is rejected, the practical implications are minimal.</p>
</div>
</div>
</div>
</div>
</section>
<section id="heteroschedasticity-of-the-errors-i" class="slide level2 center">
<h2>Heteroschedasticity of the errors I üìñ</h2>
<ul>
<li><p>Suppose now that the linearity assumption <strong>(A.1)</strong> is valid but <span class="orange">homoschedasticity</span> of the errors <strong>(A.2)</strong> is <span class="orange">not</span>. Instead, we consider <span class="blue">heteroschedastic errors</span>: <span class="math display">
\text{var}(\bm{\epsilon}) = \bm{\Sigma},\quad \text{or equivalenty} \quad \text{var}(Y_i) = \sigma^2_i, \quad i=1,\dots,n
</span> where <span class="math inline">\bm{\Sigma} = \text{diag}(\sigma^2_1,\dots,\sigma_n^2)</span> is a diagonal matrix with positive entries.</p></li>
<li><p>The OLS estimator is still <span class="orange">unbiased</span>, with a <span class="blue">modified covariance</span> structure<sup>1</sup> <span class="math display">
\mathbb{E}(\hat{\beta}) = \beta, \qquad \text{var}(\hat{\beta}) = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{\Sigma}\bm{X} (\bm{X}^T\bm{X})^{-1}.
</span> If in addition we assume Gaussianity of the errors, that is <span class="math inline">\bm{\epsilon} \sim \text{N}_n(0,\bm{\Sigma})</span>, then <span class="math display">
\hat{\beta} \sim \text{N}_p(\beta, (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{\Sigma}\bm{X} (\bm{X}^T\bm{X})^{-1}).
</span> Under suitable but mild conditions on <span class="math inline">\bm{X}</span> and <span class="math inline">\bm{\Sigma}</span>, the estimator is also <span class="orange">consistent</span>.</p></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn2"><p>These results are valid even when the matrix <span class="math inline">\bm{\Sigma}</span> is non-diagonal. This is useful to model correlated responses.</p></li></ol></aside></section>
<section id="heteroschedasticity-of-the-errors-ii" class="slide level2 center">
<h2>Heteroschedasticity of the errors II</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The OLS estimator in presence of heteroschedasticity still gives a <span class="blue">good point estimate</span>. However, the OLS estimator is <span class="orange">not efficient</span> and the classical <span class="orange">standard errors</span> are <span class="orange">wrong</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>A potential approach is to <span class="orange">accept the inefficiency</span> of the OLS estimator in this scenario and <span class="blue">correct</span> the standard errors.</p></li>
<li><p>The elements of <span class="math inline">\bm{\Sigma}</span> are <span class="orange">unknown</span>, but we can estimate them from the data. Note that <span class="math display">
\text{var}(r_i) = \text{var}(y_i - \bm{x}_i^T\hat{\beta}) = \sigma^2_i(1 - h_i),
</span> suggesting the <span class="blue">estimate</span> <span class="math inline">\hat{\sigma}^2_i = r_i^2 / (1 - h_i)</span>.</p></li>
<li><p>This leads to the so-called <span class="orange">sandwich estimator</span> of the covariance matrix: <span class="math display">
\widehat{\text{var}}(\hat{\beta}) = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\hat{\bm{\Sigma}}\bm{X} (\bm{X}^T\bm{X})^{-1},
</span> where <span class="math inline">\hat{\bm{\Sigma}} = \text{diag}(\hat{w}_1,\dots,\hat{w}_n)</span> and <span class="math inline">\hat{w}_i = r_i^2 / (1 - h_i)</span>.</p></li>
<li><p>These are known as <span class="blue">White‚Äôs</span> heteroscedasticity-consistent <span class="blue">standard errors</span>. <sup>1</sup></p></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn3"><p>White originally proposed the simpler version <span class="math inline">\hat{\sigma}^2_i = r_i^2</span>. Another variant is <span class="math inline">\hat{\sigma}^2_i = r_i^2 / (1 - h_i)^2</span>.</p></li></ol></aside></section>
<section id="weighted-least-squares-i" class="slide level2 center">
<h2>Weighted least squares I üìñ</h2>
<ul>
<li><p>Let us consider again the case of <span class="blue">heteroschedastic errors</span>: <span class="math display">
\text{var}(\bm{\epsilon}) = \sigma^2\bm{\Omega}^{-1},\quad \text{or equivalenty} \quad \text{var}(Y_i) = \sigma^2_i = \frac{\sigma^2}{\omega_i}, \quad i=1,\dots,n
</span> where <span class="math inline">\bm{\Omega} = \text{diag}(\omega_1,\dots,\omega_n)</span> are positive <span class="orange">weights</span>. However, here we assume that the weights <span class="math inline">\omega_1,\dots,\omega_n</span> are <span class="blue">known</span>, a common situation in survey design.</p></li>
<li><p>Let us define the <span class="orange">standardized</span> quantities: <span class="math display">
\bm{Y}^* = \bm{\Omega}^{1/2}\bm{Y}, \qquad \bm{X}^* = \bm{\Omega}^{1/2}\bm{X}.
</span> This is equivalent to say that <span class="math inline">Y_i^* = \sqrt{\omega_i} Y_i</span> and <span class="math inline">x_{ij}^* = \sqrt{\omega_i} x_{ij}</span>. Then, it is easy to show that <span class="math display">
\mathbb{E}(\bm{Y}^*) = \bm{X}^*\beta, \qquad \text{var}(\bm{Y}^*) = \sigma^2\bm{\Omega}^{1/2}\bm{\Omega}^{-1}\bm{\Omega}^{1/2}= \sigma^2 I_n,
</span> namely the <span class="orange">assumptions</span> <strong>(A.1)</strong> and <strong>(A.2)</strong> are valid in the <span class="blue">transformed scale</span>.</p></li>
<li><p>In other words, <span class="orange">after</span> a suitable <span class="orange">transformation</span>, we reconducted the problem to a <span class="blue">standard linear model</span>.</p></li>
</ul>
</section>
<section id="weighted-least-squares-ii" class="slide level2 center">
<h2>Weighted least squares II üìñ</h2>
<ul>
<li><p>Thus an estimator for <span class="math inline">\beta</span>, based on the transformed data, is obtained minimizing the deviance <span class="math display">
\begin{aligned}
D_\text{wls}(\beta) &amp;= (\bm{y}^* - \bm{X}^*\beta)^T(\bm{y}^* - \bm{X}^*\beta) = (\bm{y} - \bm{X}\beta)^T\bm{\Omega}(\bm{y} - \bm{X}\beta) \\
&amp;=\sum_{i=1}^n \omega_i (y_i - \bm{x}_i^T\beta)^2.
\end{aligned}
</span> which is a <span class="orange">weighted</span> version of the original <span class="blue">quadratic loss</span>, with <span class="orange">high weight = low variance</span>.</p></li>
<li><p>The resulting OLS estimate minimizing <span class="math inline">D_\text{wls}(\beta)</span> in the transformed and original scales is <span class="math display">
\hat{\beta}_\text{wls} = [(\bm{X}^*)^T\bm{X}^*]^{-1}(\bm{X}^*)^T\bm{y}^* = (\bm{X}^T\bm{\Omega}\bm{X})^{-1}\bm{X}^T\bm{\Omega}\bm{y}
</span> and it is referred to as <span class="blue">weighted least squares</span> estimator of <span class="math inline">\beta</span>.</p></li>
<li><p>Such an estimator is <span class="orange">unbiased</span> and <span class="orange">efficient</span> (<span class="blue">BLUE</span>), with <span class="math display">
\mathbb{E}(\hat{\beta}_\text{wls}) = \beta, \qquad \text{var}(\hat{\beta}_\text{wls}) = \sigma^2 (\bm{X}^T\bm{\Omega}\bm{X})^{-1}.
</span> Moreover, if <span class="math inline">\bm{\epsilon} \sim \text{N}_n(0, \sigma^2\bm{\Omega}^{-1})</span> it also coincides with the <span class="orange">maximum likelihood</span> estimator.</p></li>
</ul>
</section>
<section id="variable-transformations" class="slide level2 center">
<h2>Variable transformations</h2>
<ul>
<li>Another remedy for <span class="orange">misspecification</span> was already applied in the analysis of the car dataset, namely through <span class="blue">variable transformation</span>.</li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>While the model may have been incorrectly specified for the original data, it could become <span class="blue">appropriate</span> once the <span class="orange">transformations</span> are considered, namely <span class="math display">
g(Y_i) = h_1(\bm{x}_i)\beta_1 + \cdots + h_p(\bm{x}_i)\beta_p + \epsilon_i, \qquad i=1,\dots,n,
</span> where <span class="math inline">g(\cdot)</span> and <span class="math inline">h_j(\cdot)</span> for <span class="math inline">j=1,\dots,p</span> are <span class="orange">non-linear</span> and <span class="blue">known</span> functions.</p>
</div>
</div>
</div>
<div class="fragment">
<ul>
<li><p>This idea is conceptually <span class="blue">simple</span> and <span class="blue">powerful</span>. It also shows that linear models are capable of capturing non-linear relationships, as long as they remain <span class="orange">linear in the parameters</span>.</p></li>
<li><p>However, choosing <span class="math inline">g(\cdot)</span> and <span class="math inline">h_j(\cdot)</span> in practice is <span class="orange">not simple</span>. In our case study, we proceeded by trial and error and used <span class="blue">contextual information</span> to guide our final choice.</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Regarding the functions <span class="math inline">h_j(\cdot)</span>, <span class="blue">polynomial</span> terms are a simple and common option. More advanced approaches based on <span class="orange">splines</span> will be discussed in <a href="https://tommasorigon.github.io/datamining/">Data Mining</a>.</li>
</ul>
<!-- - Here we consider two general approaches for selecting the function $g(\cdot)$: variance stabilizing transformations and the Box-Cox transform.   -->
</div>
</section>
<section id="box-cox-transform" class="slide level2 center">
<h2>Box-Cox transform</h2>
<div class="callout callout-note no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Box-Cox transform</strong></p>
</div>
<div class="callout-content">
<p>If the data are <span class="math inline">y_i</span> are <span class="orange">positive</span>, we may consider a <span class="blue">parametric class</span> of transformations: <span class="math display">
g_\lambda(y) = \frac{y^\lambda - 1}{\lambda}, \qquad \lambda \neq 0.
</span> and <span class="math inline">g_\lambda(y) = \log{y}</span> when <span class="math inline">\lambda = 0</span>. This is the celebrated <span class="blue">Box-Cox transform</span>.</p>
<p>The case <span class="math inline">\lambda = 1</span> corresponds to no transformation, <span class="math inline">\lambda= 1/2</span> to the square root, <span class="math inline">\lambda = 0</span> to the logarithm, and <span class="math inline">\lambda= ‚àí1</span> to the reciprocal.</p>
</div>
</div>
</div>
<div class="fragment">
<ul>
<li><p>We estimate <span class="math inline">\lambda</span> from the data using <span class="orange">maximum likelihood</span>, so that the data themselves can inform us about the best transformation. We assume <span class="math display">
g_\lambda(Y_i) = \bm{x}_i^T\beta + \epsilon_i, \qquad \epsilon_i \sim \text{N}(0, \sigma^2), \qquad i=1,\dots,n.
</span></p></li>
<li><p>The aim of the transformation is to produce a response for which the <span class="blue">variance</span> of <span class="math inline">\epsilon_i</span> is <span class="blue">constant</span> with an <span class="orange">approximately normal</span> distribution.</p></li>
</ul>
</div>
</section>
<section id="box-cox-transform-derivation-i" class="slide level2 center">
<h2>Box-Cox transform: derivation I üìñ</h2>
<ul>
<li><p>By assumption, the distribution of the <span class="blue">transformed data</span> <span class="math inline">\bm{Z}_\lambda  = (g_\lambda(Y_1), \dots,g_\lambda(Y_n))^T</span> is Gaussian, therefore their joint density is <span class="math display">
f_Z(\bm{z}_\lambda) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left\{-\frac{1}{2\sigma^2}(\bm{z}_\lambda - \bm{X}\beta)^T(\bm{z}_\lambda - \bm{X}\beta)\right\}.
</span></p></li>
<li><p>Using standard tools of probability theory, we can obtain the density of the <span class="orange">original data</span>: <span class="math display">
f_Y(\bm{y})= f_Z(g_\lambda(y_1),\dots,g_\lambda(y_n))\prod_{i=1}^n\left|\frac{\partial g_\lambda(y_i)}{\partial y_i}\right|, \quad \text{where}\quad \left|\frac{\partial g_\lambda(y_i)}{\partial y_i}\right| = y_i^{\lambda - 1}.</span> The additional term is the determinant of the <span class="orange">Jacobian</span> of the transformation.</p></li>
<li><p>The <span class="blue">log-likelihood</span> therefore is <span class="math display">
\ell(\beta, \sigma^2, \lambda) = -\frac{n}{2}\log{\sigma^2} - \frac{1}{2\sigma^2}(\bm{z}_\lambda - \bm{X}\beta)^T(\bm{z}_\lambda - \bm{X}\beta) + (\lambda - 1)\sum_{i=1}^n\log{y_i}.
</span></p></li>
</ul>
</section>
<section id="box-cox-transform-derivation-ii" class="slide level2 center">
<h2>Box-Cox transform: derivation II üìñ</h2>
<ul>
<li><p>Note that, for any given value of <span class="math inline">\lambda</span>, the maximum likelihood estimates are <span class="math display">
\hat{\beta}_\lambda = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{z}_\lambda, \qquad \hat{\sigma}^2_\lambda = \frac{1}{n}(\bm{z}_\lambda - \bm{X}\hat{\beta}_\lambda)^T(\bm{z}_\lambda - \bm{X}\hat{\beta}_\lambda),
</span></p></li>
<li><p>We can <span class="orange">plug-in</span> the above estimates into the log-likelihood. This gives the <span class="blue">profile log-likelihood</span> for <span class="math inline">\lambda</span>, which admits a very simple expression: <span class="math display">
\ell_P(\lambda) = \ell(\hat{\beta}_\lambda, \hat{\sigma}^2_\lambda, \lambda) = -\frac{n}{2}\log{\hat{\sigma}^2_\lambda} + (\lambda -1)\sum_{i=1}^n\log{y_i},
</span> which must be <span class="orange">numerically maximized</span> over <span class="math inline">\lambda</span>, e.g.&nbsp;using <code>optim</code>.</p></li>
<li><p>The optimal value <span class="math inline">\hat{\lambda} = \arg\max\ell_P(\lambda)</span>, as well as a confidence interval for it, may offer guidance in choosing the right transformation.</p></li>
</ul>
<div class="fragment">
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Box and Cox suggested using this approach as an <span class="blue">exploratory tool</span>. For instance, an optimal value <span class="math inline">\hat{\lambda} = 0.4210283</span> is <span class="orange">hard to interpret</span> but it could suggest a square root transformation.</p>
</div>
</div>
</div>
</div>
</section>
<section id="box-cox-transform-for-the-auto-dataset" class="slide level2 center">
<h2>Box-Cox transform for the auto dataset</h2>

<img data-src="un_A_files/figure-revealjs/unnamed-chunk-16-1.png" class="quarto-figure quarto-figure-center r-stretch" width="630"><ul>
<li>The <span class="blue">Box-Cox transform</span> in the auto dataset suggests a <span class="orange">reciprocal</span> transformation: <span class="math display">
\frac{1}{Y_i} = \beta_1 + \beta_2 x_i +  \beta_3 w_i + \beta_4 I(z_i = \texttt{gas}) + \beta_5 I(v_i = 2) + \epsilon_i,
</span> which is a good alternative to our model based on logarithms of <span class="math inline">y_i, x_i</span>, and <span class="math inline">w_i</span> (<span class="orange">but</span>‚Ä¶).</li>
</ul>
</section>
<section id="a-fourth-model-graphical-diagnostics" class="slide level2 center">
<h2>A fourth model: graphical diagnostics</h2>

<img data-src="un_A_files/figure-revealjs/unnamed-chunk-17-1.png" class="quarto-figure quarto-figure-center r-stretch" width="702"></section>
<section id="variance-stabilizing-transformations-i" class="slide level2 center">
<h2>Variance stabilizing transformations I üìñ</h2>
<ul>
<li><p>Let <span class="math inline">Y_i \sim \text{Poisson}(\mu_i)</span> with mean <span class="math inline">\mathbb{E}(Y_i) = \mu_i = f(\bm{x}_i;\beta) = \text{var}(Y_i)</span>. Note that <span class="math display">
Y_i \,\dot{\sim}\, \text{N}(\mu_i, \mu_i),
</span> is <span class="blue">asymptotically Gaussian</span> for large values of <span class="math inline">\mu_i</span>. However, data are <span class="orange">heteroschedastic</span>.</p></li>
<li><p>In modeling count data, we could transform the counts so that, at least <span class="orange">approximately</span>, the <span class="blue">variance</span> of <span class="math inline">g(Y_i)</span> is <span class="blue">constant</span> and ordinary least squares methods can be used.</p></li>
</ul>
<div class="fragment">
<ul>
<li>As an application of the <span class="blue">delta method</span>, the following linearization holds <span class="math display">
g(Y_i) - g(\mu_i) \approx (Y_i - \mu_i)g'(\mu_i), \quad \text{ which implies }\quad \text{var}\{g(Y_i)\} \approx g'(\mu_i)^2\text{var}(Y_i).
</span> In the Poisson case <span class="math inline">\text{var}\{g(Y_i)\} \approx \mu_i \,g'(\mu_i)^2</span> and we would like this to be <span class="orange">constant</span>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>The choice <span class="math inline">g(y) = \sqrt{y}</span>, called <span class="orange">variance stabilizing</span> transformation, gives <span class="math display">
\text{var}(\sqrt{Y_i}) \approx \left(\frac{1}{2\sqrt{\mu_i}}\right)^2\mu_i = \frac{1}{4}.
</span></li>
</ul>
</div>
</section>
<section id="variance-stabilizing-transformations-ii" class="slide level2 center">
<h2>Variance stabilizing transformations II üìñ</h2>
<!-- - The variance stabilizing transformation is [broadly applicable]{.blue} to several distributions.  -->
<ul>
<li><p>Let <span class="math inline">Y_i \sim \text{Binomial}(\pi_i, m_i)</span>, with <span class="blue">success probability</span> <span class="math inline">\pi_i = f(\bm{x}_i; \beta)</span> and <span class="blue">trials</span> <span class="math inline">m_i</span>. For large values of <span class="math inline">m_i</span>, the <span class="orange">Gaussian approximation</span> holds <span class="math display">
Y_i \,\dot{\sim}\, \text{N}(m_i \pi_i, m_i\pi_i(1 - \pi_i)).
</span> However, the data are <span class="orange">heteroschedastic</span>, because <span class="math inline">\text{var}(Y_i) = m_i \pi_i(1- \pi_i)</span>.</p></li>
<li><p>Thus, a <span class="blue">variance stabilizing</span> transformation in this case is <span class="math display">
g_{m_i}(y) = \sqrt{m_i}\arcsin\left(\frac{2 y}{m_i} - 1\right),
</span> because in fact we have that <span class="math display">
\text{var}(g_{m_i}(Y_i)) \approx \left(\frac{\sqrt{m_i}}{\sqrt{1 - (2\pi_i-1)^2}} \frac{2}{m_i}\right)^2 m_i \pi_i(1- \pi_i) = 1.
</span></p></li>
</ul>
<div class="fragment">
<ul>
<li>If the data are <span class="blue">gamma distributed</span>, the <span class="orange">variance stabilizing</span> transform is <span class="math inline">g(y) = \log{y}</span>.</li>
</ul>
</div>
</section>
<section id="limitations-of-variable-transformations-i" class="slide level2 center">
<h2>Limitations of variable transformations I</h2>
<ul>
<li>Variable transformations are appealing for their simplicity and have a long history in statistics. However, they also have some <span class="orange">drawbacks</span>.</li>
</ul>
<div class="fragment">
<ul>
<li>In the case of transformations applied only to the <span class="blue">explanatory variables</span>, the model is <span class="math display">
Y_i = h_1(\bm{x}_i)\beta_1 + \cdots + h_p(\bm{x}_i)\beta_p + \epsilon_i, \qquad i=1,\dots,n,
</span> Thus, the coefficient <span class="math inline">\beta_j</span> can <span class="orange">no longer</span> be <span class="orange">interpreted</span> as the change in the mean of <span class="math inline">Y_i</span> corresponding to a <span class="blue">one-unit increase</span> <span class="math inline">x_{ij} \rightarrow x_{ij}+1</span> of the <span class="math inline">j</span>th covariate.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>In the case of transformations of the <span class="orange">response</span> variable we let <span class="math inline">\mathbb{E}(g(Y_i)) = \bm{x}_i^T\beta</span>. However:<br>
<span class="math display">
g(\mathbb{E}(Y_i)) \neq E(g(Y_i)) \quad \Longrightarrow \quad \mathbb{E}(Y_i) \neq g^{-1}(\bm{x}_i^T\beta).
</span> Thus <span class="math inline">\hat{y}_i = g^{-1}(\bm{x}_i^T\hat{\beta})</span> is a <span class="blue">reasonable prediction</span> for <span class="math inline">Y_i</span> and <span class="orange">not an estimate</span> for its <span class="orange">mean</span>.</p></li>
<li><p>When <span class="math inline">g(y) = \log{y}</span> this distinction can be made explicit, because we have <span class="math display">
g^{-1}(\mathbb{E}\{g(Y_i)\}) = g^{-1}(\bm{x}_i^T\beta) = \exp(\bm{x}_i^T\beta), \qquad \mathbb{E}(Y_i) = \exp(\bm{x}_i^T\beta + \sigma^2/2),
</span> the former being the <span class="orange">geometric mean</span> of <span class="math inline">Y_i</span>, whereas the latter is the usual <span class="blue">mean</span>.</p></li>
</ul>
</div>
</section>
<section id="limitations-of-variable-transformations-ii" class="slide level2 center">
<h2>Limitations of variable transformations II</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Suppose <span class="math inline">Y_i \sim \text{Binomial}(\pi, m_i)</span>. The variance stabilizing transformation is not fully satisfactory:</p>
<ul>
<li>It <span class="orange">complicates</span> the <span class="orange">interpretation</span>, because it models <span class="math inline">\mathbb{E}\{g(Y_i)\}</span> instead of <span class="math inline">\mathbb{E}(Y_i)</span>;</li>
<li>It is an <span class="blue">asymptotic approximation</span>, and is only valid for <span class="math inline">m_i \rightarrow \infty</span>.</li>
<li>The transform depends on <span class="math inline">m_i</span>, therefore we cannot make predictions for a generic covariate value <span class="math inline">\bm{x}_i</span> without knowing the associated <span class="math inline">m_i</span>.</li>
</ul>
<p>Besides, this transform is clearly not applicable when <span class="math inline">m_i = 1</span> and <span class="math inline">Y_i \in \{0, 1\}</span>, a very common problem called <span class="orange">binary regression</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>If we know that <span class="math inline">Y_i</span> follows, say, a Bernoulli or a Gamma distribution, then we should use the <span class="blue">appropriate likelihood</span> rather than a <span class="orange">Gaussian approximation</span>.</p></li>
<li><p><span class="blue">Generalized Linear Models</span> provide a <span class="orange">much more elegant solution</span> to the above problem.</p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered smaller scrollable">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Agresti2015" class="csl-entry" role="listitem">
Agresti, A. (2015), <em><span class="nocase">Foundations of Linear and Generalized Linear Models</span></em>, Wiley.
</div>
<div id="ref-Azzalini2008" class="csl-entry" role="listitem">
Azzalini, A. (2008), <em>Inferenza statistica</em>, Springer Verlag.
</div>
<div id="ref-Azzalini2012" class="csl-entry" role="listitem">
Azzalini, A., and Scarpa, B. (2012), <em>Data analysis and data mining: An introduction</em>, Oxford University Press.
</div>
<div id="ref-Salvan2020" class="csl-entry" role="listitem">
Salvan, A., Sartori, N., and Pace, L. (2020), <em>Modelli lineari generalizzati</em>, Springer.
</div>
</div>

</section></section>

    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="img/logoB.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://tommasorigon.github.io/StatIII">Home page</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="un_A_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="un_A_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="un_A_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="un_A_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="un_A_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="un_A_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="un_A_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="un_A_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="un_A_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>