---
title: "Generalized Linear Models"
subtitle: "Statistics III - CdL SSE"
author: "[Tommaso Rigon]{.orange}"
institute: "_UniversitÃ  degli Studi di Milano-Bicocca_"
page-layout: full
bibliography: ../biblio.bib
citeproc: true
csl: https://www.zotero.org/styles/journal-of-the-american-statistical-association
reference-location: margin
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_B_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 250
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/StatIII)"
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [simplex, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    smooth-scroll: true
    fig-dpi: 250
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false
rm(list = ls())
knitr::purl("un_B.qmd", output = "../code/un_B.R", documentation = 0)
styler:::style_file("../code/un_B.R")

library(ggplot2)
library(ggthemes)
library(MLGdata)
```

::: columns
::: {.column width="30%"}
![](img/gaussian.png)
:::

::: {.column width="70%"}
- This unit will cover the following [topics]{.orange}:
    - Exponential dispersion families
    - Likelihood, inference, and testing
    - Iteratively Re-weighted Least Squares (IRLS)
    - Deviance, model checking, and residuals
    - Model selection
    
- GLMs are regression models with a linear predictor, where the response variable follows an [exponential dispersion family]{.blue}.

- The symbol ðŸ“– means that a few extra steps are discussed in the [handwritten notes]{.blue}. 
:::
:::

:::callout-tip
The content of this Unit is covered in [Chapter 2]{.orange} of @Salvan2020. Alternatively, see [Chapter 4]{.blue} of @Agresti2015 or [Chapter 6]{.grey} of @Azzalini2008.
:::

# Introduction

## Preliminaries

- GLMs are a [class]{.blue} of [regression models]{.blue} in which a [response]{.orange} random variable $Y_i$ is modeled as a function of a vector of [covariates]{.grey} $\bm{x}_i \in \mathbb{R}^p$.  

- The random variables $Y_i$ are not restricted to be Gaussian. For example:  
  - $Y_i \in \{0,1\}$, known as [binary regression]{.blue}  
  - $Y_i \in \{0,1,\dots\}$, known as [count regression]{.orange}  
  - $Y_i \in (0,\infty)$ or $Y_i \in (-\infty,\infty)$  

- Gaussian linear models are a special case of GLMs, arising when $Y_i \in (-\infty,\infty)$.  

. . .

- The [response random variables]{.orange} are collected in the random vector $\bm{Y} = (Y_1,\dots,Y_n)^T$, whose [observed realization]{.blue} is $\bm{y} = (y_1,\dots,y_n)^T$.  

- The [design matrix]{.blue} $\bm{X}$ is an $n \times p$ [non-stochastic]{.orange} matrix containing the covariate values. The $j$th variable (column) is denoted by $\tilde{\bm{x}}_j$, while the $i$th observation (row) is $\bm{x}_i$.  

- We assume that $\bm{X}$ has [full rank]{.orange}, that is, $\text{rk}(\bm{X}) = p$ with $p \le n$.  

## `Beetles` data, from Bliss (1935) 

- The `Beetles` dataset originates from Bliss (1935). It records the number of adult flour beetles that died after a 5-hour exposure to gaseous carbon disulphide.

```{r}
data("Beetles")
colnames(Beetles) <- c("n", "deaths", "logdose")
knitr::kable(Beetles)
```

- We aim to predict the proportion of `deaths` as a function of `logdose`.  

- Modeling death proportions directly with [linear models]{.orange} is [inappropriate]{.orange}. A [variable transformation]{.blue} provides a more [principled]{.blue} solution, but it comes with [drawbacks]{.orange}.  

## `Beetles` data, a dose-response plot

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
ggplot(data = Beetles, aes(x = logdose, y = deaths/n)) +
  geom_point() +
  theme_light() +
  xlab("log-dose") +
  ylab("Proportion of deaths")
```

- There is a clear [positive]{.orange} and [non-linear]{.blue} pattern between the [proportion of deaths]{.orange} as a function of the logdose. The response variable take values in $[0, 1]$. 

## Modelling the `Beetles` data

- Let $Y_i$ be the number of dead beetles out of $m_i$, and let $x_i$ denote the log-dose. By definition, $Y_i \in \{0, 1, \dots, m_i\}$ for $i = 1,\dots,8$.  

- It is natural to model each $Y_i$ as [independent binomial]{.blue} random variables, counting the number of deaths out of $m_i$ individuals. In other words:
$$
Y_i \overset{\text{ind}}{\sim} \text{Binomial}(m_i, \pi_i), \qquad i = 1,\dots,8,
$$
where $\pi_i$ is the [probability]{.orange} of death at a given dose $x_i$. Moreover, we have
$$
\mathbb{E}\!\left(\frac{Y_i}{m_i}\right) = \mu_i = \pi_i.
$$

- A modeling approach, called [logistic regression]{.blue}, specifies:  
$$
g(\pi_i) = \log\left(\frac{\pi_i}{1 - \pi_i}\right) = \beta_1 + \beta_2 x_i \quad \Longrightarrow \quad \pi_i = g^{-1}(\beta_1 + \beta_2 x_i) = \frac{\exp(\beta_1 + \beta_2 x_i)}{1 + \exp(\beta_1 + \beta_2 x_i)}.
$$
for some parameters $\beta_1, \beta_2 \in \mathbb{R}$. [Note]{.orange} that $\pi_i \in (0, 1)$ by construction.  

## `Beetles` data, fitted model

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center

fit_Beetles <- glm(cbind(deaths, n - deaths) ~ logdose, family = "binomial", data = Beetles)

ggplot(data = Beetles, aes(x = logdose, y = deaths/n)) +
  geom_point() +
  geom_function(fun = function(x) plogis(coef(fit_Beetles)[1] + coef(fit_Beetles)[2] * x), linetype = "dashed") +
  theme_light() +
  xlab("log-dose") +
  ylab("Proportion of deaths")
```

- The [maximum likelihood]{.orange} estimates are $\hat{\beta}_1 = -60.72$ and $\hat{\beta}_2 = 34.3$. This yields the [predictive curve]{.blue} $\hat{\pi}(x) = g^{-1}(\hat{\beta}_1 + \hat{\beta}_2 x),$  which estimates the mean proportion $\mathbb{E}(Y_i / m_i)$.  


## A comparison with old tools I ðŸ“–

:::callout-tip
Let $Z_i = Y_i / m_i$ be the proportion of deaths. A direct application of linear models implies:
$$
Z_i = \beta_1 + \beta_2 x_i + \epsilon_i.
$$
The coefficients $\beta_1$ and $\beta_2$ are then estimated using OLS using $Z_i$ as response.
:::

- This is [not]{.orange} a [good approach]{.orange}. 

- First, the prediction $\hat{\beta}_1 + \hat{\beta}_2 x_i$ is [unrestricted]{.orange}, meaning it could produce values like "1.3" or "-2" as estimated [proportions]{.blue}, which is clearly undesirable.

- Moreover, the [additive structure]{.blue} $Z_i = \beta_1 + \beta_2 x_i + \epsilon_i$ cannot hold with iid errors $\epsilon_i$, because $Y_i$, and thus $Z_i$, are [discrete]{.orange}. As a result, the errors would be [heteroschedastic]{.orange}. 

- If $m_i = 1$, i.e. when the data are [binary]{.orange}, all the above issues are [exacerbated]{.orange}.

:::callout-tip
This approach is sometimes called the [linear probability model]{.blue}. Before GLMs, it was considered acceptable despite its issues, but by modern standards it should [not be used]{.orange}.
:::

## A comparison with old tools II ðŸ“–

:::callout-tip
We consider the [empirical logit]{.blue} variable transformation of $Z_i = Y_i / m_i$, obtaining  
$$
\text{logit}(\tilde{Z}_i) = \log\left(\frac{Y_i + 0.5}{m_i - Y_i + 0.5}\right) = \beta_1 + \beta_2 x_i + \epsilon_i, \qquad \tilde{Z}_i = \frac{Y_i + 0.5}{m_i +1}.
$$
A correction term is necessary because otherwise $g(\cdot) = \text{logit}(\cdot)$ is undefined. The predictions belong to $(0, 1)$, since
$$
\hat{\pi}_i = g^{-1}[\mathbb{E}\{g(\tilde{Z}_i)\}] = g^{-1}(\hat{\beta}_1 + \hat{\beta}_2 x_i),$$ 
in which $\hat{\beta}_1$ and $\hat{\beta}_2$ are estimated with OLS using $\text{logit}(\tilde{Z}_i)$ as response. 
:::

- The [interpretation]{.blue} of $\hat{\beta}$ is less clear, as they refer to the mean of $\text{logit}(\tilde{Z}_i)$ instead of $\mathbb{E}(Z_i)$.

- Inference is problematic and requires further corrections, because of [heteroschedastic]{.orange} errors. 

- This approach is [not compatible]{.orange} with the very reasonable assumption $$
Y_i \sim \text{Binomial}(m_i, \pi_i).
$$
- An arbitrary [boundary correction]{.orange} is needed. 

## A comparison with old tools III

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center

fit_Beetles_lm <- lm(I(deaths / n) ~ logdose, data = Beetles)
fit_Beetles_logit <- lm(qlogis((deaths + 0.5) / (n + 1)) ~ logdose, data = Beetles)

ggplot(data = Beetles, aes(x = logdose, y = deaths/n)) +
  xlim(c(1.65, 1.92)) + 
  geom_point() +
  geom_function(fun = function(x) plogis(coef(fit_Beetles)[1] + coef(fit_Beetles)[2] * x), linetype = "dashed") + geom_function(fun = function(x) coef(fit_Beetles_lm)[1] + coef(fit_Beetles_lm)[2] * x, linetype = "dashed", col = "darkorange") + geom_function(fun = function(x) plogis(coef(fit_Beetles_logit)[1] + coef(fit_Beetles_logit)[2] * x), linetype = "dashed", col = "darkblue") +
  theme_light() +
  xlab("log-dose") +
  ylab("Proportion of deaths")
```

- The black line is the predicted curve of a [logistic regression GLM]{.grey}. The orange line is the predictived curve of a [linear model]{.orange}. The blue line is the predictive curve of a [linear model]{.blue} after an [empirical logit variable transformation]{.blue}.

## `Aids` data

- Number of AIDS `deaths` in Australia in a sequence of three-months periods between 1983 and 1986.

:::{.smaller}
```{r}
data(Aids)
colnames(Aids) <- c("deaths", "period")
rownames(Aids) <- paste(1983:1986, rep(1:4, each = 4), sep = "-")[-c(15:16)]
knitr::kable(t(Aids[1:7, ]))
knitr::kable(t(Aids[8:14, ]))
```
:::

- We are interested in predicting the number of `deaths` as a function of the `period` of time.

- The response variable $Y_i \in \{0, 1, \dots\}$ is a [count]{.orange}. 

## `Aids` data, scatter plot

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
ggplot(data = Aids, aes(x = period, y = deaths)) +
  geom_point() +
  theme_light() +
  xlab("Period") +
  ylab("Deaths")
```

- There is a clear positive association between period and deaths. However, the increase appears to be [faster]{.orange} than [linear]{.orange}. Note that both the mean and the [variability]{.blue} of $Y_i$ increase over time.

## Modelling the `Aids` data

## `Aids` data, fitted model

```{r}
fit_Aids <- glm(deaths ~ period, family = "poisson", data = Aids)

ggplot(data = Aids, aes(x = period, y = deaths)) +
  geom_point() +
  geom_function(fun = function(x) exp(coef(fit_Aids)[1] + coef(fit_Aids)[2] * x), linetype = "dashed") +
  theme_light() +
  xlab("Period") +
  ylab("Deaths")
```


## A comparison with old tools I

## A comparison with old tools II

## Components of a GLM

## Random component of a GLM

## Linear predictor of a GLM

## Link function of a GLM

# Exponential dispersion families

## Overview

- Figure 1 of @Efron2023. [Three level]{.grey} of statistical modeling. 


![](img/EF.png){width=6in fig-align="center"}

- The [prime role]{.orange} of [exponential families]{.orange} in the theory of statistical inference was first emphasized by @Fisher1934.

- Most [well-known]{.blue} [distributions]{.blue}---such as Gaussian, Poisson, Binomial, and Gamma---are instances of exponential families.



# Likelihood, inference, and testing

# IRLS algorithm

# Deviance, model checking, residuals

# Model selection

## References {.unnumbered}

::: {#refs}
:::
