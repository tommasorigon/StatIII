---
title: "Generalized Linear Models"
subtitle: "Statistics III - CdL SSE"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
page-layout: full
bibliography: ../biblio.bib
citeproc: true
csl: https://www.zotero.org/styles/journal-of-the-american-statistical-association
reference-location: margin
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_B_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 250
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/StatIII)"
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [simplex, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    smooth-scroll: true
    fig-dpi: 250
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false
rm(list = ls())
knitr::purl("un_B.qmd", output = "../code/un_B.R", documentation = 0)
styler:::style_file("../code/un_B.R")

library(ggplot2)
library(ggthemes)
library(MLGdata)
```

::: columns
::: {.column width="30%"}
![](img/gaussian.png)
:::

::: {.column width="70%"}
- This unit will cover the following [topics]{.orange}:
    - Exponential dispersion families
    - Likelihood, inference, and testing
    - Iteratively Re-weighted Least Squares (IRLS)
    - Deviance, model checking, and residuals
    - Model selection
    
- GLMs are regression models with a linear predictor, where the response variable follows an [exponential dispersion family]{.blue}.

- The symbol 📖 means that a few extra steps are discussed in the [handwritten notes]{.blue}. 
:::
:::

:::callout-tip
The content of this Unit is covered in [Chapter 2]{.orange} of @Salvan2020. Alternatively, see [Chapter 4]{.blue} of @Agresti2015 or [Chapter 6]{.grey} of @Azzalini2008.
:::

# Introduction

## Preliminaries

- GLMs are a [class]{.blue} of [regression models]{.blue} in which a [response]{.orange} random variable $Y_i$ is modeled as a function of a vector of [covariates]{.grey} $\bm{x}_i \in \mathbb{R}^p$.  

- The random variables $Y_i$ are not restricted to be Gaussian. For example:  
  - $Y_i \in \{0,1\}$, known as [binary regression]{.blue}  
  - $Y_i \in \{0,1,\dots\}$, known as [count regression]{.orange}  
  - $Y_i \in (0,\infty)$ or $Y_i \in (-\infty,\infty)$  

- Gaussian linear models are a special case of GLMs, arising when $Y_i \in (-\infty,\infty)$.  

. . .

- The [response random variables]{.orange} are collected in the random vector $\bm{Y} = (Y_1,\dots,Y_n)^T$, whose [observed realization]{.blue} is $\bm{y} = (y_1,\dots,y_n)^T$.  

- The [design matrix]{.blue} $\bm{X}$ is an $n \times p$ [non-stochastic]{.orange} matrix containing the covariate values. The $j$th variable (column) is denoted by $\tilde{\bm{x}}_j$, while the $i$th observation (row) is $\bm{x}_i$.  

- We assume that $\bm{X}$ has [full rank]{.orange}, that is, $\text{rk}(\bm{X}) = p$ with $p \le n$.  

## `Beetles` data, from Bliss (1935) 

- The `Beetles` dataset originates from Bliss (1935). It records the number of adult flour beetles that died after a 5-hour exposure to gaseous carbon disulphide.

```{r}
data("Beetles")
colnames(Beetles) <- c("n", "deaths", "logdose")
knitr::kable(Beetles)
```

- We aim to predict the proportion of `deaths` as a function of `logdose`.  

- Modeling death proportions directly with [linear models]{.orange} is [inappropriate]{.orange}. A [variable transformation]{.blue} provides a more [principled]{.blue} solution, but it comes with [drawbacks]{.orange}.  

## `Beetles` data, a dose-response plot

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
ggplot(data = Beetles, aes(x = logdose, y = deaths/n)) +
  geom_point() +
  theme_light() +
  xlab("log-dose") +
  ylab("Proportion of deaths")
```

- There is a clear [positive]{.orange} and [non-linear]{.blue} pattern between the [proportion of deaths]{.orange} as a function of the logdose. The response variable take values in $[0, 1]$. 

## Modelling the `Beetles` data

- Let $Y_i$ be the number of dead beetles out of $m_i$, and let $x_i$ denote the log-dose. By definition, $S_i \in \{0, 1, \dots, m_i\}$ for $i = 1,\dots,8$.  

- It is natural to model each $Y_i$ as [independent binomial]{.blue} random variables, counting the number of deaths out of $m_i$ individuals. In other words:
$$
S_i \overset{\text{ind}}{\sim} \text{Binomial}(m_i, \pi_i), \qquad i = 1,\dots,8,
$$
where $\pi_i$ is the [probability]{.orange} of death at a given dose $x_i$. Moreover, we have
$$
\mathbb{E}\left(\frac{S_i}{m_i}\right)  = \pi_i = \mu_i.
$$

- A modeling approach, called [logistic regression]{.blue}, specifies:  
$$
g(\pi_i) = \log\left(\frac{\pi_i}{1 - \pi_i}\right) = \beta_1 + \beta_2 x_i \quad \Longrightarrow \quad \pi_i = g^{-1}(\beta_1 + \beta_2 x_i) = \frac{\exp(\beta_1 + \beta_2 x_i)}{1 + \exp(\beta_1 + \beta_2 x_i)}.
$$
for some parameters $\beta_1, \beta_2 \in \mathbb{R}$. [Note]{.orange} that $\pi_i \in (0, 1)$ by construction.  

## `Beetles` data, fitted model

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center

fit_Beetles <- glm(cbind(deaths, n - deaths) ~ logdose, family = "binomial", data = Beetles)

ggplot(data = Beetles, aes(x = logdose, y = deaths/n)) +
  geom_point() +
  geom_function(fun = function(x) plogis(coef(fit_Beetles)[1] + coef(fit_Beetles)[2] * x), linetype = "dashed", linewidth = 0.6) +
  theme_light() +
  xlab("log-dose") +
  ylab("Proportion of deaths")
```

- The [maximum likelihood]{.orange} estimates are $\hat{\beta}_1 = -60.72$ and $\hat{\beta}_2 = 34.3$. This yields the [predictive curve]{.blue} $\hat{\pi}(x) = g^{-1}(\hat{\beta}_1 + \hat{\beta}_2 x),$  which estimates the mean proportion $\mathbb{E}(S_i / m_i)$.  


## A comparison with old tools I 📖

:::callout-tip
Let $Y_i = S_i / m_i$ be the proportion of deaths. A direct application of linear models implies:
$$
Y_i = \beta_1 + \beta_2 x_i + \epsilon_i.
$$
The coefficients $\beta_1$ and $\beta_2$ are then estimated using OLS using $Y_i$ as response.
:::

- The prediction $\hat{\beta}_1 + \hat{\beta}_2 x_i$ is [unrestricted]{.orange}, meaning it could produce values like "1.3" or "-2" as estimated [proportions]{.blue}, which is clearly undesirable.

- The [additive structure]{.blue} $Y_i = \beta_1 + \beta_2 x_i + \epsilon_i$ cannot hold with [iid]{.blue} errors $\epsilon_i$, because $S_i$, and thus $Y_i$, are [discrete]{.orange}. As a result, the errors are always [heteroschedastic]{.orange}. 

- If $m_i = 1$, i.e. when the data are [binary]{.orange}, all the above issues are [exacerbated]{.orange}.

:::callout-tip
This approach is sometimes called the [linear probability model]{.blue}. Before GLMs, it was considered acceptable despite its issues, but by modern standards it should [not be used]{.orange}.
:::

## A comparison with old tools II 📖

:::callout-tip
We consider the [empirical logit]{.blue} variable transformation of $S_i = Y_i / m_i$, obtaining  
$$
\text{logit}(\tilde{Y}_i) = \log\left(\frac{S_i + 0.5}{m_i - S_i + 0.5}\right) = \beta_1 + \beta_2 x_i + \epsilon_i, \qquad \tilde{Y}_i = \frac{S_i + 0.5}{m_i +1}.
$$
A correction term is necessary because otherwise $g(\cdot) = \text{logit}(\cdot)$ is undefined. The predictions belong to $(0, 1)$, since
$$
\hat{\pi}_i = g^{-1}[\mathbb{E}\{g(\tilde{Y}_i)\}] = g^{-1}(\hat{\beta}_1 + \hat{\beta}_2 x_i) = \frac{\exp(\hat{\beta}_1 + \hat{\beta}_2 x_i)}{1 + \exp(\hat{\beta}_1 + \hat{\beta}_2 x_i)},$$ 
in which $\hat{\beta}_1$ and $\hat{\beta}_2$ are estimated with OLS using $\text{logit}(\tilde{Z}_i)$ as response. 
:::

- The [interpretation]{.blue} of $\hat{\beta}$ is less clear, as they refer to the mean of $\text{logit}(\tilde{Y}_i)$ instead of $\mathbb{E}(Y_i)$.

- An arbitrary [boundary correction]{.orange} is needed. 

- Inference is problematic and requires further corrections, because of [heteroschedastic]{.orange} errors. 

- This approach is [not compatible]{.orange} with the reasonable assumption $S_i \sim \text{Binomial}(m_i, \pi_i)$.

## A comparison with old tools III

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center

fit_Beetles_lm <- lm(I(deaths / n) ~ logdose, data = Beetles)
fit_Beetles_logit <- lm(qlogis((deaths + 0.5) / (n + 1)) ~ logdose, data = Beetles)

ggplot(data = Beetles, aes(x = logdose, y = deaths/n)) +
  xlim(c(1.65, 1.92)) + 
  geom_point() +
  geom_function(fun = function(x) plogis(coef(fit_Beetles)[1] + coef(fit_Beetles)[2] * x), linetype = "dashed", linewidth = 0.6) + geom_function(fun = function(x) coef(fit_Beetles_lm)[1] + coef(fit_Beetles_lm)[2] * x, linetype = "dashed", col = "darkorange", linewidth = 0.6) + geom_function(fun = function(x) plogis(coef(fit_Beetles_logit)[1] + coef(fit_Beetles_logit)[2] * x), linetype = "dashed", col = "darkblue", linewidth = 0.6) +
  theme_light() +
  xlab("log-dose") +
  ylab("Proportion of deaths")
```

- The black line is the predicted curve of a [logistic regression GLM]{.grey}. The orange line is the predictived curve of a [linear model]{.orange}. The blue line is the predictive curve of a [linear model]{.blue} after an [empirical logit variable transformation]{.blue}.

## `Aids` data

- Number of AIDS `deaths` in Australia in a sequence of three-months periods between 1983 and 1986.

:::{.smaller}
```{r}
data(Aids)
colnames(Aids) <- c("deaths", "period")
rownames(Aids) <- paste(1983:1986, rep(1:4, each = 4), sep = "-")[-c(15:16)]
knitr::kable(t(Aids[1:7, ]))
knitr::kable(t(Aids[8:14, ]))
```
:::

- We are interested in predicting the number of `deaths` as a function of the `period` of time.

- The response variable $Y_i \in \{0, 1, \dots\}$ is a non-negative [count]{.orange}.

## `Aids` data, scatter plot

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
ggplot(data = Aids, aes(x = period, y = deaths)) +
  geom_point() +
  theme_light() +
  xlab("Period") +
  ylab("Deaths")
```

- There is a clear positive association between period and deaths. However, the increase appears to be [faster]{.orange} than [linear]{.orange}. Note that both the mean and the [variability]{.blue} of $Y_i$ increase over time.

## Modelling the `Aids` data

- Let $Y_i$ be the number of deaths, and let $x_i$ denote the period. By definition, $Y_i \in \{0, 1, \dots\}$ are non-negative counts, for $i = 1,\dots,14$.  

. . .

-  We model $Y_i$ as [independent Poisson]{.blue} random variables, counting the number of deaths: 
$$
Y_i \overset{\text{ind}}{\sim} \text{Poisson}(\mu_i), \qquad i = 1,\dots,14,
$$
where $\mu_i$ is the [mean]{.orange} of $Y_i$, namely $\mathbb{E}(Y_i) = \mu_i$.

- A modeling approach, called [Poisson regression]{.blue}, specifies:  
$$
g(\mu_i) = \log(\mu_i) = \beta_1 + \beta_2 x_i \quad \Longrightarrow \quad \mu_i = g^{-1}(\beta_1 + \beta_2 x_i) = \exp(\beta_1 + \beta_2 x_i),
$$
for some parameters $\beta_1, \beta_2 \in \mathbb{R}$. [Note]{.orange} that $\mu_i > 0$ by construction. 

- Under this specification, the [variances]{.blue} of the observations are  
$$
\text{var}(Y_i) = \mu_i = \exp(\beta_1 + \beta_2 x_i),
$$
which increases with $x$, as desired. This implies that $Y_1,\dots,Y_n$ are [heteroschedastic]{.orange}, but this is not an issue in GLMs, as this aspect is [automatically accounted]{.blue} for.


## `Aids` data, fitted model

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
fit_Aids <- glm(deaths ~ period, family = "poisson", data = Aids)

ggplot(data = Aids, aes(x = period, y = deaths)) +
  geom_point() +
  geom_function(fun = function(x) exp(coef(fit_Aids)[1] + coef(fit_Aids)[2] * x), linetype = "dashed", linewidth = 0.6) +
  theme_light() +
  xlab("Period") +
  ylab("Deaths")
```

- The [maximum likelihood]{.orange} estimates are $\hat{\beta}_1 = 0.30$ and $\hat{\beta}_2 = 0.26$. This yields the [predictive curve]{.blue} $\hat{\mu}(x) = \exp(\hat{\beta}_1 + \hat{\beta}_2 x),$  which estimates the mean $\mathbb{E}(Y_i)$.  


## A comparison with old tools I


:::callout-tip
We consider the [variance-stabilizing]{.blue} transformation $S_i = \sqrt{Y_i}$, obtaining  
$$
\sqrt{Y_i} = \beta_1 + \beta_2 x_i + \epsilon_i.
$$
The predictions belong to $(0, \infty)$, since
$$
\hat{\mu}_i = \mathbb{E}(\sqrt{Y_i})^2 = (\hat{\beta}_1 + \hat{\beta}_2 x_i)^2,$$ 
in which $\hat{\beta}_1$ and $\hat{\beta}_2$ are estimated with OLS using $\sqrt{Y_i}$ as response. 
:::

- The [interpretation]{.blue} of $\hat{\beta}$ is less clear, as they refer to the mean of $\sqrt{Y}_i$ instead of $\mathbb{E}(Y_i)$.

- This approach is [not compatible]{.orange} with the reasonable assumption $Y_i \sim \text{Poisson}(\mu_i)$ and it only valid as an [asymptotic approximation]{.blue}. 

:::{.callout-tip}
To compare such a model with a similar specification, we also fit another Poisson GLM in which
$$
Y_i \overset{\text{ind}}{\sim} \text{Poisson}(\mu_i), \qquad \sqrt{\mu_i} = \beta_1 + \beta_2 x_i, \qquad i=1,\dots,14.
$$
:::

## A comparison with old tools II

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
fit_Aids_lm <- lm(sqrt(deaths) ~ period, data = Aids)
fit_Aids_sqrt <- glm(deaths ~ period, family = poisson(link = "sqrt"), data = Aids)

ggplot(data = Aids, aes(x = period, y = deaths)) +
  geom_point() +
  geom_function(fun = function(x) exp(coef(fit_Aids)[1] + coef(fit_Aids)[2] * x), linetype = "dashed", linewidth = 0.6) +
    geom_function(fun = function(x) (coef(fit_Aids_lm)[1] + coef(fit_Aids_lm)[2] * x)^2, linetype = "dashed", col = "darkorange", linewidth = 0.6) +
    geom_function(fun = function(x) (coef(fit_Aids_sqrt)[1] + coef(fit_Aids_sqrt)[2] * x)^2, linetype = "dashed", col = "darkblue", linewidth = 0.6) +
  theme_light() +
  xlab("Period") +
  ylab("Deaths")
```

- The black line is the predicted curve of a [Poisson regression GLM]{.grey} with [logarithmic link]{.gray}. The orange line is the predicted curve of a [linear model]{.orange} with a [square-root transformation]{.orange}. The blue line is the predictive curve of a [Poisson regression GLM]{.blue} with [square-root link]{.blue}.


## The components of a GLM


- [Random component]{.orange}. This specifies the probability distribution response variable $Y_i$. The observations $\bm{y} =(y_1,\dots,y_n)$ on that distribution are treated as [independent]{.orange}. 

. . .

- [Linear predictor]{.blue}. For a parameter vector $\bm{\beta} = (\beta_1,\dots,\beta_p)^T$ and an $n \times p$ design matrix $\bm{X}$, the linear predictor is $\bm{\eta} = \bm{X}\beta$. We will also write
$$
\eta_i = \bm{x}_i^T\beta = \beta_1x_{i1} + \cdots + x_{ip}\beta_p, \qquad i=1,\dots,n.
$$

. . .

- [Link function]{.grey}. This is an invertible and differentiable function $g(\cdot)$ applied to each component of the [mean]{.grey} $\mu_i = \mathbb{E}(Y_i)$ that relates it to the linear predictor:
$$
g(\mu_i) = \eta_i = \bm{x}_i^T\beta, \qquad \Longrightarrow \qquad \mu_i = g^{-1}(\eta_i) = g^{-1}(\bm{x}_i^T\beta).
$$

:::callout-tip
Note that, in general, we [cannot]{.orange} express the response in an additive way $Y_i = g^{-1}(\eta_i) + \epsilon_i$.
:::



## Random component of a GLM

- In GLMs the random variables $Y_i$ are [independent]{.orange} and they are distributed according to an [exponential dispersion family]{.blue}, whose definition will be provided in a few slides. 

- The [distributions most commonly]{.orange} used in Statistics, such as the normal, binomial, gamma, and Poisson, are exponential family distributions.

- Exponential dispersion families are [characterized]{.orange} by their [mean]{.blue} and [variance]{.blue}. Let $v(\mu) > 0$ be a function of the mean, called [variance function]{.blue} and let $a_i(\phi) >0$ be functions of an additional unknown parameter $\phi > 0$ called [dispersion]{.orange}. 



. . .

:::callout-tip
In a GLMs the observations are independent draws from a distribution $\text{ED}(\mu_i, a_i(\phi)v(\mu_i))$:
$$
Y_i \overset{\text{ind}}{\sim} \text{ED}(\mu_i, a_i(\phi)v(\mu_i)), \qquad \mathbb{E}(Y_i) = \mu_i, \qquad g(\mu_i) = \bm{x}_i^T\beta,
$$
with $\mu_i \in \mathcal{M}$. Moreover, the [variance]{.orange} is connected to the [mean]{.blue} via $v(\mu)$:
$$
\text{var}(Y_i) = a_i(\phi) v(\mu_i),
$$
where $a_i(\phi) = \phi / \omega_i$ and $\omega_i$ are [known weights]{.blue}. Special cases are $a_i(\phi) = \phi$ and $a_i(\phi) = 1$.

:::

## Notable examples

:::{.callout-tip}
In a [Gaussian linear model]{.blue} we consider the [identity link]{.blue} $g(\mu) = \mu$ and let
$$
Y_i \overset{\text{ind}}{\sim}\text{N}(\mu_i, \sigma^2), \qquad \mu_i = \bm{x}_i^T\beta.
$$
The unknown variance $\sigma^2 = \phi$ is called [dispersion]{.orange} in GLMs. The [parameter space]{.orange} is $\mathcal{M} = \mathbb{R}$, whereas $a_i(\phi) = \phi$ and the variance function is [constant]{.blue} $v(\mu) = 1$ (homoschedasticity).
:::

:::{.callout-tip}
In a [binomial regression model]{.blue} with [logit link]{.blue} $g(\mu) = \text{logit}(\mu)$ we let $Y_i = S_i/m_i$ and
$$
S_i \overset{\text{ind}}{\sim}\text{Binomial}(m_i, \pi_i),\qquad \mathbb{E}\left(Y_i\right) = \pi_i = \mu_i, \qquad \text{logit}(\mu_i) =  \bm{x}_i^T\beta.
$$
We have $a_i(\phi) = 1/m_i$ and $v(\mu) = \mu(1-\mu)$. There is [no dispersion]{.orange} parameter.
:::

:::{.callout-tip}
In [Poisson regression]{.blue} with [logarithmic link]{.blue} $g(\mu) = \log(\mu)$ we let
$$
Y_i \overset{\text{ind}}{\sim}\text{Poisson}(\mu), \qquad \log(\mu_i) =  \bm{x}_i^T\beta.
$$
We have $a_i(\phi) = 1$ and $v(\mu) = \mu$. There is [no dispersion]{.orange} parameter.
:::

# Exponential dispersion families

## Overview

- Figure 1 of @Efron2023. [Three level]{.grey} of statistical modeling. 


![](img/EF.png){width=6in fig-align="center"}

- The [prime role]{.orange} of [exponential families]{.orange} in the theory of statistical inference was first emphasized by @Fisher1934.

- Most [well-known]{.blue} [distributions]{.blue}---such as Gaussian, Poisson, Binomial, and Gamma---are instances of exponential families.

## Definition

:::callout-note
The [density]{.orange} of $Y_i$ belongs to an [exponential dispersion family]{.blue} if it can be written as
$$
p(y_i; \theta_i, \phi) = \exp\left\{\frac{\theta_i y_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi)\right\},
$$
where $y_i \in \mathcal{Y} \subseteq \mathbb{R}$, $\theta_i \in \Theta \subseteq\mathbb{R}$ and $a_i(\phi) = \phi / \omega_i$ where $\omega_i$ are [known positive weights]{.blue}. The parameter $\theta_i$ is called [natural parameter]{.orange} while $\phi$ is called [dispersion]{.blue} parameter. 
:::

. . .

- By specifying the functions $a_i(\cdot), b(\cdot)$ and $c(\cdot)$ one obtain a particular [parametric model]{.orange}. 

- The support $\mathcal{Y}$ of $Y_i$ does not depend on the parameters $\phi$ or $\theta_i$ and $b(\cdot)$ can be [differentiated]{.blue} infinitely many times. In particular, this is a [regular statistical model]{.orange}. 

- As mentioned, special cases are $a_i(\phi) = \phi$ and $a_i(\phi) = 1$. When $a_i(\phi) = 1$ and $c(y_i, \phi) = c(y_i)$ we obtain
$$
p(y_i; \theta_i) = \exp\left\{\theta_i y_i - b(\theta_i) + c(y_i)\right\},
$$
which is called [natural exponential family]{.orange} of order 1.


## Mean and variance I 📖

- Let us consider the [log-likelihood]{.orange} contribution of the $i$th observations, which is defined as
$$
\ell(\theta_i, \phi; y_i) = \log{p(y_i; \theta_i, \phi)} = \frac{\theta_i y_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi).
$$
If you prefer, this is the log-likelihood when the sample size $n = 1$ and we only observe $Y_i$. 

- The [score]{.blue} and [hessian]{.orange} functions, namely the first and second derivative over $\theta_i$ are
$$
\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; y_i) = \frac{y_i - b'(\theta_i)}{a_i(\phi)}, \qquad \frac{\partial^2}{\partial \theta_i^2}\ell(\theta_i, \phi; y_i) = \frac{-b''(\theta_i)}{a_i(\phi)}.
$$
where $b'(\cdot)$ and $b''(\cdot)$ denote the [first]{.blue} and [second]{.orange} derivative of $b(\cdot)$. 

- Recall the following [Bartlett identities]{.grey}, valid in any [regular]{.orange} statistical model:
$$
\begin{aligned}
\mathbb{E}\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right) &= 0, \\
\mathbb{E}\left\{\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right)^2\right\} = \text{var}\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right) &= \mathbb{E}\left(-\frac{\partial^2}{\partial \theta_i^2}\ell(\theta_i, \phi; Y_i)\right).
\end{aligned}
$$

## Mean and variance II 📖

- Specializing Bartlett identities in [exponential dispersion families]{.blue}, we obtain
$$
\mathbb{E}\left(\frac{Y_i - b'(\theta_i)}{a_i(\phi)}\right) = 0, \qquad \text{var}\left(\frac{Y_i - b'(\theta_i)}{a_i(\phi)}\right) = \frac{\text{var}(Y_i)}{a_i(\phi)^2} = \frac{b''(\theta_i)}{a_i(\phi)}.
$$
Re-arranging the terms, we finally get the following key result.

:::callout-warning
Let $Y_i$ be an exponential dispersion family, identified by the functions $a_i(\cdot), b(\cdot)$ and $c(\cdot)$, and with natural parameter $\theta_i$. Then the [mean]{.orange} and the [variance]{.blue} of $Y_i$ equal
$$
\mathbb{E}(Y_i) =  b'(\theta_i), \qquad \text{var}(Y_i) = a_i(\phi) b''(\theta_i).
$$
:::

- The mean $\mu_i = b'(\theta_i)$ does [not]{.orange} depend on the [dispersion]{.orange} parameter.

- We have $b''(\cdot) > 0$ because $\text{var}(Y_i)$, which means that $b(\cdot)$ is a [convex function]{.blue}. 

- Moreover, the function $b'(\theta)$ is [continuous]{.orange} and [monotone increasing]{.orange} and hence [invertible]{.orange}. 

:::aside
The function $b(\cdot)$ is related to the moment generating function of $Y_i$. Thus, higher order derivatives of $b(\cdot)$ allows the calculations of skewness, kurtosis, etc. 
:::

## Mean parametrization, variance function

:::callout-note
Let $Y_i$ be an exponential dispersion family, identified by the functions $a_i(\cdot), b(\cdot)$ and $c(\cdot)$, and with [natural parameter]{.orange} $\theta_i$, then 
$$
\mu(\theta_i):= \mu_i = \mathbb{E}(Y_i) = b'(\theta_i).
$$
The function $\mu(\cdot) : \Theta \to\mathcal{M}$ is [one-to-one]{.blue} and invertible, that is, a [reparametrization]{.orange} of $\theta_i$. We call $\mu_i$ the [mean parametrization]{.blue} of an exponential dispersion family. 
:::

. . .

- The [inverse]{.orange} relationship, re-obtaining $\theta_i$ as a function of $\mu_i$, is denoted with
$$
\theta_i = \theta(\mu_i) = b'^{-1}(\mu_i).
$$

- Using this notation, we can express the variance of $Y_i$ as a function of $\mu_i$ as follows
$$
\text{var}(Y_i) = a_i(\phi)b''(\theta_i) =  a_i(\phi)b''(\theta(\mu_i)) = a_i(\phi)v(\mu_i),
$$
where $v(\mu_i) := b''(\theta(\mu_i))$ is the [variance function]{.blue}. 

- The domain $\mathcal{M}$ and the variance function $v(\mu)$ [characterize]{.orange} the function $b(\cdot)$ and the entire distribution, for any given $a_i(\phi)$. This justifies the [notation]{.blue} $Y_i \sim \text{ED}(\mu_i, a_i(\phi)v(\mu_i))$.

## Gaussian distribution 📖

- Let $Y_i \sim \text{N}(\mu_i, \sigma^2)$. The [density]{.blue} function of $Y_i$ can be written as
$$
\begin{aligned}
p(y_i; \mu_i, \sigma^2) &= \frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(y_i - \mu_i)^2\right\}
\\
&=\exp\left\{\frac{y_i \mu_i - \mu_i^2/2}{\sigma^2}- \frac{\log(2\pi\sigma^2)}{2}-\frac{y_i^2}{2\sigma^2}\right\}
\end{aligned}
$$

- Then, we can recognize the following relationships:
$$
\theta_i = \theta(\mu_i) = \mu_i, \quad a_i(\phi) = \phi = \sigma^2, \quad b(\theta_i) = \frac{\theta_i^2}{2}, \quad c(y_i, \phi) = - \frac{\log(2\pi\phi)}{2}-\frac{y_i^2}{2\phi}.
$$
In the Gaussian case, the [mean parametrization]{.blue} and the [natural parametrization]{.orange} coincide. Moreover, the [dispersion]{.orange} $\phi$ coincides with the [variance]{.blue} $\sigma^2$. 

- Using the results we previously discussed, we obtain the well-known relationships
$$
\mathbb{E}(Y_i) = b'(\theta_i) = \theta_i, \qquad \text{var}(Y_i) = a_i(\phi)b''(\theta_i) = \phi.
$$
The [variance function]{.blue} $v(\mu_i) = 1$ is [constant]{.orange}. We will write $Y_i \sim \text{ED}(\mu_i, \phi)$ with $\mu_i \in \mathcal{M} = \mathbb{R}$. 

## Poisson distribution 📖

Let $Y_i \sim \text{Poisson}(\mu_i)$. The [pdf]{.blue} function of $Y_i$ can be written as
$$
\begin{aligned}
p(y_i; \mu_i) &= \frac{\mu_i^{y_i} e^{-\mu_i}}{y_i!}=\exp\{y_i \log(\mu_i) - \mu_i - \log(y_i!)\} \\
&=\exp\{y_i \theta_i - e^{\theta_i} - \log(y_i!)\}, \qquad y_i = 0, 1, 2,\dots.
\end{aligned}
$$

- Then, we can recognize the following relationships:
$$
\begin{aligned}
\theta_i &= \theta(\mu_i) = \log(\mu_i), \quad &&a_i(\phi) = 1, \\
b(\theta_i) &= e^{\theta_i}, \quad &&c(y_i, \phi) = c(y_i) = -\log(y_i!).
\end{aligned}
$$
There is [no dispersion]{.orange} parameter since $a_i(\phi) = 1$. 

- Using the results we previously discussed, we obtain the well-known relationships $$
\begin{aligned}
\mathbb{E}(Y_i) &= b'(\theta_i) = e^{\theta_i} = \mu_i, \\
\text{var}(Y_i) &= a_i(\phi) b''(\theta_i) = e^{\theta_i} = \mu_i.
\end{aligned}
$$
The [variance function]{.blue} $v(\mu_i) = \mu_i$ is [linear]{.orange}. We will write $Y_i \sim \text{ED}(\mu_i, \mu_i)$ with $\mu_i \in (0, \infty)$. 

## Gamma distribution I 📖

- Let $Y_i \sim \text{Gamma}(\alpha, \lambda_i)$. The [density]{.blue} function of $Y_i$ can be written as
$$
\begin{aligned}
p(y_i; \alpha, \lambda_i) &= \frac{\lambda_i^\alpha y_i^{\alpha-1}\alpha e^{-\lambda_i y_i}}{\Gamma(\alpha)}
\\
&=\exp\left\{\alpha\log{\lambda_i} - \lambda_i y_i + (\alpha-1)\log{y_i}  - \log{\Gamma}(\alpha)\right\} \\
&=\exp\left\{\alpha\left(\log{\lambda_i} - \frac{\lambda_i}{\alpha} y_i\right) + (\alpha-1)\log{y_i}  - \log{\Gamma}(\alpha)\right\} \\
&=\exp\left\{\frac{\theta_i y_i + \log(-\theta_i)}{\phi} - (1/\phi)\log{\phi}+ (1/\phi - 1)\log{y_i}  - \log{\Gamma}(1/\phi)\right\}, \qquad y > 0,\\
\end{aligned}
$$
having defined the [dispersion]{.blue} $\phi = 1/\alpha$ and the [natural parameter]{.orange} $\theta_i = -\lambda_i/\alpha$. 

- Then, we can recognize the following relationships:
$$
\begin{aligned}
\quad a_i(\phi) &= \phi, \qquad b(\theta_i) = - \log(-\theta_i), \\
c(y_i, \phi) &= -  (1/\phi)\log{\phi}+ (1/\phi - 1)\log{y_i}  - \log{\Gamma}(1/\phi).
\end{aligned}
$$

## Gamma distribution II 📖


- Using the results we previously discussed, we obtain the well-known relationships
$$
\mathbb{E}(Y_i) = b'(\theta_i) = - \frac{1}{\theta_i} = \frac{\alpha}{\lambda_i} = \mu_i, \qquad \text{var}(Y_i) = a_i(\phi)b''(\theta_i) = \frac{\phi}{\theta_i^2} = \frac{\alpha}{\lambda_i^2}.
$$

- At the same time, we can write the [inverse]{.orange} relationship linking $\theta_i$ to the [mean]{.blue} as
$$
\theta_i = \theta(\mu_i) = - \frac{1}{\mu_i}
$$
from which we finally obtain the following [quadratic]{.blue} variance function
$$
v(\mu_i) = \mu_i^2. 
$$
- We will write $Y_i \sim \text{ED}(\mu_i, \phi\mu_i^2)$ with $\mu_i \in (0, \infty)$.

## Binomial distribution I 📖

- Let $S_i \sim \text{Binomial}(m_i, \pi_i)$, with $\pi_i \in (0, 1)$. The random variable $Y_i = S_i/m_i$ has [density]{.blue}
$$
\begin{aligned}
p(y_i; m_i, \pi_i) &= \binom{m_i}{m_i y_i}\pi_i^{m_i y_i}(1 - \pi_i)^{m_i - m_i y_i}\\
&=\binom{m_i}{m_i y_i}\left(\frac{\pi_i}{1 - \pi_i}\right)^{m_i y_i}(1 - \pi_i)^{m_i}\\
&=\exp\left\{m_iy_i\log\left(\frac{\pi_i}{1 - \pi_i}\right) + m_i\log(1 - \pi_i) + \log\binom{m_i}{m_i y_i}\right\},
\end{aligned}
$$
for $y_i \in \{0, 1/m_i, 2/m_2, \dots, m_i/m_i\}$. This can be written as
$$
p(y_i; m_i, \pi_i) =\exp\left\{\frac{y_i\theta_i - \log\{1 + \exp(\theta_i)\}}{1/m_i}+ \log\binom{m_i}{m_i y_i}\right\},
$$
where the [natural parameter]{.orange} is $\theta_i = \text{logit}(\pi_i) = \log\{\pi/(1-\pi_i)\}$. 




## Binomial distribution II 📖

- Note that $\mathbb{E}(Y_i) = \mathbb{E}(Z_i / m_i) = \pi_i = \mu_i$. This means there [no dispersion]{.orange} parameter $\phi$ and
$$
\theta_i = \text{logit}(\mu_i), \quad a_i(\phi) = \frac{1}{m_i}, \quad b(\theta_i) = \log\{1 + \exp(\theta_i)\}, \quad c(y_i) = \log\binom{m_i}{m_i y_i}. 
$$

- Using the general formulas therefore we obtain
$$
\begin{aligned}
\mathbb{E}(Y_i) &= b'(\theta_i) = \frac{\exp(\theta_i)}{1 + \exp(\theta_i)} = \mu_i, \\
\text{var}(Y_i) &= a_i(\phi)b''(\theta_i) = \frac{1}{m_i}\frac{\exp(\theta_i)}{[1 + \exp(\theta_i)]^2} = \frac{\mu_i (1 - \mu_i)}{m_i},
\end{aligned}
$$
from which we obtain that the [variance function]{.blue} is $v(\mu_i) = \mu_i(1-\mu_i)$ is quadratic. 

- We will write $Y_i \sim \text{ED}(\mu_i, \mu_i(1-\mu_i))$ with $\mu_i \in \mathcal{M} = (0, 1)$. 

## Notable exponential dispersion families

| Model | $\text{N}(\mu_i, \sigma^2)$ | $\text{Gamma}(\alpha, \alpha/\mu_i)$ | $\frac{1}{m_i}\text{Binomial}(m_i, \mu_i)$ | $\text{Poisson}(\mu_i)$ |
|--------------------|---------------|---------------|---------------|---------------|
| [Support]{.blue} $\mathcal{Y}$ |  $\mathbb{R}$| $[0, \infty)$ | $\{0, 1/m_i,\dots, 1\}$ | $\mathbb{N}$ |
| $\theta_i$ | $\mu_i$ | $- 1/\mu_i$ | $\log\left(\frac{\mu_i}{1 - \mu_i}\right)$ | $\log{\mu_i}$ |
| [Parametric space]{.orange} $\Theta$ | $\mathbb{R}$ | $(-\infty, 0)$ | $\mathbb{R}$ | $\mathbb{R}$ | 
| $b(\theta_i)$ | $\theta_i^2/2$ | $-\log(-\theta_i)$ | $\log\{1 + \exp(\theta_i)\}$| $\exp(\theta_i)$ |
| $\phi$ | $\sigma^2$ | $1/\alpha$ | $1$ | $1$ |
| $a_i(\phi)$ | $\sigma^2$ | $1/\alpha$ | $1/m_i$ | $1$ |
| $\mathcal{M}$ | $\mathbb{R}$ | $(0, \infty)$ | $(0, 1)$ | $(0, \infty)$ |
| $v(\mu_i)$ | $1$ | $\mu_i^2$ | $\mu_i(1-\mu_i)$ | $\mu_i$ |

:::callout-tip
The list of exponential dispersion families does not end here. Other examples are the [inverse Gaussian]{.blue}, the [negative binomial]{.orange} and [hyperbolic secant]{.grey} distributions. 
:::

## Link functions and canonical link

- To complete the GLM specification, we need to choose a [link function]{.blue} $g(\cdot)$ such that:
$$
g(\mu_i) = \bm{x}_i^T\beta, \qquad \theta_i = \theta(\mu_i) \quad \Longrightarrow \quad \theta_i = \theta(g^{-1}(\bm{x}_i^T\beta)).
$$

- It is fairly natural to consider a [monotone]{.blue} and [differentiable]{.orange} link function $g(\cdot) : \mathcal{M} \to \mathbb{R}$ so that the inverse $g^{-1}(\cdot) : \mathbb{R} \to \mathcal{M}$. This ensures that the predictions are well-defined.
$$
\mathbb{E}(Y_i) = g^{-1}(\bm{x}_i^T\beta) \in \mathcal{M}.
$$

- For example, in [binary regression]{.blue} any continuous [cumulative distribution function]{.orange} for $g^{-1}(\cdot)$ leads to a good link function, such as $g(\cdot) = \Phi(\cdot)$ (probit) or $g^{-1}(\eta_i) = e^{\eta_i}/(1 + e^{\eta_i})$ (logistic). 

. . .

:::callout-note
The following link is called [canonical link]{.orange} and it is implied by the distribution:
$$
g(\mu_i) = \theta(\mu_i) \quad \Longrightarrow \quad \theta_i = \bm{x}_i^T\beta. 
$$
Such a choice leads to remarkable simplifications in the likelihood function.
:::

- The [identity]{.blue} link is canonical for the [Gaussian]{.blue}, the [logarithm]{.orange} is canonical for the [Poisson]{.orange}, the [logit]{.grey} is canonical for the Binomial and the [reciprocal]{.blue} is canonical for the [Gamma]{.blue}. 

<!-- - In a Gamma GLM, the reciprocal canonical link does not map $\bm{x}_i^T\beta$ into positive values, therefore the logarithmic link is sometimes preferred.  -->

# Likelihood, inference, and testing

## Likelihood function

- Let $Y_i \overset{\text{ind}}{\sim}\text{ED}(\mu_i, a_i(\phi)v(\mu_i))$ be the response variable of a GLM, with $g(\mu_i) = \bm{x}_i^T\beta$. The [joint distribution]{.blue} of the responses $\bm{Y} = (Y_1,\dots,Y_n)$ is
$$
p(\bm{y}; \beta, \phi) = \prod_{i=1}^np(y_i; \beta, \phi) = \prod_{i=1}^n \exp\left\{\frac{y_i\theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi)\right\}.
$$
with $\theta_i = \theta(\mu_i) = \theta(g^{-1}(\bm{x}_i^T\beta))$. 

- The [log-likelihood]{.orange} function therefore is
$$
\ell(\beta, \phi) = \sum_{i=1}^n\frac{y_i\theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi).
$$

. . .

- In general, there is [no sufficient statistic]{.orange} with dimension smaller than $n$. 

## ☠️ - Sufficient statistics and canonical link

- Consider the [canonical link]{.blue}, namely $\theta(\mu_i) = g(\mu_i)$ so that $\theta_i = \bm{x}_i^T\beta$. Then the log-likelihood function [simplifies]{.orange}:
$$
\begin{aligned}
\ell(\beta, \phi) &= \sum_{i=1}^n\frac{y_i\bm{x}_i^T\beta - b(\bm{x}_i^T\beta)}{a_i(\phi)} + c(y_i, \phi) \\
&= \left[\beta_1\sum_{i=1}^n\frac{x_{i1}y_i}{a_i(\phi)} + \cdots + \beta_p\sum_{i=1}^n\frac{x_{ip}y_i}{a_i(\phi)} \right] - \sum_{i=1}^n\frac{b(\bm{x}_i^T\beta)}{a_i(\phi)} + c(y_i, \phi).
\end{aligned}
$$
If $\phi$ were [known]{.blue}, say $a_i(\phi) = 1$ or $a_i(\phi) = 1/\omega_i$, then
$$
\left(\sum_{i=1}^n\frac{1}{a_i(\phi)}x_{i1}y_i, \dots, \sum_{i=1}^n\frac{1}{a_i(\phi)}x_{ip}y_i \right),
$$
is (minimal) [sufficient]{.blue} of dimension $p \le n$ for inference on $\beta$. 

. . .

- In [logistic regression]{.blue} for binary observations and [Poisson regression]{.orange} with logarithmic link we have $a_i(\phi) = 1$. The [sufficient]{.blue} statistic is $\bm{X}^T\bm{y} = \sum_{i=1}^n\bm{x}_iy_i = \left(\sum_{i=1}^nx_{i1}y_i, \dots, \sum_{i=1}^nx_{ip}y_i \right).$


## Likelihood equations I 📖

- To conduct inference using the [classical theory]{.orange} (as in _Statistica II_), we need to consider the first and second derivative of the log-likelihood, that is, the [score function]{.blue}
$$
\frac{\partial}{\partial \beta_r}\ell(\beta; \phi), \qquad r=1,\dots,p,
$$
and the [observed information matrix]{.orange} $\bm{J}$, whose elements are
$$
j_{rs} = - \frac{\partial}{\partial \beta_r}\frac{\partial}{\partial \beta_s}\ell(\beta; \phi), \qquad r, s=1,\dots,p.
$$

- These quantities have a [simple expression]{.blue} in the end, but getting there requires quite a [bit of calculus]{.orange}.

. . .

:::callout-warning
Let us focus on the estimation of $\beta$, assuming for now that $\phi$ is a [known parameter]{.blue}, as is the case in binomial or Poisson regression.

This assumption is not restrictive, even when $\phi$ is actually unknown. In fact, we will show that the maximum likelihood estimate $\hat{\beta}$ does not depend on $\phi$, and that $\beta$ and $\phi$ are [orthogonal]{.orange}.
:::

## Likelihood equations II 📖

- Let us begin by noting that
$$
\frac{\partial}{\partial \beta_r}\ell(\beta; \phi) =  \sum_{i=1}^n\frac{1}{a_i(\phi)} \left(y_i \frac{\partial \theta_i}{\partial \beta_r} - \frac{\partial b(\theta_i)}{\partial \beta_r} \right), \qquad r = 1,\dots,p.
$$
Such an expression can be [simplified]{.blue} because
$$
\frac{\partial b(\theta_i)}{\partial \beta_r} = b'(\theta_i)\frac{\partial \theta_i}{\partial \beta_r} = \mu_i\frac{\partial \theta_i}{\partial \beta_r},
$$
which implies that the score function will have the following [structure]{.orange}:
$$
\frac{\partial}{\partial \beta_r}\ell(\beta; \phi) = \sum_{i=1}^n\frac{1}{a_i(\phi)}(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r}, \qquad r=1,\dots,p.
$$

- Recall that $a_i(\phi) = \phi/\omega_i$, hence the [maximum likelihood estimator]{.blue} is obtained by solving:
$$
\textcolor{red}{\cancel{\frac{1}{\phi}}}\sum_{i=1}^n\omega_i(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r} = 0, \qquad r=1,\dots,p.
$$

## Likelihood equations III 📖

::: {.callout-tip}
Let $f(x)$ be a function with [inverse]{.orange} $g(x) = f^{-1}(x)$ and [first derivative]{.blue} $f'(x)$. Then
$$
\frac{\partial g}{\partial{x}} = [f^{-1}]'(x) = \frac{1}{f'(f^{-1}(x))}.
$$
:::

- Recall that $g(\mu_i) = \bm{x}_i^T\beta = \eta_i$ and that $\theta_i = \theta(\mu_i)$ is the inverse of $\mu(\theta_i)$. As an [application]{.blue} of the above [lemma]{.blue}:
$$
\frac{\partial \theta_i}{\partial \mu_i} = \theta'(\mu_i) = \frac{1}{\mu'(\theta(\mu_i))}= \frac{1}{b''(\theta(\mu_i))} = \frac{1}{v(\mu_i)},
$$
Moreover, since we $\mu_i = g^{-1}(\eta_i)$ we obtain 
$$
\frac{\partial \mu_i}{\partial \eta_i} = \frac{1}{g'(g^{-1}(\eta_i))} = \frac{1}{g'(\mu_i)}.
$$
- Summing up, the [chain rule of derivation]{.orange} for [composite functions]{.blue} gives:
$$
\frac{\partial \theta_i}{\partial \beta_r} = \frac{\partial \theta_i}{\partial \mu_i} \frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \eta_i}{\partial\beta_r} = \frac{1}{v(\mu_i)}\frac{1}{g'(\mu_i)}x_{ir}, \qquad r=1,\dots,p.
$$

## Likelihood equations IV 📖

- Combining all the above equations, we obtain an explicit formula for the [score function]{.blue}
$$
\frac{\partial}{\partial \beta_r}\ell(\beta; \phi)  = \frac{1}{\phi}\sum_{i=1}^n \omega_i \frac{(y_i - \mu_i)}{v(\mu_i)}\frac{x_{ir}}{g'(\mu_i)} = \sum_{i=1}^n \frac{(y_i - \mu_i)}{\text{var}(Y_i)}\frac{x_{ir}}{g'(\mu_i)}, \qquad r=1,\dots,p.
$$

:::{.callout-note}

The [maximum likelihood estimator]{.blue} solves the [likelihood equations]{.orange}:
$$
\sum_{i=1}^n \omega_i \frac{(y_i - \mu_i)}{v(\mu_i)}\frac{x_{ir}}{g'(\mu_i)} = 0, \qquad r=1,\dots,p,
$$
which [do not depend]{.orange} on $\phi$. In [matrix notation]{.blue}
$$
\bm{D}^T \bm{V}^{-1}(\bm{y} - \bm{\mu}) = \bm{0},
$$
where $\bm{V} = \text{diag}(v(\mu_1)/\omega_1,\dots,v(\mu_n)/\omega_n)$ and $\bm{D}$ is an $n \times p$ matrix whose elements are
$$
d_{ir} = \frac{\partial \mu_i}{\partial \beta_r} =\frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \eta_i}{\partial\beta_r} =\frac{1}{g'(\mu_i)}x_{ir}, \qquad i=1,\dots,n, \quad r=1,\dots,p.
$$
:::

## Canonical link: simplifications

- When using the [canonical link]{.orange} $\theta(\mu_i) = g(\mu_i)$ significant simplifications arise, because
$$
\frac{\partial \theta_i}{\partial \mu_i}= \frac{1}{v(\mu_i)} = g'(\mu_i) \quad \Longrightarrow\quad v(\mu_i)g'(\mu_i) = 1.
$$
Thus, plugging-in this equality in the former equations, gives:
$$
\frac{\partial \theta_i}{\partial \beta_r} = x_{ir}, \qquad r=1,\dots,p,
$$
which is not surprising, because the canonical link implies $\theta_i = x_{i1}\beta_1 + \cdots + x_{ip}\beta_p$.

:::callout-note
The [likelihood equations]{.orange} under the [canonical link]{.blue} are
$$
\sum_{i=1}^n \omega_i (y_i - \mu_i)x_{ir} = 0, \qquad r=1,\dots,p.
$$
Let $\bm{\Omega} = \text{diag}(\omega_1,\dots,\omega_n)$, then in [matrix notation]{.blue} we have $\bm{X}^T\bm{\Omega}(\bm{y} - \bm{\mu}) = \bm{0}$. The equations simplify even further when the weights are constant, i.e. $\bm{\Omega} = I_n$, yielding $\bm{X}^T(\bm{y} - \bm{\mu}) = \bm{0}$.
:::

## Examples of estimating equations

:::callout-tip
Let $Y_i \sim \text{ED}(\mu_i, \phi)$ with $g(\mu_i) = \mu_i$, namely the [Gaussian]{.orange} linear model with the [identity]{.grey} (canonical) link. The likelihood equations are
$$
\bm{X}^T(\bm{y} - \bm{X}\beta) = \bm{0},
$$
which are also called [normal equations]{.orange}. Their solution over $\beta$ is the OLS $\hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}$. 
:::

:::callout-tip
Let $Y_i \sim \text{ED}(\mu_i, \phi/\omega_i)$ with $g(\mu_i) = \mu_i$, namely the [Gaussian]{.orange} linear model with the [identity]{.grey} (canonical) link and [heteroschedastic errors]{.blue}. The likelihood equations are
$$
\bm{X}^T\bm{\Omega}(\bm{y} - \bm{X}\beta) = \bm{0},
$$
Their solution over $\beta$ is the weighted least square estimator $\hat{\beta}_\text{wls} = (\bm{X}^T\bm{\Omega}\bm{X})^{-1}\bm{X}^T\bm{\Omega}\bm{y}$. 
:::

:::callout-tip
Let $Y_i \sim \text{ED}(\mu_i, \mu_i)$ with $g(\mu_i) = \log{\mu_i}$, namely a [Poisson]{.orange} regression model with the [logarithmic]{.grey} (canonical) link. The likelihood equations can be solved [numerically]{.orange}
$$
\bm{X}^T(\bm{y} - \bm{\mu}) = \bm{0}, \qquad \bm{\mu} = (e^{\bm{x}_1^T\beta}, \dots,  e^{\bm{x}_n^T\beta}). 
$$
:::

## Example: `Aids` data

- In the `Aids` data, we specified a [Poisson]{.blue} regression model with $\mathbb{E}(Y_i) = \exp(\beta_1 + \beta_2 x_i)$. 

- The maximum likelihood estimate $(\hat{\beta}_1, \hat{\beta}_2)$ [solve]{.orange} simultaneously:
$$
\sum_{i=1}^n y_i =  \sum_{i=1}^n \exp(\beta_1 + \beta_2x_i), \quad
\text{and }\quad \sum_{i=1}^n x_i y_i =  \sum_{i=1}^n x_i\exp(\beta_1 + \beta_2 x_i).
$$
- This system does [not]{.orange} always admits a [solution]{.orange}. This happens, for example, in the [extreme case]{.blue} $\sum_{i=1}^ny_i = 0$, occurring when all counts equal zero.

. . .

- Using the `Aids` data we have $\sum_{i=1}^ny_i = 217$ and $\sum_{i=1}^nx_i y_i = 2387$. Via [numerical methods]{.orange} we solve the above system of equations and we obtain $\hat{\beta}_1 = 0.304$ and $\hat{\beta}_2 = 0.259$.

- The estimated mean values are $\hat{\mu}_i = \mathbb{E}(Y_i) = \exp(0.304 + 0.259 x_i)$ and in particular the mean for the [next period]{.blue} is
$$
\hat{\mu}_{i+1} = \exp(0.304 + 0.259 (x_i +1)) = \exp(0.259) \hat{\mu}_i = 1.296 \hat{\mu}_i.
$$
In other words, the estimated number of deaths increases by approximately $30%$ every trimester.

```{r}
#| echo: false
#| include: false
sum(Aids$deaths)
sum(Aids$period * Aids$deaths)
```

## Observed and expected information I

- Let us first consider the negative derivative of the score function, that is the [observed information matrix]{.orange} $\bm{J}$ with entries:
$$
\begin{aligned}
j_{rs} &=  -\frac{\partial}{\partial \beta_s}\left[\frac{\partial}{\partial \beta_r}\ell(\beta; \phi)\right] = -\frac{\partial}{\partial \beta_s}\sum_{i=1}^n\frac{1}{a_i(\phi)}(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r} \\
&=\sum_{i=1}^n\frac{1}{a_i(\phi)}\left[\frac{\partial\mu_i}{\partial \beta_s}\frac{\partial\theta_i}{\partial \beta_r} - (y_i - \mu_i) \frac{\partial^2\theta_i}{\partial \beta_r \partial \beta_s}\right], \qquad r,s = 1,\dots,p.
\end{aligned}
$$

- Let $\bm{I} = \mathbb{E}(\bm{J})$ be the $p \times p$ [Fisher information matrix]{.blue} associated with $\beta$, whose elements are
$$
i_{rs} = \mathbb{E}(j_{rs}) = \mathbb{E}\left(- \frac{\partial}{\partial \beta_r}\frac{\partial}{\partial \beta_s}\ell(\beta; \phi)\right), \qquad r,s = 1,\dots,p.
$$

- Thus, the Fisher information matrix substantially simplifies because $\mathbb{E}(Y_i) = \mu_i$, obtaining:
$$
i_{rs} = \sum_{i=1}^n\frac{1}{a_i(\phi)}\frac{\partial\mu_i}{\partial \beta_s}\frac{\partial\theta_i}{\partial \beta_r}, \qquad r,s = 1,\dots,p.
$$

## Observed and expected information II

- In the previous slides we already computed the explicit values of these derivatives: $$
\frac{\partial\mu_i}{\partial \beta_s} = \frac{x_{is}}{g'(\mu_i)}, \qquad \frac{\partial\theta_i}{\partial \beta_r} = \frac{x_{is}}{v(\mu_i) g'(\mu_i)}.
$$

::: callout-note

Combining the above equations, we obtain that the [Fisher information]{.orange} $\bm{I}$ of a GLM has entries
$$
i_{rs} = \frac{1}{\phi}\sum_{i=1}^n \omega_i \frac{x_{ir} x_{is}}{v(\mu_i)g'(\mu_i)^2} = \sum_{i=1}^n \frac{x_{ir}x_{is}}{\text{var}(Y_i) g'(\mu_i)^2}, \qquad r,s = 1,\dots,p.
$$
In [matrix notation]{.blue}, we have that
$$
\bm{I} = \bm{X}^T\bm{W}\bm{X},
$$
where $\bm{W} = \text{diag}(w_1,\dots,w_n)$ and $w_i$ are [weights]{.blue} such that
$$
w_i = \frac{1}{\phi}\frac{\omega_i}{v(\mu_i) g'(\mu_i)^2} = \frac{1}{\text{var}(Y_i) g'(\mu_i)^2}, \qquad i=1,\dots,n. 
$$
:::

## Canonical link: simplifications

:::callout-note
Under the [canonical link]{.orange} we have that $\theta_i = \beta_1x_{i1} + \cdots \beta_p x_{ip}$, which means that
$$
\frac{\partial^2\theta_i}{\partial \beta_r \partial \beta_s} = 0 \qquad \Longrightarrow \qquad i_{rs} = j_{rs} = \sum_{i=1}^n\frac{1}{a_i(\phi)}\frac{\partial\mu_i}{\partial \beta_s}\frac{\partial\theta_i}{\partial \beta_r}.
$$
The [observed information]{.blue} $\bm{J}$ is [non-stochastic]{.orange}, which means that observed information and expected (Fisher) information coincide, that is $i_{rs} = j_{rs}$ and $\bm{I} = \bm{J}$. 
:::

. . .

:::callout-note
Under the [canonical link]{.orange}, we also have the simplifications $1/v(\mu_i) = g'(\mu_i)$, yielding
$$
i_{rs}  = \frac{1}{\phi}\sum_{i=1}^n \omega_i v(\mu_i)x_{ir} x_{is}, \qquad r,s = 1,\dots,p.
$$
In [matrix notation]{.blue}, we have that $\bm{I} = \bm{X}^T\bm{W}\bm{X}$ with weights
$$
w_i = \frac{1}{\phi} \omega_iv(\mu_i) = \frac{v(\mu_i)}{a_i(\phi)}, \qquad i=1,\dots,n. 
$$
:::

## Further considerations 

- The [observed]{.orange} and [expected]{.blue} information matrices $\bm{J}$ and $\bm{I}$, as well as weights $\bm{W}$, [depend]{.blue} on $\beta$ and $\phi$. We write $\hat{\bm{J}}$, $\hat{\bm{I}}$ and $\hat{\bm{W}}$ to indicate that $\beta$ and $\phi$ have been estimated with $\hat{\beta}$ and $\hat{\phi}$.

. . .

- If $\bm{X}$ has [full rank]{.blue} and $g'(\mu) \neq 0$, then $\bm{I}$ is [positive definite]{.orange} for any value of $\beta$ and $\phi$. 

. . .

- Under the [canonical link]{.orange}, we have $\bm{J} = \bm{I}$, and both matrices are [positive definite]{.orange} if $\text{rk}(\bm{X}) = p$. 

- This implies that the log-likelihood function is [concave]{.orange} because its second derivative is negative definite, so any [solution]{.orange} to the [estimating equations]{.orange} is also a [global optimum]{.blue}.

. . .

- The Fisher information matrix could be computed exploiting [Bartlett identity]{.grey}, namely $$
i_{rs} = \mathbb{E}\left[\left(\frac{\partial}{\partial \beta_r}\ell(\beta; \phi)\right)\left(\frac{\partial}{\partial \beta_s}\ell(\beta; \phi)\right)\right], \qquad r,s = 1,\dots,p.$$
as in @Agresti2015. Of course, the final result [coincide]{.orange} with ours. 



## ☠️ - Orthogonality of $\beta$ and $\psi$

- Let us now consider the case in which $\phi$ is [unknown]{.blue} so that $a_i(\phi) = \phi/\omega_i$. We obtain:
$$
j_{r \phi} = - \frac{\partial}{\partial \beta_r}\frac{\partial}{\partial \phi}\ell(\beta; \phi) = \frac{1}{\phi^2}\sum_{i=1}^n \omega_i(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r}, \qquad r = 1,\dots,p.
$$
whose [expected value]{.orange} is $i_{r\phi} = \mathbb{E}(j_{r\phi}) = 0$ since $\mathbb{E}(Y_i) = \mu_i$. 

. . .

- This means the Fisher information matrix accounting for $\phi$ takes the form:
$$
\begin{bmatrix}
\bm{I} & \bm{0} \\
\bm{0} & i_{\phi \phi}
\end{bmatrix} \qquad\Longrightarrow\qquad  \begin{bmatrix}
\bm{I} & \bm{0} \\
\bm{0} & i_{\phi \phi}
\end{bmatrix}^{-1} = \begin{bmatrix}
\bm{I}^{-1} & \bm{0} \\
\bm{0} & 1 /i_{\phi \phi}
\end{bmatrix}
$$
where $[\bm{I}]_{rs} = i_{rs}$ are the elements associated to $\beta$ as before. 


:::callout-note
The parameters $\beta$ and $\phi$ are [orthogonal]{.orange} and their maximum likelihood estimates are [asymptotically independent]{.blue}.

Moreover, the matrices $\bm{I}$ and $\bm{I}^{-1}$ are sufficient for inference on $\beta$ and there is no need to compute $i_{\phi \phi}$. Note that the maximum likelihood $\hat{\beta}$ can also be computed without knowing $\phi$. 
:::

<!-- - In a GLM with $p = 2$ the Fisher information matrix, evaluated on a given $\beta,\phi$, looks like: -->
<!-- $$ -->
<!-- \begin{bmatrix} -->
<!-- i_{11} & i_{12} &  i_{1\phi} \\ -->
<!-- i_{21} & i_{22} &  i_{2\phi} \\ -->
<!-- i_{\phi 1} & i_{\phi2 } &  i_{\phi\phi} \\ -->
<!-- \end{bmatrix}= \begin{bmatrix} -->
<!-- 10.39 & -1.22 & 0 \\  -->
<!-- -1.22 & 10.39 & 0 \\  -->
<!-- 0 & 0 & 10 \\  -->
<!--   \end{bmatrix} \Longrightarrow \begin{bmatrix} -->
<!-- i_{11} & i_{12} &  i_{1\phi} \\ -->
<!-- i_{21} & i_{22} &  i_{2\phi} \\ -->
<!-- i_{\phi 1} & i_{\phi2 } &  i_{\phi\phi} \\ -->
<!-- \end{bmatrix}^{-1}= \begin{bmatrix} -->
<!-- 0.098 & 0.011 & 0 \\  -->
<!-- 0.011 & 0.098 & 0 \\  -->
<!-- 0 & 0 & 0.1 \\  -->
<!--   \end{bmatrix} -->
<!-- $$ -->

<!-- ```{r} -->
<!-- set.seed(1234) -->
<!-- XtX <- crossprod(matrix(rnorm(20), 10, 2)) -->
<!-- XtX <- rbind(cbind(XtX, 0),0) -->
<!-- XtX[3, 3] <- 10 -->
<!-- # xtable::xtable(XtX, digits = 2) -->
<!-- # xtable::xtable(solve(XtX), digits = 3) -->

<!-- ``` -->


## Asymptotic distribution of $\hat{\beta}$

:::callout-note
The [asymptotic distribution]{.orange} of the [maximum likelihood estimator]{.blue} is
$$
\hat{\beta} \, \dot{\sim} \, \text{N}_p\left(\beta, (\bm{X}^T\bm{W}\bm{X})^{-1}\right),
$$
for large values of $n$ and under mild regularity conditions on $\bm{X}$. 
:::

- This means that, under correct specification and mild conditions on $\bm{X}$, the maximum likelihood estimator is [asymptotically unbiased]{.orange} and with known asymptotic [variance]{.blue}
$$
\mathbb{E}(\hat{\beta}) \approx 0, \qquad \text{var}(\hat{\beta}) \approx (\bm{X}^T\bm{W}\bm{X})^{-1}.
$$

- In practice, since $\bm{W}$ depends on $\beta$ and $\phi$, rely on the following approximation
$$
\widehat{\text{var}}(\hat{\beta}) = (\bm{X}^T\hat{\bm{W}}\bm{X})^{-1},
$$
where we plugged in the consistent estimates $\hat{\beta}$ and $\hat{\phi}$ into $\bm{W}$ obtaining $\hat{\bm{W}}$. Moreover, we can computed the [standard errors]{.orange} of each $\hat{\beta}_j$ as:
$$
\texttt{Std. Error} = [\widehat{\text{se}}(\hat{\beta})]_j = \sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}}
$$

<!-- - Note that the asymptotic variance implicitly takes into account [heteroschedasticity]{.orange}.  -->

## Example: `Aids` data

- In the `Aids` data, we specified a [Poisson]{.blue} regression model with $\mathbb{E}(Y_i) = \exp(\beta_1 + \beta_2 x_i)$ and estimated $\hat{\beta} = (0.304, 0.259)$.

- This means that the weights are estimated as
$$
\hat{\bm{W}} = \text{diag}(\hat{\mu}_1,\dots,\hat{\mu}_n) = \text{diag}(1.755, \dots, 50.863).
$$
from which we obtain the [estimated]{.orange} [Fisher information matrix]{.blue}:
$$
\bm{X}^T\hat{\bm{W}}\bm{X} = \begin{bmatrix} 
\sum_{i=1}^n\hat{\mu}_i & \sum_{i=1}^n x_i\hat{\mu_i}\\
\sum_{i=1}^n x_i\hat{\mu_i} & \sum_{i=1}^n x_i^2\hat{\mu_i}
\end{bmatrix} = \begin{bmatrix}
217 & 2387\\
2387 & 28279.05
\end{bmatrix}.
$$
- Hence, the [estimated covariance]{.blue} matrix of the maximum likelihood estimator is
$$
\widehat{\text{var}}(\hat{\beta}) = (\bm{X}^T\hat{\bm{W}}\bm{X})^{-1} = \begin{bmatrix}
0.06445 & -0.00544 \\
-0.00544 & 0.00049
\end{bmatrix}.
$$
- Therefore the [estimated standard errors]{.orange} are
$$
[\widehat{\text{se}}(\hat{\beta})]_j = \sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}} \quad\Longrightarrow \quad \widehat{\text{se}}(\hat{\beta}) = (0.254, 0.022).
$$

```{r}
# solve(vcov(fit_Aids))
# round(vcov(fit_Aids), 3)
```

## Test and confidence intervals

- Consider the [hypothesis]{.blue} $H_0: \beta_j = \beta_0$ against the [alternative]{.orange} $H_1: \beta_j \neq \beta_0$. The [Wald test statistic]{.blue} $z_j$, rejecting the hypothesis for large values of $|z_j|$ is:
$$
\texttt{z value} = z_j = \frac{\hat{\beta}_j - \beta_0}{\sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}}} \, \dot{\sim}\,\text{N}(0, 1). 
$$
which is [approximately]{.orange} distributed as a [standard normal]{.blue} under $H_0$.

- The p-value is defined in the usual way, namely
$$
\texttt{p value} = \alpha_\text{obs} \approx \mathbb{P}(Z \ge |z_j|) = 2 (1 - \Phi(|z_j|)), \qquad Z \sim \text{N}(0, 1).
$$

. . .

- By inverting the the Wald test, we obtained the associated [confidence interval]{.blue}
$$
\hat{\beta}_j \pm z_{1 - \alpha/2} \sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}}.
$$
of [approximate]{.orange} level $1-\alpha$, where $z_{1-\alpha/2}$ is the quantile of a standard Gaussian.


## Example: `Aids` data

- The Wald test is the [default]{.orange} choice in **R** for checking the hypotheses $H_0 : \beta_j = 0$. In the `Aids` data we get the familiar summary.

```{r}
lmtest::coeftest(fit_Aids)
```

- The associated confidence intervals are:

```{r}
lmtest::coefci(fit_Aids)
```

## Likelihood based testing

## Example: `Aids` data

```{r}
confint(fit_Aids)
```


## Delta methods and fitted values

## Example: `Aids` data

# IRLS algorithm

## Numerical methods for maximum likelihood estimation

## Iteratively reweighted least squares I

## Iteratively reweighted least squares II

## Iteratively reweighted least squares III

## Estimation of $\phi$ 

# Deviance, model checking, residuals

# Model selection

## References {.unnumbered}

::: {#refs}
:::
