---
title: "Generalized Linear Models"
subtitle: "Statistics III - CdL SSE"
author: "[Tommaso Rigon]{.orange}"
institute: "_UniversitÃ  degli Studi di Milano-Bicocca_"
page-layout: full
bibliography: ../biblio.bib
citeproc: true
csl: https://www.zotero.org/styles/journal-of-the-american-statistical-association
reference-location: margin
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_B_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 250
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/StatIII)"
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [simplex, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    smooth-scroll: true
    fig-dpi: 250
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false
rm(list = ls())
knitr::purl("un_B.qmd", output = "../code/un_B.R", documentation = 0)
styler:::style_file("../code/un_B.R")

library(ggplot2)
library(ggthemes)
library(MLGdata)
```

::: columns
::: {.column width="30%"}
![](img/gaussian.png)
:::

::: {.column width="70%"}
- This unit will cover the following [topics]{.orange}:
    - Exponential dispersion families
    - Likelihood, inference, and testing
    - Iteratively Re-weighted Least Squares (IRLS)
    - Deviance, model checking, and residuals
    - Model selection
    
- GLMs are regression models with a linear predictor, where the response variable follows an [exponential dispersion family]{.blue}.

- The symbol ðŸ“– means that a few extra steps are discussed in the [handwritten notes]{.blue}. 
:::
:::

:::callout-tip
The content of this Unit is covered in [Chapter 2]{.orange} of @Salvan2020. Alternatively, see [Chapter 4]{.blue} of @Agresti2015 or [Chapter 6]{.grey} of @Azzalini2008.
:::

# Introduction

## Preliminaries

- GLMs are a [class]{.blue} of [regression models]{.blue} in which a [response]{.orange} random variable $Y_i$ is modeled as a function of a vector of [covariates]{.grey} $\bm{x}_i \in \mathbb{R}^p$.  

- The random variables $Y_i$ are not restricted to be Gaussian. For example:  
  - $Y_i \in \{0,1\}$, known as [binary regression]{.blue}  
  - $Y_i \in \{0,1,\dots\}$, known as [count regression]{.orange}  
  - $Y_i \in (0,\infty)$ or $Y_i \in (-\infty,\infty)$  

- Gaussian linear models are a special case of GLMs, arising when $Y_i \in (-\infty,\infty)$.  

. . .

- The [response random variables]{.orange} are collected in the random vector $\bm{Y} = (Y_1,\dots,Y_n)^T$, whose [observed realization]{.blue} is $\bm{y} = (y_1,\dots,y_n)^T$.  

- The [design matrix]{.blue} $\bm{X}$ is an $n \times p$ [non-stochastic]{.orange} matrix containing the covariate values. The $j$th variable (column) is denoted by $\tilde{\bm{x}}_j$, while the $i$th observation (row) is $\bm{x}_i$.  

- We assume that $\bm{X}$ has [full rank]{.orange}, that is, $\text{rk}(\bm{X}) = p$ with $p \le n$.  

## `Beetles` data, from Bliss (1935) 

- The `Beetles` dataset originates from Bliss (1935). It records the number of adult flour beetles that died after a 5-hour exposure to gaseous carbon disulphide.

```{r}
data("Beetles")
colnames(Beetles) <- c("n", "deaths", "logdose")
knitr::kable(Beetles)
```

- We aim to predict the proportion of `deaths` as a function of `logdose`.  

- Modeling death proportions directly with [linear models]{.orange} is [inappropriate]{.orange}. A [variable transformation]{.blue} provides a more [principled]{.blue} solution, but it comes with [drawbacks]{.orange}.  

## `Beetles` data, a dose-response plot

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
ggplot(data = Beetles, aes(x = logdose, y = deaths/n)) +
  geom_point() +
  theme_light() +
  xlab("log-dose") +
  ylab("Proportion of deaths")
```

- There is a clear [positive]{.orange} and [non-linear]{.blue} pattern between the [proportion of deaths]{.orange} as a function of the logdose. The response variable take values in $[0, 1]$. 

## Modelling the `Beetles` data

- Let $Y_i$ be the number of dead beetles out of $m_i$, and let $x_i$ denote the log-dose. By definition, $S_i \in \{0, 1, \dots, m_i\}$ for $i = 1,\dots,8$.  

- It is natural to model each $Y_i$ as [independent binomial]{.blue} random variables, counting the number of deaths out of $m_i$ individuals. In other words:
$$
S_i \overset{\text{ind}}{\sim} \text{Binomial}(m_i, \pi_i), \qquad i = 1,\dots,8,
$$
where $\pi_i$ is the [probability]{.orange} of death at a given dose $x_i$. Moreover, we have
$$
\mathbb{E}\left(\frac{S_i}{m_i}\right)  = \pi_i = \mu_i.
$$

- A modeling approach, called [logistic regression]{.blue}, specifies:  
$$
g(\pi_i) = \log\left(\frac{\pi_i}{1 - \pi_i}\right) = \beta_1 + \beta_2 x_i \quad \Longrightarrow \quad \pi_i = g^{-1}(\beta_1 + \beta_2 x_i) = \frac{\exp(\beta_1 + \beta_2 x_i)}{1 + \exp(\beta_1 + \beta_2 x_i)}.
$$
for some parameters $\beta_1, \beta_2 \in \mathbb{R}$. [Note]{.orange} that $\pi_i \in (0, 1)$ by construction.  

## `Beetles` data, fitted model

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center

fit_Beetles <- glm(cbind(deaths, n - deaths) ~ logdose, family = "binomial", data = Beetles)

ggplot(data = Beetles, aes(x = logdose, y = deaths/n)) +
  geom_point() +
  geom_function(fun = function(x) plogis(coef(fit_Beetles)[1] + coef(fit_Beetles)[2] * x), linetype = "dashed", linewidth = 0.6) +
  theme_light() +
  xlab("log-dose") +
  ylab("Proportion of deaths")
```

- The [maximum likelihood]{.orange} estimates are $\hat{\beta}_1 = -60.72$ and $\hat{\beta}_2 = 34.3$. This yields the [predictive curve]{.blue} $\hat{\pi}(x) = g^{-1}(\hat{\beta}_1 + \hat{\beta}_2 x),$  which estimates the mean proportion $\mathbb{E}(S_i / m_i)$.  


## A comparison with old tools I ðŸ“–

:::callout-tip
Let $Y_i = S_i / m_i$ be the proportion of deaths. A direct application of linear models implies:
$$
Y_i = \beta_1 + \beta_2 x_i + \epsilon_i.
$$
The coefficients $\beta_1$ and $\beta_2$ are then estimated using OLS using $Y_i$ as response.
:::

- The prediction $\hat{\beta}_1 + \hat{\beta}_2 x_i$ is [unrestricted]{.orange}, meaning it could produce values like "1.3" or "-2" as estimated [proportions]{.blue}, which is clearly undesirable.

- The [additive structure]{.blue} $Y_i = \beta_1 + \beta_2 x_i + \epsilon_i$ cannot hold with [iid]{.blue} errors $\epsilon_i$, because $S_i$, and thus $Y_i$, are [discrete]{.orange}. As a result, the errors are always [heteroschedastic]{.orange}. 

- If $m_i = 1$, i.e. when the data are [binary]{.orange}, all the above issues are [exacerbated]{.orange}.

:::callout-tip
This approach is sometimes called the [linear probability model]{.blue}. Before GLMs, it was considered acceptable despite its issues, but by modern standards it should [not be used]{.orange}.
:::

## A comparison with old tools II ðŸ“–

:::callout-tip
We consider the [empirical logit]{.blue} variable transformation of $S_i = Y_i / m_i$, obtaining  
$$
\text{logit}(\tilde{Y}_i) = \log\left(\frac{S_i + 0.5}{m_i - S_i + 0.5}\right) = \beta_1 + \beta_2 x_i + \epsilon_i, \qquad \tilde{Y}_i = \frac{S_i + 0.5}{m_i +1}.
$$
A correction term is necessary because otherwise $g(\cdot) = \text{logit}(\cdot)$ is undefined. The predictions belong to $(0, 1)$, since
$$
\hat{\pi}_i = g^{-1}[\mathbb{E}\{g(\tilde{Y}_i)\}] = g^{-1}(\hat{\beta}_1 + \hat{\beta}_2 x_i) = \frac{\exp(\hat{\beta}_1 + \hat{\beta}_2 x_i)}{1 + \exp(\hat{\beta}_1 + \hat{\beta}_2 x_i)},$$ 
in which $\hat{\beta}_1$ and $\hat{\beta}_2$ are estimated with OLS using $\text{logit}(\tilde{Z}_i)$ as response. 
:::

- The [interpretation]{.blue} of $\hat{\beta}$ is less clear, as they refer to the mean of $\text{logit}(\tilde{Y}_i)$ instead of $\mathbb{E}(Y_i)$.

- An arbitrary [boundary correction]{.orange} is needed. 

- Inference is problematic and requires further corrections, because of [heteroschedastic]{.orange} errors. 

- This approach is [not compatible]{.orange} with the reasonable assumption $S_i \sim \text{Binomial}(m_i, \pi_i)$.

## A comparison with old tools III

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center

fit_Beetles_lm <- lm(I(deaths / n) ~ logdose, data = Beetles)
fit_Beetles_logit <- lm(qlogis((deaths + 0.5) / (n + 1)) ~ logdose, data = Beetles)

ggplot(data = Beetles, aes(x = logdose, y = deaths/n)) +
  xlim(c(1.65, 1.92)) + 
  geom_point() +
  geom_function(fun = function(x) plogis(coef(fit_Beetles)[1] + coef(fit_Beetles)[2] * x), linetype = "dashed", linewidth = 0.6) + geom_function(fun = function(x) coef(fit_Beetles_lm)[1] + coef(fit_Beetles_lm)[2] * x, linetype = "dashed", col = "darkorange", linewidth = 0.6) + geom_function(fun = function(x) plogis(coef(fit_Beetles_logit)[1] + coef(fit_Beetles_logit)[2] * x), linetype = "dashed", col = "darkblue", linewidth = 0.6) +
  theme_light() +
  xlab("log-dose") +
  ylab("Proportion of deaths")
```

- The black line is the predicted curve of a [logistic regression GLM]{.grey}. The orange line is the predictived curve of a [linear model]{.orange}. The blue line is the predictive curve of a [linear model]{.blue} after an [empirical logit variable transformation]{.blue}.

## `Aids` data

- Number of AIDS `deaths` in Australia in a sequence of three-months periods between 1983 and 1986.

:::{.smaller}
```{r}
data(Aids)
colnames(Aids) <- c("deaths", "period")
rownames(Aids) <- paste(1983:1986, rep(1:4, each = 4), sep = "-")[-c(15:16)]
knitr::kable(t(Aids[1:7, ]))
knitr::kable(t(Aids[8:14, ]))
```
:::

- We are interested in predicting the number of `deaths` as a function of the `period` of time.

- The response variable $Y_i \in \{0, 1, \dots\}$ is a non-negative [count]{.orange}.

## `Aids` data, scatter plot

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
ggplot(data = Aids, aes(x = period, y = deaths)) +
  geom_point() +
  theme_light() +
  xlab("Period") +
  ylab("Deaths")
```

- There is a clear positive association between period and deaths. However, the increase appears to be [faster]{.orange} than [linear]{.orange}. Note that both the mean and the [variability]{.blue} of $Y_i$ increase over time.

## Modelling the `Aids` data

- Let $Y_i$ be the number of deaths, and let $x_i$ denote the period. By definition, $Y_i \in \{0, 1, \dots\}$ are non-negative counts, for $i = 1,\dots,14$.  

. . .

-  We model $Y_i$ as [independent Poisson]{.blue} random variables, counting the number of deaths: 
$$
Y_i \overset{\text{ind}}{\sim} \text{Poisson}(\mu_i), \qquad i = 1,\dots,14,
$$
where $\mu_i$ is the [mean]{.orange} of $Y_i$, namely $\mathbb{E}(Y_i) = \mu_i$.

- A modeling approach, called [Poisson regression]{.blue}, specifies:  
$$
g(\mu_i) = \log(\mu_i) = \beta_1 + \beta_2 x_i \quad \Longrightarrow \quad \mu_i = g^{-1}(\beta_1 + \beta_2 x_i) = \exp(\beta_1 + \beta_2 x_i),
$$
for some parameters $\beta_1, \beta_2 \in \mathbb{R}$. [Note]{.orange} that $\mu_i > 0$ by construction. 

- Under this specification, the [variances]{.blue} of the observations are  
$$
\text{var}(Y_i) = \mu_i = \exp(\beta_1 + \beta_2 x_i),
$$
which increases with $x$, as desired. This implies that $Y_1,\dots,Y_n$ are [heteroschedastic]{.orange}, but this is not an issue in GLMs, as this aspect is [automatically accounted]{.blue} for.


## `Aids` data, fitted model

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
fit_Aids <- glm(deaths ~ period, family = "poisson", data = Aids)

ggplot(data = Aids, aes(x = period, y = deaths)) +
  geom_point() +
  geom_function(fun = function(x) exp(coef(fit_Aids)[1] + coef(fit_Aids)[2] * x), linetype = "dashed", linewidth = 0.6) +
  theme_light() +
  xlab("Period") +
  ylab("Deaths")
```

- The [maximum likelihood]{.orange} estimates are $\hat{\beta}_1 = 0.30$ and $\hat{\beta}_2 = 0.26$. This yields the [predictive curve]{.blue} $\hat{\mu}(x) = \exp(\hat{\beta}_1 + \hat{\beta}_2 x),$  which estimates the mean $\mathbb{E}(Y_i)$.  


## A comparison with old tools I


:::callout-tip
We consider the [variance-stabilizing]{.blue} transformation $S_i = \sqrt{Y_i}$, obtaining  
$$
\sqrt{Y_i} = \beta_1 + \beta_2 x_i + \epsilon_i.
$$
The predictions belong to $(0, \infty)$, since
$$
\hat{\mu}_i = \mathbb{E}(\sqrt{Y_i})^2 = (\hat{\beta}_1 + \hat{\beta}_2 x_i)^2,$$ 
in which $\hat{\beta}_1$ and $\hat{\beta}_2$ are estimated with OLS using $\sqrt{Y_i}$ as response. 
:::

- The [interpretation]{.blue} of $\hat{\beta}$ is less clear, as they refer to the mean of $\sqrt{Y}_i$ instead of $\mathbb{E}(Y_i)$.

- This approach is [not compatible]{.orange} with the reasonable assumption $Y_i \sim \text{Poisson}(\mu_i)$ and it only valid as an [asymptotic approximation]{.blue}. 

:::{.callout-tip}
To compare such a model with a similar specification, we also fit another Poisson GLM in which
$$
Y_i \overset{\text{ind}}{\sim} \text{Poisson}(\mu_i), \qquad \sqrt{\mu_i} = \beta_1 + \beta_2 x_i, \qquad i=1,\dots,14.
$$
:::

## A comparison with old tools II

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
fit_Aids_lm <- lm(sqrt(deaths) ~ period, data = Aids)
fit_Aids_sqrt <- glm(deaths ~ period, family = poisson(link = "sqrt"), data = Aids)

ggplot(data = Aids, aes(x = period, y = deaths)) +
  geom_point() +
  geom_function(fun = function(x) exp(coef(fit_Aids)[1] + coef(fit_Aids)[2] * x), linetype = "dashed", linewidth = 0.6) +
    geom_function(fun = function(x) (coef(fit_Aids_lm)[1] + coef(fit_Aids_lm)[2] * x)^2, linetype = "dashed", col = "darkorange", linewidth = 0.6) +
    geom_function(fun = function(x) (coef(fit_Aids_sqrt)[1] + coef(fit_Aids_sqrt)[2] * x)^2, linetype = "dashed", col = "darkblue", linewidth = 0.6) +
  theme_light() +
  xlab("Period") +
  ylab("Deaths")
```

- The black line is the predicted curve of a [Poisson regression GLM]{.grey} with [logarithmic link]{.gray}. The orange line is the predicted curve of a [linear model]{.orange} with a [square-root transformation]{.orange}. The blue line is the predictive curve of a [Poisson regression GLM]{.blue} with [square-root link]{.blue}.


## The components of a GLM


- [Random component]{.orange}. This specifies the probability distribution response variable $Y_i$. The observations $\bm{y} =(y_1,\dots,y_n)$ on that distribution are treated as [independent]{.orange}. 

. . .

- [Linear predictor]{.blue}. For a parameter vector $\bm{\beta} = (\beta_1,\dots,\beta_p)^T$ and an $n \times p$ design matrix $\bm{X}$, the linear predictor is $\bm{\eta} = \bm{X}\beta$. We will also write
$$
\eta_i = \bm{x}_i^T\beta = \beta_1x_{i1} + \cdots + x_{ip}\beta_p, \qquad i=1,\dots,n.
$$

. . .

- [Link function]{.grey}. This is an invertible and differentiable function $g(\cdot)$ applied to each component of the [mean]{.grey} $\mu_i = \mathbb{E}(Y_i)$ that relates it to the linear predictor:
$$
g(\mu_i) = \eta_i = \bm{x}_i^T\beta, \qquad \Longrightarrow \qquad \mu_i = g^{-1}(\eta_i) = g^{-1}(\bm{x}_i^T\beta).
$$

:::callout-tip
Note that, in general, we [cannot]{.orange} express the response in an additive way $Y_i = g^{-1}(\eta_i) + \epsilon_i$.
:::



## Random component of a GLM

- In GLMs the random variables $Y_i$ are [independent]{.orange} and they are distributed according to an [exponential dispersion family]{.blue}, whose definition will be provided in a few slides. 

- For now, it suffices to say that the distributions most commonly used in Statistics, such as the normal, binomial, gamma, and Poisson, are exponential family distributions.

- Exponential dispersion families are [characterized]{.orange} by their [mean]{.blue} and [variance]{.blue}. Let $v(\mu) > 0$ be a function of the mean, called [variance function]{.blue} and let $a_i(\phi) >0$ be functions of an additional unknown parameter $\phi > 0$ called [dispersion]{.orange}. 



. . .

:::callout-tip
In a GLMs the observations are independent draws from a distribution $\text{ED}(\mu_i, a_i(\phi)v(\mu_i))$:
$$
Y_i \overset{\text{ind}}{\sim} \text{ED}(\mu_i, a_i(\phi)v(\mu_i)), \qquad \mathbb{E}(Y_i) = \mu_i, \qquad g(\mu_i) = \bm{x}_i^T\beta,
$$
with $\mu_i \in \mathcal{M}$. Moreover, the [variance]{.orange} is connected to the [mean]{.blue} via $v(\mu)$:
$$
\text{var}(Y_i) = a_i(\phi) v(\mu_i),
$$
We typically have $a_i(\phi) = \phi$, $a_i(\phi) = 1$ or $a_i(\phi) = \phi / w_i$  where $w_i$ are [known weights]{.blue}.

:::

## Notable examples

:::{.callout-tip}
In a [Gaussian linear model]{.blue} we consider the [identity link]{.blue} $g(\mu) = \mu$ and let
$$
Y_i \overset{\text{ind}}{\sim}\text{N}(\mu_i, \sigma^2), \qquad \mu_i = \bm{x}_i^T\beta.
$$
The unknown variance $\sigma^2 = \phi$ is called [dispersion]{.orange} in GLMs. The [parameter space]{.orange} is $\mathcal{M} = \mathbb{R}$, whereas $a_i(\phi) = \phi$ and the variance function is [constant]{.blue} $v(\mu) = 1$ (homoschedasticity).
:::

:::{.callout-tip}
In a [binomial regression model]{.blue} with [logit link]{.blue} $g(\mu) = \text{logit}(\mu)$ we let $Y_i = S_i/m_i$ and
$$
S_i \overset{\text{ind}}{\sim}\text{Binomial}(m_i, \pi_i),\qquad \mathbb{E}\left(Y_i\right) = \pi_i = \mu_i, \qquad \text{logit}(\mu_i) =  \bm{x}_i^T\beta.
$$
We have $a_i(\phi) = 1/m_i$ and $v(\mu) = \mu(1-\mu)$. There is [no dispersion]{.orange} parameter.
:::

:::{.callout-tip}
In [Poisson regression]{.blue} with [logarithmic link]{.blue} $g(\mu) = \log(\mu)$ we let
$$
Y_i \overset{\text{ind}}{\sim}\text{Poisson}(\mu), \qquad \log(\mu_i) =  \bm{x}_i^T\beta.
$$
We have $a_i(\phi) = 1$ and $v(\mu) = \mu$. There is [no dispersion]{.orange} parameter.
:::

# Exponential dispersion families

## Overview

- Figure 1 of @Efron2023. [Three level]{.grey} of statistical modeling. 


![](img/EF.png){width=6in fig-align="center"}

- The [prime role]{.orange} of [exponential families]{.orange} in the theory of statistical inference was first emphasized by @Fisher1934.

- Most [well-known]{.blue} [distributions]{.blue}---such as Gaussian, Poisson, Binomial, and Gamma---are instances of exponential families.

## Definition

:::callout-note
The [density]{.orange} of $Y_i$ belongs to an [exponential dispersion family]{.blue} if it can be written as
$$
p(y_i; \theta_i, \phi) = \exp\left\{\frac{\theta_i y_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi)\right\},
$$
where $y_i \in \mathcal{Y} \subseteq \mathbb{R}$, $\theta_i \in \Theta \subseteq\mathbb{R}$ and $a_i(\phi) > 0$. The parameter $\theta_i$ is called [natural parameter]{.orange} while $\phi$ is called [dispersion]{.blue} parameter. 
:::

- By specifying the functions $a_i(\cdot), b(\cdot)$ and $c(\cdot)$ one obtain a particular [parametric model]{.orange}. 

- The support $\mathcal{Y}$ of $Y_i$ does not depend on the parameters $\phi$ or $\theta_i$ and $b(\cdot)$ can be [differentiated]{.blue} infinitely many times. In particular, this is a [regular statistical model]{.orange}. 

- As mentioned, we typically have $a_i(\phi) = \phi$, $a_i(\phi) = 1$ or $a_i(\phi) = \phi / w_i$  where $w_i$ are [known weights]{.blue}. When $a_i(\phi) = 1$ and $c(y_i, \phi) = c(y_i)$ we obtain
$$
p(y_i; \theta_i) = \exp\left\{\theta_i y_i - b(\theta_i) + c(y_i)\right\},
$$
which is called [natural exponential family]{.orange} of order 1.


## Mean and variance I ðŸ“–

- Let us consider the [log-likelihood]{.orange} contribution of the $i$th observations, which is defined as
$$
\ell(\theta_i, \phi; y_i) = \log{p(y_i; \theta_i, \phi)} = \frac{\theta_i y_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi).
$$
If you prefer, this is the log-likelihood when the sample size $n = 1$ and we only observe $Y_i$. 

- The [score]{.blue} and [hessian]{.orange} functions, namely the first and second derivative over $\theta_i$ are
$$
\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; y_i) = \frac{y_i - b'(\theta_i)}{a_i(\phi)}, \qquad \frac{\partial^2}{\partial \theta_i^2}\ell(\theta_i, \phi; y_i) = \frac{-b''(\theta_i)}{a_i(\phi)}.
$$
where $b'(\cdot)$ and $b''(\cdot)$ denote the [first]{.blue} and [second]{.orange} derivative of $b(\cdot)$. 

- Recall the following [Bartlett identities]{.grey}, valid in any [regular]{.orange} statistical model:
$$
\begin{aligned}
\mathbb{E}\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right) &= 0, \\
\mathbb{E}\left\{\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right)^2\right\} = \text{var}\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right) &= \mathbb{E}\left(-\frac{\partial^2}{\partial \theta_i^2}\ell(\theta_i, \phi; Y_i)\right).
\end{aligned}
$$

## Mean and variance II ðŸ“–

- Specializing Bartlett identities in [exponential dispersion families]{.blue}, we obtain
$$
\mathbb{E}\left(\frac{Y_i - b'(\theta_i)}{a_i(\phi)}\right) = 0, \qquad \text{var}\left(\frac{Y_i - b'(\theta_i)}{a_i(\phi)}\right) = \frac{\text{var}(Y_i)}{a_i(\phi)^2} = \frac{b''(\theta_i)}{a_i(\phi)}.
$$
Re-arranging the terms, we finally get the following key result.

:::callout-warning
Let $Y_i$ be an exponential dispersion family, identified by the functions $a_i(\cdot), b(\cdot)$ and $c(\cdot)$, and with natural parameter $\theta_i$. Then the [mean]{.orange} and the [variance]{.blue} of $Y_i$ equal
$$
\mathbb{E}(Y_i) =  b'(\theta_i), \qquad \text{var}(Y_i) = a_i(\phi) b''(\theta_i).
$$
:::

- The mean $\mu_i = b'(\theta_i)$ does [not]{.orange} depend on the [dispersion]{.orange} parameter.

- We have $b''(\cdot) > 0$ because $\text{var}(Y_i)$, which means that $b(\cdot)$ is a [convex function]{.blue}. 

- Moreover, the function $b'(\theta)$ is [continuous]{.orange} and [monotone increasing]{.orange} and hence [invertible]{.orange}. 

:::aside
The function $b(\cdot)$ is related to the moment generating function of $Y_i$. Thus, higher order derivatives of $b(\cdot)$ allows the calculations of skewness, kurtosis, etc. 
:::

## Mean parametrization, variance function

:::callout-note
Let $Y_i$ be an exponential dispersion family, identified by the functions $a_i(\cdot), b(\cdot)$ and $c(\cdot)$, and with [natural parameter]{.orange} $\theta_i$, then 
$$
\mu(\theta_i):= \mu_i = \mathbb{E}(Y_i) = b'(\theta_i).
$$
The function $\mu(\cdot) : \Theta \to\mathcal{M}$ is [one-to-one]{.blue} and invertible, that is, a [reparametrization]{.orange} of $\theta_i$. We call $\mu_i$ the [mean parametrization]{.blue} of an exponential dispersion family. 
:::

. . .

- The [inverse]{.orange} relationship, re-obtaining $\theta_i$ as a function of $\mu_i$, is denoted with
$$
\theta_i = \theta(\mu_i) = b'^{-1}(\mu_i).
$$

- Using this notation, we can express the variance of $Y_i$ as a function of $\mu_i$ as follows
$$
\text{var}(Y_i) = a_i(\phi)b''(\theta_i) =  a_i(\phi)b''(\theta(\mu_i)) = a_i(\phi)v(\mu_i),
$$
where $v(\mu_i) := b''(\theta(\mu_i))$ is the [variance function]{.blue}. 

- The domain $\mathcal{M}$ and the variance function $v(\mu)$ [characterize]{.orange} the function $b(\cdot)$ and the entire distribution, for any given $a_i(\phi)$. This justifies the [notation]{.blue} $Y_i \sim \text{ED}(\mu_i, a_i(\phi)v(\mu_i))$.

## Gaussian distribution ðŸ“–

- Let $Y_i \sim \text{N}(\mu_i, \sigma^2)$. The [density]{.blue} function of $Y_i$ can be written as
$$
\begin{aligned}
p(y_i; \mu_i, \sigma^2) &= \frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(y_i - \mu_i)^2\right\}
\\
&=\exp\left\{\frac{y_i \mu_i - \mu_i^2/2}{\sigma^2}- \frac{\log(2\pi\sigma^2)}{2}-\frac{y_i^2}{2\sigma^2}\right\}
\end{aligned}
$$

- Then, we can recognize the following relationships:
$$
\theta_i = \theta(\mu_i) = \mu_i, \quad a_i(\phi) = \phi = \sigma^2, \quad b(\theta_i) = \frac{\theta_i^2}{2}, \quad c(y_i, \phi) = - \frac{\log(2\pi\phi)}{2}-\frac{y_i^2}{2\phi}.
$$
In the Gaussian case, the [mean parametrization]{.blue} and the [natural parametrization]{.orange} coincide. Moreover, the [dispersion]{.orange} $\phi$ coincides with the [variance]{.blue} $\sigma^2$. 

- Using the results we previously discussed, we obtain the well-known relationships
$$
\mathbb{E}(Y_i) = b'(\theta_i) = \theta_i, \qquad \text{var}(Y_i) = a_i(\phi)b''(\theta_i) = \phi.
$$
The [variance function]{.blue} $v(\mu_i) = 1$ is constant. We will write $Y_i \sim \text{ED}(\mu_i, \phi)$ with $\mu_i \in \mathcal{M} = \mathbb{R}$. 

## Poisson distribution ðŸ“–

Let $Y_i \sim \text{Poisson}(\mu_i)$. The [pdf]{.blue} function of $Y_i$ can be written as
$$
\begin{aligned}
p(y_i; \mu_i) &= \frac{\mu_i^{y_i} e^{-\mu_i}}{y_i!}=\exp\{y_i \log(\mu_i) - \mu_i - \log(y_i!)\} \\
&=\exp\{y_i \theta_i - e^{\theta_i} - \log(y_i!)\}, \qquad y_i = 0, 1, 2,\dots.
\end{aligned}
$$

- Then, we can recognize the following relationships:
$$
\begin{aligned}
\theta_i &= \theta(\mu_i) = \log(\mu_i), \quad &&a_i(\phi) = 1, \\
b(\theta_i) &= e^{\theta_i}, \quad &&c(y_i, \phi) = c(y_i) = -\log(y_i!).
\end{aligned}
$$
There is [no dispersion]{.orange} parameter since $a_i(\phi) = 1$. 

- Using the results we previously discussed, we obtain the well-known relationships
$$
\begin{aligned}
\mathbb{E}(Y_i) &= b'(\theta_i) = e^{\theta_i} = \mu_i, \\
\text{var}(Y_i) &= a_i(\phi) b''(\theta_i) = e^{\theta_i} = \mu_i.
\end{aligned}
$$
The [variance function]{.blue} $v(\mu_i) = \mu_i$ is [linear]{.blue}. We will write $Y_i \sim \text{ED}(\mu_i, \mu_i)$ with $\mu_i \in (0, \infty)$. 

## Gamma distribution ðŸ“–

## Binomial distribution ðŸ“–

## A summary of exponential dispersion families

## Link functions and canonical link

# Likelihood, inference, and testing

## Likelihood function

## Sufficient statistics

## Likelihood equations I

## Likelihood equations II

## Likelihood equations III

## Example: Poisson loglinear model

## Observed and expected information I

## Observed and expected information II

## Asymptotic distribution of $\hat{\beta}$

## Wald confidence intervals for $\beta$

## Delta methods and fitted values

# IRLS algorithm

# Deviance, model checking, residuals

# Model selection

## References {.unnumbered}

::: {#refs}
:::
