<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tommaso Rigon">

<title>Generalized Linear Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="un_B_files/libs/clipboard/clipboard.min.js"></script>
<script src="un_B_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="un_B_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="un_B_files/libs/quarto-html/popper.min.js"></script>
<script src="un_B_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="un_B_files/libs/quarto-html/anchor.min.js"></script>
<link href="un_B_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="un_B_files/libs/quarto-html/quarto-syntax-highlighting-2dd8675203c32411a5a6bac4cfca86ce.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="un_B_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="un_B_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="un_B_files/libs/bootstrap/bootstrap-f461a50d3c54b5bcd96bf8a6f9e0f9b8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-full">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#homepage" id="toc-homepage" class="nav-link active" data-scroll-target="#homepage">Homepage</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#preliminaries" id="toc-preliminaries" class="nav-link" data-scroll-target="#preliminaries">Preliminaries</a></li>
  <li><a href="#beetles-data-from-bliss-1935" id="toc-beetles-data-from-bliss-1935" class="nav-link" data-scroll-target="#beetles-data-from-bliss-1935"><code>Beetles</code> data, from Bliss (1935)</a></li>
  <li><a href="#beetles-data-a-dose-response-plot" id="toc-beetles-data-a-dose-response-plot" class="nav-link" data-scroll-target="#beetles-data-a-dose-response-plot"><code>Beetles</code> data, a dose-response plot</a></li>
  <li><a href="#modelling-the-beetles-data" id="toc-modelling-the-beetles-data" class="nav-link" data-scroll-target="#modelling-the-beetles-data">Modelling the <code>Beetles</code> data</a></li>
  <li><a href="#beetles-data-fitted-model" id="toc-beetles-data-fitted-model" class="nav-link" data-scroll-target="#beetles-data-fitted-model"><code>Beetles</code> data, fitted model</a></li>
  <li><a href="#a-comparison-with-old-tools-i" id="toc-a-comparison-with-old-tools-i" class="nav-link" data-scroll-target="#a-comparison-with-old-tools-i">A comparison with old tools I</a></li>
  <li><a href="#a-comparison-with-old-tools-ii" id="toc-a-comparison-with-old-tools-ii" class="nav-link" data-scroll-target="#a-comparison-with-old-tools-ii">A comparison with old tools II</a></li>
  <li><a href="#a-comparison-with-old-tools-iii" id="toc-a-comparison-with-old-tools-iii" class="nav-link" data-scroll-target="#a-comparison-with-old-tools-iii">A comparison with old tools III</a></li>
  <li><a href="#aids-data" id="toc-aids-data" class="nav-link" data-scroll-target="#aids-data"><code>Aids</code> data</a></li>
  <li><a href="#aids-data-scatter-plot" id="toc-aids-data-scatter-plot" class="nav-link" data-scroll-target="#aids-data-scatter-plot"><code>Aids</code> data, scatter plot</a></li>
  <li><a href="#modelling-the-aids-data" id="toc-modelling-the-aids-data" class="nav-link" data-scroll-target="#modelling-the-aids-data">Modelling the <code>Aids</code> data</a></li>
  <li><a href="#aids-data-fitted-model" id="toc-aids-data-fitted-model" class="nav-link" data-scroll-target="#aids-data-fitted-model"><code>Aids</code> data, fitted model</a></li>
  <li><a href="#a-comparison-with-old-tools-i-1" id="toc-a-comparison-with-old-tools-i-1" class="nav-link" data-scroll-target="#a-comparison-with-old-tools-i-1">A comparison with old tools I</a></li>
  <li><a href="#a-comparison-with-old-tools-ii-1" id="toc-a-comparison-with-old-tools-ii-1" class="nav-link" data-scroll-target="#a-comparison-with-old-tools-ii-1">A comparison with old tools II</a></li>
  <li><a href="#the-components-of-a-glm" id="toc-the-components-of-a-glm" class="nav-link" data-scroll-target="#the-components-of-a-glm">The components of a GLM</a></li>
  <li><a href="#random-component-of-a-glm" id="toc-random-component-of-a-glm" class="nav-link" data-scroll-target="#random-component-of-a-glm">Random component of a GLM</a></li>
  <li><a href="#notable-examples" id="toc-notable-examples" class="nav-link" data-scroll-target="#notable-examples">Notable examples</a></li>
  </ul></li>
  <li><a href="#exponential-dispersion-families" id="toc-exponential-dispersion-families" class="nav-link" data-scroll-target="#exponential-dispersion-families">Exponential dispersion families</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#exponential-dispersion-family-definition" id="toc-exponential-dispersion-family-definition" class="nav-link" data-scroll-target="#exponential-dispersion-family-definition">Exponential dispersion family: definition</a></li>
  <li><a href="#mean-and-variance-i" id="toc-mean-and-variance-i" class="nav-link" data-scroll-target="#mean-and-variance-i">Mean and variance I 📖</a></li>
  <li><a href="#mean-and-variance-ii" id="toc-mean-and-variance-ii" class="nav-link" data-scroll-target="#mean-and-variance-ii">Mean and variance II 📖</a></li>
  <li><a href="#mean-parametrization-variance-function" id="toc-mean-parametrization-variance-function" class="nav-link" data-scroll-target="#mean-parametrization-variance-function">Mean parametrization, variance function</a></li>
  <li><a href="#gaussian-distribution" id="toc-gaussian-distribution" class="nav-link" data-scroll-target="#gaussian-distribution">Gaussian distribution 📖</a></li>
  <li><a href="#poisson-distribution" id="toc-poisson-distribution" class="nav-link" data-scroll-target="#poisson-distribution">Poisson distribution 📖</a></li>
  <li><a href="#gamma-distribution-i" id="toc-gamma-distribution-i" class="nav-link" data-scroll-target="#gamma-distribution-i">Gamma distribution I 📖</a></li>
  <li><a href="#gamma-distribution-ii" id="toc-gamma-distribution-ii" class="nav-link" data-scroll-target="#gamma-distribution-ii">Gamma distribution II 📖</a></li>
  <li><a href="#binomial-distribution-i" id="toc-binomial-distribution-i" class="nav-link" data-scroll-target="#binomial-distribution-i">Binomial distribution I 📖</a></li>
  <li><a href="#binomial-distribution-ii" id="toc-binomial-distribution-ii" class="nav-link" data-scroll-target="#binomial-distribution-ii">Binomial distribution II 📖</a></li>
  <li><a href="#notable-exponential-dispersion-families" id="toc-notable-exponential-dispersion-families" class="nav-link" data-scroll-target="#notable-exponential-dispersion-families">Notable exponential dispersion families</a></li>
  <li><a href="#link-functions-and-canonical-link" id="toc-link-functions-and-canonical-link" class="nav-link" data-scroll-target="#link-functions-and-canonical-link">Link functions and canonical link</a></li>
  </ul></li>
  <li><a href="#likelihood-quantities" id="toc-likelihood-quantities" class="nav-link" data-scroll-target="#likelihood-quantities">Likelihood quantities</a>
  <ul class="collapse">
  <li><a href="#likelihood-function" id="toc-likelihood-function" class="nav-link" data-scroll-target="#likelihood-function">Likelihood function</a></li>
  <li><a href="#likelihood-equations-i" id="toc-likelihood-equations-i" class="nav-link" data-scroll-target="#likelihood-equations-i">Likelihood equations I 📖</a></li>
  <li><a href="#likelihood-equations-ii" id="toc-likelihood-equations-ii" class="nav-link" data-scroll-target="#likelihood-equations-ii">Likelihood equations II 📖</a></li>
  <li><a href="#likelihood-equations-iii" id="toc-likelihood-equations-iii" class="nav-link" data-scroll-target="#likelihood-equations-iii">Likelihood equations III 📖</a></li>
  <li><a href="#likelihood-equations-iv" id="toc-likelihood-equations-iv" class="nav-link" data-scroll-target="#likelihood-equations-iv">Likelihood equations IV 📖</a></li>
  <li><a href="#canonical-link-simplifications" id="toc-canonical-link-simplifications" class="nav-link" data-scroll-target="#canonical-link-simplifications">Canonical link: simplifications 📖</a></li>
  <li><a href="#examples-of-estimating-equations" id="toc-examples-of-estimating-equations" class="nav-link" data-scroll-target="#examples-of-estimating-equations">Examples of estimating equations</a></li>
  <li><a href="#example-beetles-data" id="toc-example-beetles-data" class="nav-link" data-scroll-target="#example-beetles-data">Example: <code>Beetles</code> data</a></li>
  <li><a href="#example-beetles-data-1" id="toc-example-beetles-data-1" class="nav-link" data-scroll-target="#example-beetles-data-1">Example: <code>Beetles</code> data</a></li>
  <li><a href="#example-aids-data" id="toc-example-aids-data" class="nav-link" data-scroll-target="#example-aids-data">Example: <code>Aids</code> data</a></li>
  <li><a href="#example-aids-data-1" id="toc-example-aids-data-1" class="nav-link" data-scroll-target="#example-aids-data-1">Example: <code>Aids</code> data</a></li>
  <li><a href="#observed-and-expected-information-i" id="toc-observed-and-expected-information-i" class="nav-link" data-scroll-target="#observed-and-expected-information-i">Observed and expected information I 📖</a></li>
  <li><a href="#observed-and-expected-information-ii" id="toc-observed-and-expected-information-ii" class="nav-link" data-scroll-target="#observed-and-expected-information-ii">Observed and expected information II 📖</a></li>
  <li><a href="#canonical-link-simplifications-1" id="toc-canonical-link-simplifications-1" class="nav-link" data-scroll-target="#canonical-link-simplifications-1">Canonical link: simplifications 📖</a></li>
  <li><a href="#further-considerations" id="toc-further-considerations" class="nav-link" data-scroll-target="#further-considerations">Further considerations</a></li>
  <li><a href="#orthogonality-of-beta-and-psi" id="toc-orthogonality-of-beta-and-psi" class="nav-link" data-scroll-target="#orthogonality-of-beta-and-psi">☠️ - Orthogonality of <span class="math inline">\beta</span> and <span class="math inline">\psi</span></a></li>
  </ul></li>
  <li><a href="#irls-algorithm" id="toc-irls-algorithm" class="nav-link" data-scroll-target="#irls-algorithm">IRLS algorithm</a>
  <ul class="collapse">
  <li><a href="#numerical-methods-for-maximum-likelihood-estimation" id="toc-numerical-methods-for-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#numerical-methods-for-maximum-likelihood-estimation">Numerical methods for maximum likelihood estimation</a></li>
  <li><a href="#newton-raphson-algorithm-i" id="toc-newton-raphson-algorithm-i" class="nav-link" data-scroll-target="#newton-raphson-algorithm-i">Newton-Raphson algorithm I</a></li>
  <li><a href="#newton-raphson-algorithm-ii" id="toc-newton-raphson-algorithm-ii" class="nav-link" data-scroll-target="#newton-raphson-algorithm-ii">Newton-Raphson algorithm II</a></li>
  <li><a href="#iteratively-re-weighted-least-squares-i" id="toc-iteratively-re-weighted-least-squares-i" class="nav-link" data-scroll-target="#iteratively-re-weighted-least-squares-i">Iteratively re-weighted least squares I 📖</a></li>
  <li><a href="#iteratively-re-weighted-least-squares-ii" id="toc-iteratively-re-weighted-least-squares-ii" class="nav-link" data-scroll-target="#iteratively-re-weighted-least-squares-ii">Iteratively re-weighted least squares II 📖</a></li>
  <li><a href="#iteratively-re-weighted-least-squares-iii" id="toc-iteratively-re-weighted-least-squares-iii" class="nav-link" data-scroll-target="#iteratively-re-weighted-least-squares-iii">Iteratively re-weighted least squares III 📖</a></li>
  <li><a href="#example-irls-for-logistic-regression" id="toc-example-irls-for-logistic-regression" class="nav-link" data-scroll-target="#example-irls-for-logistic-regression">Example: IRLS for logistic regression</a></li>
  <li><a href="#estimation-of-the-dispersion-phi" id="toc-estimation-of-the-dispersion-phi" class="nav-link" data-scroll-target="#estimation-of-the-dispersion-phi">Estimation of the dispersion <span class="math inline">\phi</span></a></li>
  </ul></li>
  <li><a href="#inference-and-hypothesis-testing" id="toc-inference-and-hypothesis-testing" class="nav-link" data-scroll-target="#inference-and-hypothesis-testing">Inference and hypothesis testing</a>
  <ul class="collapse">
  <li><a href="#asymptotic-distribution-of-hatbeta" id="toc-asymptotic-distribution-of-hatbeta" class="nav-link" data-scroll-target="#asymptotic-distribution-of-hatbeta">Asymptotic distribution of <span class="math inline">\hat{\beta}</span></a></li>
  <li><a href="#example-beetles-data-2" id="toc-example-beetles-data-2" class="nav-link" data-scroll-target="#example-beetles-data-2">Example: <code>Beetles</code> data</a></li>
  <li><a href="#example-aids-data-2" id="toc-example-aids-data-2" class="nav-link" data-scroll-target="#example-aids-data-2">Example: <code>Aids</code> data</a></li>
  <li><a href="#wald-test-and-confidence-intervals" id="toc-wald-test-and-confidence-intervals" class="nav-link" data-scroll-target="#wald-test-and-confidence-intervals">Wald test and confidence intervals</a></li>
  <li><a href="#comparison-with-the-gaussian-linear-model" id="toc-comparison-with-the-gaussian-linear-model" class="nav-link" data-scroll-target="#comparison-with-the-gaussian-linear-model">Comparison with the Gaussian linear model</a></li>
  <li><a href="#example-beetles-data-3" id="toc-example-beetles-data-3" class="nav-link" data-scroll-target="#example-beetles-data-3">Example: <code>Beetles</code> data</a></li>
  <li><a href="#example-aids-data-3" id="toc-example-aids-data-3" class="nav-link" data-scroll-target="#example-aids-data-3">Example: <code>Aids</code> data</a></li>
  <li><a href="#general-hypothesis-testing" id="toc-general-hypothesis-testing" class="nav-link" data-scroll-target="#general-hypothesis-testing">General hypothesis testing</a></li>
  <li><a href="#testing-hypothesis-in-glms-i" id="toc-testing-hypothesis-in-glms-i" class="nav-link" data-scroll-target="#testing-hypothesis-in-glms-i">Testing hypothesis in GLMs I</a></li>
  <li><a href="#log-likelihood-ratio-test" id="toc-log-likelihood-ratio-test" class="nav-link" data-scroll-target="#log-likelihood-ratio-test">Log-likelihood ratio test</a></li>
  <li><a href="#score-or-rao-test" id="toc-score-or-rao-test" class="nav-link" data-scroll-target="#score-or-rao-test">Score or Rao test</a></li>
  <li><a href="#a-graphical-representation-when-p-1" id="toc-a-graphical-representation-when-p-1" class="nav-link" data-scroll-target="#a-graphical-representation-when-p-1">A graphical representation when <span class="math inline">p = 1</span></a></li>
  <li><a href="#three-asymptotically-equivalent-tests" id="toc-three-asymptotically-equivalent-tests" class="nav-link" data-scroll-target="#three-asymptotically-equivalent-tests">Three asymptotically equivalent tests</a></li>
  <li><a href="#comparison-with-the-gaussian-linear-model-1" id="toc-comparison-with-the-gaussian-linear-model-1" class="nav-link" data-scroll-target="#comparison-with-the-gaussian-linear-model-1">Comparison with the Gaussian linear model</a></li>
  <li><a href="#example-beetles-data-4" id="toc-example-beetles-data-4" class="nav-link" data-scroll-target="#example-beetles-data-4">Example: <code>Beetles</code> data</a></li>
  <li><a href="#example-beetles-data-5" id="toc-example-beetles-data-5" class="nav-link" data-scroll-target="#example-beetles-data-5">Example: <code>Beetles</code> data</a></li>
  <li><a href="#example-aids-data-4" id="toc-example-aids-data-4" class="nav-link" data-scroll-target="#example-aids-data-4">Example: <code>Aids</code> data</a></li>
  <li><a href="#example-aids-data-5" id="toc-example-aids-data-5" class="nav-link" data-scroll-target="#example-aids-data-5">Example: <code>Aids</code> data</a></li>
  <li><a href="#example-aids-data-6" id="toc-example-aids-data-6" class="nav-link" data-scroll-target="#example-aids-data-6">Example: <code>Aids</code> data</a></li>
  </ul></li>
  <li><a href="#deviance-model-checking-residuals" id="toc-deviance-model-checking-residuals" class="nav-link" data-scroll-target="#deviance-model-checking-residuals">Deviance, model checking, residuals</a>
  <ul class="collapse">
  <li><a href="#deviance-some-intuitions" id="toc-deviance-some-intuitions" class="nav-link" data-scroll-target="#deviance-some-intuitions">Deviance: some intuitions</a></li>
  <li><a href="#example-beetles-data-saturated-model" id="toc-example-beetles-data-saturated-model" class="nav-link" data-scroll-target="#example-beetles-data-saturated-model">Example: <code>Beetles</code> data, saturated model</a></li>
  <li><a href="#saturated-model" id="toc-saturated-model" class="nav-link" data-scroll-target="#saturated-model">Saturated model</a></li>
  <li><a href="#deviance" id="toc-deviance" class="nav-link" data-scroll-target="#deviance">Deviance</a></li>
  <li><a href="#deviance-and-log-likelihood-ratio-test" id="toc-deviance-and-log-likelihood-ratio-test" class="nav-link" data-scroll-target="#deviance-and-log-likelihood-ratio-test">Deviance and log-likelihood ratio test</a></li>
  <li><a href="#the-null-model" id="toc-the-null-model" class="nav-link" data-scroll-target="#the-null-model">The null model</a></li>
  <li><a href="#pearson-x2-statistic" id="toc-pearson-x2-statistic" class="nav-link" data-scroll-target="#pearson-x2-statistic">Pearson <span class="math inline">X^2</span> statistic</a></li>
  <li><a href="#deviance-of-a-gaussian-linear-model" id="toc-deviance-of-a-gaussian-linear-model" class="nav-link" data-scroll-target="#deviance-of-a-gaussian-linear-model">Deviance of a Gaussian linear model</a></li>
  <li><a href="#deviance-of-a-poisson-model" id="toc-deviance-of-a-poisson-model" class="nav-link" data-scroll-target="#deviance-of-a-poisson-model">Deviance of a Poisson model</a></li>
  <li><a href="#deviance-of-a-binomial-model-i" id="toc-deviance-of-a-binomial-model-i" class="nav-link" data-scroll-target="#deviance-of-a-binomial-model-i">Deviance of a binomial model I</a></li>
  <li><a href="#deviance-of-a-binomial-model-ii" id="toc-deviance-of-a-binomial-model-ii" class="nav-link" data-scroll-target="#deviance-of-a-binomial-model-ii">Deviance of a binomial model II</a></li>
  <li><a href="#deviance-as-goodness-of-fit-measure-i" id="toc-deviance-as-goodness-of-fit-measure-i" class="nav-link" data-scroll-target="#deviance-as-goodness-of-fit-measure-i">Deviance as goodness of fit measure I</a></li>
  <li><a href="#deviance-as-goodness-of-fit-measure-ii" id="toc-deviance-as-goodness-of-fit-measure-ii" class="nav-link" data-scroll-target="#deviance-as-goodness-of-fit-measure-ii">Deviance as goodness of fit measure II</a></li>
  <li><a href="#on-pseudo-r2" id="toc-on-pseudo-r2" class="nav-link" data-scroll-target="#on-pseudo-r2">On pseudo-<span class="math inline">R^2</span></a></li>
  <li><a href="#example-beetles-data-output-of-summary" id="toc-example-beetles-data-output-of-summary" class="nav-link" data-scroll-target="#example-beetles-data-output-of-summary">Example: <code>Beetles</code> data, output of <code>summary</code></a></li>
  <li><a href="#example-beetles-data-output-of-anova" id="toc-example-beetles-data-output-of-anova" class="nav-link" data-scroll-target="#example-beetles-data-output-of-anova">Example: <code>Beetles</code> data, output of <code>anova</code></a></li>
  <li><a href="#example-beetles-data-goodness-of-fit" id="toc-example-beetles-data-goodness-of-fit" class="nav-link" data-scroll-target="#example-beetles-data-goodness-of-fit">Example: <code>Beetles</code> data, goodness of fit</a></li>
  <li><a href="#example-aids-data-output-of-summary" id="toc-example-aids-data-output-of-summary" class="nav-link" data-scroll-target="#example-aids-data-output-of-summary">Example: <code>Aids</code> data, output of <code>summary</code></a></li>
  <li><a href="#example-aids-data-output-of-anova-and-lrtest" id="toc-example-aids-data-output-of-anova-and-lrtest" class="nav-link" data-scroll-target="#example-aids-data-output-of-anova-and-lrtest">Example: <code>Aids</code> data, output of <code>anova</code> and <code>lrtest</code></a></li>
  <li><a href="#example-aids-data-goodness-of-fit" id="toc-example-aids-data-goodness-of-fit" class="nav-link" data-scroll-target="#example-aids-data-goodness-of-fit">Example: <code>Aids</code> data, goodness of fit</a></li>
  <li><a href="#residuals" id="toc-residuals" class="nav-link" data-scroll-target="#residuals">Residuals</a></li>
  <li><a href="#pearson-residuals" id="toc-pearson-residuals" class="nav-link" data-scroll-target="#pearson-residuals">Pearson residuals</a></li>
  <li><a href="#deviance-residuals" id="toc-deviance-residuals" class="nav-link" data-scroll-target="#deviance-residuals">Deviance residuals</a></li>
  <li><a href="#a-weighted-projection-matrix" id="toc-a-weighted-projection-matrix" class="nav-link" data-scroll-target="#a-weighted-projection-matrix">A weighted projection matrix</a></li>
  <li><a href="#standardized-residuals" id="toc-standardized-residuals" class="nav-link" data-scroll-target="#standardized-residuals">Standardized residuals</a></li>
  <li><a href="#on-q-q-plots-and-other-practicalities" id="toc-on-q-q-plots-and-other-practicalities" class="nav-link" data-scroll-target="#on-q-q-plots-and-other-practicalities">On Q-Q plots and other practicalities</a></li>
  <li><a href="#on-identifying-and-removing-outliers" id="toc-on-identifying-and-removing-outliers" class="nav-link" data-scroll-target="#on-identifying-and-removing-outliers">On identifying and removing outliers</a></li>
  <li><a href="#example-beetles-data-6" id="toc-example-beetles-data-6" class="nav-link" data-scroll-target="#example-beetles-data-6">Example: <code>Beetles</code> data</a></li>
  <li><a href="#example-beetles-data-7" id="toc-example-beetles-data-7" class="nav-link" data-scroll-target="#example-beetles-data-7">Example: <code>Beetles</code> data</a></li>
  <li><a href="#example-aids-data-7" id="toc-example-aids-data-7" class="nav-link" data-scroll-target="#example-aids-data-7">Example: <code>Aids</code> data</a></li>
  <li><a href="#example-aids-data-8" id="toc-example-aids-data-8" class="nav-link" data-scroll-target="#example-aids-data-8">Example: <code>Aids</code> data</a></li>
  </ul></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection">Model selection</a>
  <ul class="collapse">
  <li><a href="#model-selection-process" id="toc-model-selection-process" class="nav-link" data-scroll-target="#model-selection-process">Model selection process</a></li>
  <li><a href="#automatic-model-selection" id="toc-automatic-model-selection" class="nav-link" data-scroll-target="#automatic-model-selection">Automatic model selection</a></li>
  <li><a href="#stepwise-procedures-forward-and-backward-selection" id="toc-stepwise-procedures-forward-and-backward-selection" class="nav-link" data-scroll-target="#stepwise-procedures-forward-and-backward-selection">Stepwise procedures: forward and backward selection</a></li>
  <li><a href="#comments-on-forwardbackward-selection-i" id="toc-comments-on-forwardbackward-selection-i" class="nav-link" data-scroll-target="#comments-on-forwardbackward-selection-i">Comments on forward/backward selection I</a></li>
  <li><a href="#comments-on-forwardbackward-selection-ii" id="toc-comments-on-forwardbackward-selection-ii" class="nav-link" data-scroll-target="#comments-on-forwardbackward-selection-ii">Comments on forward/backward selection II</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="un_B_slides.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></nav>
</div>
<main class="content page-columns page-full column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Generalized Linear Models</h1>
<p class="subtitle lead">Statistics III - CdL SSE</p>
</div>


<div class="quarto-title-meta-author column-page-left">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><span class="orange">Tommaso Rigon</span> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <em>Università degli Studi di Milano-Bicocca</em>
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-page-left">

      
  
    
  </div>
  


</header>


<section id="homepage" class="level2">
<h2 class="anchored" data-anchor-id="homepage"><a href="../index.html">Homepage</a></h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><img src="img/gaussian.png" class="img-fluid"></p>
</div><div class="column" style="width:70%;">
<ul>
<li><p>This unit will cover the following <span class="orange">topics</span>:</p>
<ul>
<li>Exponential dispersion families</li>
<li>Likelihood, inference, and testing</li>
<li>Iteratively Re-weighted Least Squares (IRLS)</li>
<li>Deviance, model checking, and residuals</li>
<li>Model selection</li>
</ul></li>
<li><p>GLMs are regression models with a linear predictor, where the response variable follows an <span class="blue">exponential dispersion family</span>.</p></li>
<li><p>The symbol 📖 means that a few extra steps are discussed in the <span class="blue">handwritten notes</span>.</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The content of this Unit is covered in <span class="orange">Chapter 2</span> of <span class="citation" data-cites="Salvan2020">Salvan et al. (<a href="#ref-Salvan2020" role="doc-biblioref">2020</a>)</span>. Alternatively, see <span class="blue">Chapter 4</span> of <span class="citation" data-cites="Agresti2015">Agresti (<a href="#ref-Agresti2015" role="doc-biblioref">2015</a>)</span> or <span class="grey">Chapter 6</span> of <span class="citation" data-cites="Azzalini2008">Azzalini (<a href="#ref-Azzalini2008" role="doc-biblioref">2008</a>)</span>.</p>
</div>
</div>
</div>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<section id="preliminaries" class="level2">
<h2 class="anchored" data-anchor-id="preliminaries">Preliminaries</h2>
<ul>
<li><p>GLMs are a <span class="blue">class</span> of <span class="blue">regression models</span> in which a <span class="orange">response</span> random variable <span class="math inline">Y_i</span> is modeled as a function of a vector of <span class="grey">covariates</span> <span class="math inline">\bm{x}_i \in \mathbb{R}^p</span>.</p></li>
<li><p>The random variables <span class="math inline">Y_i</span> are not restricted to be Gaussian. For example:</p>
<ul>
<li><span class="math inline">Y_i \in \{0,1\}</span>, known as <span class="blue">binary regression</span><br>
</li>
<li><span class="math inline">Y_i \in \{0,1,\dots\}</span>, known as <span class="orange">count regression</span><br>
</li>
<li><span class="math inline">Y_i \in (0,\infty)</span> or <span class="math inline">Y_i \in (-\infty,\infty)</span></li>
</ul></li>
<li><p>Gaussian linear models are a special case of GLMs, arising when <span class="math inline">Y_i \in (-\infty,\infty)</span>.</p></li>
</ul>
<ul>
<li><p>The <span class="orange">response random variables</span> are collected in the random vector <span class="math inline">\bm{Y} = (Y_1,\dots,Y_n)^T</span>, whose <span class="blue">observed realization</span> is <span class="math inline">\bm{y} = (y_1,\dots,y_n)^T</span>.</p></li>
<li><p>The <span class="blue">design matrix</span> <span class="math inline">\bm{X}</span> is an <span class="math inline">n \times p</span> <span class="orange">non-stochastic</span> matrix containing the covariate values. The <span class="math inline">j</span>th variable (column) is denoted by <span class="math inline">\tilde{\bm{x}}_j</span>, while the <span class="math inline">i</span>th observation (row) is <span class="math inline">\bm{x}_i</span>.</p></li>
<li><p>We assume that <span class="math inline">\bm{X}</span> has <span class="orange">full rank</span>, that is, <span class="math inline">\text{rk}(\bm{X}) = p</span> with <span class="math inline">p \le n</span>.</p></li>
</ul>
</section>
<section id="beetles-data-from-bliss-1935" class="level2">
<h2 class="anchored" data-anchor-id="beetles-data-from-bliss-1935"><code>Beetles</code> data, from Bliss (1935)</h2>
<ul>
<li>The <code>Beetles</code> dataset originates from Bliss (1935). It records the number of adult flour beetles that died after a 5-hour exposure to gaseous carbon disulphide.</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">m</th>
<th style="text-align: right;">deaths</th>
<th style="text-align: right;">logdose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">59</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">1.6907</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">1.7242</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">1.7552</td>
</tr>
<tr class="even">
<td style="text-align: right;">56</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">1.7842</td>
</tr>
<tr class="odd">
<td style="text-align: right;">63</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">1.8113</td>
</tr>
<tr class="even">
<td style="text-align: right;">59</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">1.8369</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">1.8610</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">1.8839</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li><p>We aim to predict the proportion of <code>deaths</code> as a function of <code>logdose</code>.</p></li>
<li><p>Modeling death proportions directly with <span class="orange">linear models</span> is <span class="orange">inappropriate</span>. A <span class="blue">variable transformation</span> provides a more <span class="blue">principled</span> solution, but it comes with <span class="orange">drawbacks</span>.</p></li>
</ul>
</section>
<section id="beetles-data-a-dose-response-plot" class="level2">
<h2 class="anchored" data-anchor-id="beetles-data-a-dose-response-plot"><code>Beetles</code> data, a dose-response plot</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1950"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>There is a clear <span class="orange">positive</span> and <span class="blue">non-linear</span> pattern between the <span class="orange">proportion of deaths</span> as a function of the logdose. The response variable take values in <span class="math inline">[0, 1]</span>.</li>
</ul>
</section>
<section id="modelling-the-beetles-data" class="level2">
<h2 class="anchored" data-anchor-id="modelling-the-beetles-data">Modelling the <code>Beetles</code> data</h2>
<ul>
<li><p>Let <span class="math inline">S_i</span> be the number of dead beetles out of <span class="math inline">m_i</span>, and let <span class="math inline">x_i</span> denote the log-dose. By definition, <span class="math inline">S_i \in \{0, 1, \dots, m_i\}</span> for <span class="math inline">i = 1,\dots,8</span>.</p></li>
<li><p>It is natural to model each <span class="math inline">S_i</span> as <span class="blue">independent binomial</span> random variables, counting the number of deaths out of <span class="math inline">m_i</span> individuals. In other words: <span class="math display">
S_i \overset{\text{ind}}{\sim} \text{Binomial}(m_i, \pi_i), \qquad i = 1,\dots,8,
</span> where <span class="math inline">\pi_i</span> is the <span class="orange">probability</span> of death at a given dose <span class="math inline">x_i</span>. Moreover, et <span class="math inline">Y_i = S_i / m_i</span> be the proportion of deaths, then: <span class="math display">
\mathbb{E}(Y_i) = \mathbb{E}\left(\frac{S_i}{m_i}\right)  = \pi_i = \mu_i.
</span></p></li>
<li><p>A modeling approach, called <span class="blue">logistic regression</span>, specifies:<br>
<span class="math display">
g(\pi_i) = \log\left(\frac{\pi_i}{1 - \pi_i}\right) = \beta_1 + \beta_2 x_i \quad \Longrightarrow \quad \pi_i = g^{-1}(\beta_1 + \beta_2 x_i) = \frac{\exp(\beta_1 + \beta_2 x_i)}{1 + \exp(\beta_1 + \beta_2 x_i)}.
</span> for some parameters <span class="math inline">\beta_1, \beta_2 \in \mathbb{R}</span>. <span class="orange">Note</span> that <span class="math inline">\pi_i \in (0, 1)</span> by construction.</p></li>
</ul>
</section>
<section id="beetles-data-fitted-model" class="level2">
<h2 class="anchored" data-anchor-id="beetles-data-fitted-model"><code>Beetles</code> data, fitted model</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1950"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>The <span class="orange">maximum likelihood</span> estimates are <span class="math inline">\hat{\beta}_1 = -60.72</span> and <span class="math inline">\hat{\beta}_2 = 34.3</span>. This yields the <span class="blue">predictive curve</span> <span class="math inline">\hat{\pi}(x) = g^{-1}(\hat{\beta}_1 + \hat{\beta}_2 x),</span> which estimates the mean proportion <span class="math inline">\mathbb{E}(S_i / m_i)</span>.</li>
</ul>
</section>
<section id="a-comparison-with-old-tools-i" class="level2">
<h2 class="anchored" data-anchor-id="a-comparison-with-old-tools-i">A comparison with old tools I</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">Y_i = S_i / m_i</span> be the proportion of deaths. A direct application of linear models implies: <span class="math display">
Y_i = \beta_1 + \beta_2 x_i + \epsilon_i.
</span> The coefficients <span class="math inline">\beta_1</span> and <span class="math inline">\beta_2</span> are then estimated using OLS using <span class="math inline">Y_i</span> as response.</p>
</div>
</div>
</div>
<ul>
<li><p>The prediction <span class="math inline">\hat{\beta}_1 + \hat{\beta}_2 x_i</span> is <span class="orange">unrestricted</span>, meaning it could produce values like “1.3” or “-2” as estimated <span class="blue">proportions</span>, which is clearly undesirable.</p></li>
<li><p>The <span class="blue">additive structure</span> <span class="math inline">Y_i = \beta_1 + \beta_2 x_i + \epsilon_i</span> cannot hold with <span class="blue">iid</span> errors <span class="math inline">\epsilon_i</span>, because <span class="math inline">S_i</span>, and thus <span class="math inline">Y_i</span>, are <span class="orange">discrete</span>. As a result, the errors are always <span class="orange">heteroschedastic</span>.</p></li>
<li><p>If <span class="math inline">m_i = 1</span>, i.e.&nbsp;when the data are <span class="orange">binary</span>, all the above issues are <span class="orange">exacerbated</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>This approach is sometimes called the <span class="blue">linear probability model</span>. Before GLMs, it was considered acceptable despite its issues, but by modern standards it should <span class="orange">not be used</span>.</p>
</div>
</div>
</div>
</section>
<section id="a-comparison-with-old-tools-ii" class="level2">
<h2 class="anchored" data-anchor-id="a-comparison-with-old-tools-ii">A comparison with old tools II</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>We consider the <span class="blue">empirical logit</span> variable transformation of <span class="math inline">S_i = Y_i / m_i</span>, obtaining<br>
<span class="math display">
\text{logit}(\tilde{Y}_i) = \log\left(\frac{S_i + 0.5}{m_i - S_i + 0.5}\right) = \beta_1 + \beta_2 x_i + \epsilon_i, \qquad \tilde{Y}_i = \frac{S_i + 0.5}{m_i +1}.
</span> A correction term is necessary because otherwise <span class="math inline">g(\cdot) = \text{logit}(\cdot)</span> is undefined. The predictions belong to <span class="math inline">(0, 1)</span>, since <span class="math display">
\hat{\pi}_i = g^{-1}[\mathbb{E}\{g(\tilde{Y}_i)\}] = g^{-1}(\hat{\beta}_1 + \hat{\beta}_2 x_i) = \frac{\exp(\hat{\beta}_1 + \hat{\beta}_2 x_i)}{1 + \exp(\hat{\beta}_1 + \hat{\beta}_2 x_i)},</span> in which <span class="math inline">\hat{\beta}_1</span> and <span class="math inline">\hat{\beta}_2</span> are estimated with OLS using <span class="math inline">\text{logit}(\tilde{Z}_i)</span> as response.</p>
</div>
</div>
</div>
<ul>
<li><p>The <span class="blue">interpretation</span> of <span class="math inline">\hat{\beta}</span> is less clear, as they refer to the mean of <span class="math inline">\text{logit}(\tilde{Y}_i)</span> instead of <span class="math inline">\mathbb{E}(Y_i)</span>.</p></li>
<li><p>An arbitrary <span class="orange">boundary correction</span> is needed.</p></li>
<li><p>Inference is problematic and requires further corrections, because of <span class="orange">heteroschedastic</span> errors.</p></li>
<li><p>This approach is <span class="orange">not compatible</span> with the reasonable assumption <span class="math inline">S_i \sim \text{Binomial}(m_i, \pi_i)</span>.</p></li>
</ul>
</section>
<section id="a-comparison-with-old-tools-iii" class="level2">
<h2 class="anchored" data-anchor-id="a-comparison-with-old-tools-iii">A comparison with old tools III</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1950"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>The black line is the predicted curve of a <span class="grey">logistic regression GLM</span>. The orange line is the predictived curve of a <span class="orange">linear model</span>. The blue line is the predictive curve of a <span class="blue">linear model</span> after an <span class="blue">empirical logit variable transformation</span>.</li>
</ul>
</section>
<section id="aids-data" class="level2">
<h2 class="anchored" data-anchor-id="aids-data"><code>Aids</code> data</h2>
<ul>
<li>Number of AIDS <code>deaths</code> in Australia in a sequence of three-months periods between 1983 and 1986.</li>
</ul>
<div class="smaller">
<div class="cell">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">1983-1</th>
<th style="text-align: right;">1984-1</th>
<th style="text-align: right;">1985-1</th>
<th style="text-align: right;">1986-1</th>
<th style="text-align: right;">1983-2</th>
<th style="text-align: right;">1984-2</th>
<th style="text-align: right;">1985-2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">deaths</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="even">
<td style="text-align: left;">period</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">7</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">1986-2</th>
<th style="text-align: right;">1983-3</th>
<th style="text-align: right;">1984-3</th>
<th style="text-align: right;">1985-3</th>
<th style="text-align: right;">1986-3</th>
<th style="text-align: right;">1983-4</th>
<th style="text-align: right;">1984-4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">deaths</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">23</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">37</td>
<td style="text-align: right;">45</td>
</tr>
<tr class="even">
<td style="text-align: left;">period</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">14</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<ul>
<li><p>We are interested in predicting the number of <code>deaths</code> as a function of the <code>period</code> of time.</p></li>
<li><p>The response variable <span class="math inline">Y_i \in \{0, 1, \dots\}</span> is a non-negative <span class="orange">count</span>.</p></li>
</ul>
</section>
<section id="aids-data-scatter-plot" class="level2">
<h2 class="anchored" data-anchor-id="aids-data-scatter-plot"><code>Aids</code> data, scatter plot</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1950"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>There is a clear positive association between period and deaths. However, the increase appears to be <span class="orange">faster</span> than <span class="orange">linear</span>. Note that both the mean and the <span class="blue">variability</span> of <span class="math inline">Y_i</span> increase over time.</li>
</ul>
</section>
<section id="modelling-the-aids-data" class="level2">
<h2 class="anchored" data-anchor-id="modelling-the-aids-data">Modelling the <code>Aids</code> data</h2>
<ul>
<li>Let <span class="math inline">Y_i</span> be the number of deaths, and let <span class="math inline">x_i</span> denote the period. By definition, <span class="math inline">Y_i \in \{0, 1, \dots\}</span> are non-negative counts, for <span class="math inline">i = 1,\dots,14</span>.</li>
</ul>
<ul>
<li><p>We model <span class="math inline">Y_i</span> as <span class="blue">independent Poisson</span> random variables, counting the number of deaths: <span class="math display">
Y_i \overset{\text{ind}}{\sim} \text{Poisson}(\mu_i), \qquad i = 1,\dots,14,
</span> where <span class="math inline">\mu_i</span> is the <span class="orange">mean</span> of <span class="math inline">Y_i</span>, namely <span class="math inline">\mathbb{E}(Y_i) = \mu_i</span>.</p></li>
<li><p>A modeling approach, called <span class="blue">Poisson regression</span>, specifies:<br>
<span class="math display">
g(\mu_i) = \log(\mu_i) = \beta_1 + \beta_2 x_i \quad \Longrightarrow \quad \mu_i = g^{-1}(\beta_1 + \beta_2 x_i) = \exp(\beta_1 + \beta_2 x_i),
</span> for some parameters <span class="math inline">\beta_1, \beta_2 \in \mathbb{R}</span>. <span class="orange">Note</span> that <span class="math inline">\mu_i &gt; 0</span> by construction.</p></li>
<li><p>Under this specification, the <span class="blue">variances</span> of the observations are<br>
<span class="math display">
\text{var}(Y_i) = \mu_i = \exp(\beta_1 + \beta_2 x_i),
</span> which increases with <span class="math inline">x</span>, as desired. This implies that <span class="math inline">Y_1,\dots,Y_n</span> are <span class="orange">heteroschedastic</span>, but this is not an issue in GLMs, as this aspect is <span class="blue">automatically accounted</span> for.</p></li>
</ul>
</section>
<section id="aids-data-fitted-model" class="level2">
<h2 class="anchored" data-anchor-id="aids-data-fitted-model"><code>Aids</code> data, fitted model</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1950"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>The <span class="orange">maximum likelihood</span> estimates are <span class="math inline">\hat{\beta}_1 = 0.304</span> and <span class="math inline">\hat{\beta}_2 = 0.259</span>. This yields the <span class="blue">predictive curve</span> <span class="math inline">\hat{\mu}(x) = \exp(\hat{\beta}_1 + \hat{\beta}_2 x),</span> which estimates the mean <span class="math inline">\mathbb{E}(Y_i)</span>.</li>
</ul>
</section>
<section id="a-comparison-with-old-tools-i-1" class="level2">
<h2 class="anchored" data-anchor-id="a-comparison-with-old-tools-i-1">A comparison with old tools I</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>We consider the <span class="blue">variance-stabilizing</span> transformation <span class="math inline">S_i = \sqrt{Y_i}</span>, obtaining<br>
<span class="math display">
\sqrt{Y_i} = \beta_1 + \beta_2 x_i + \epsilon_i.
</span> The predictions belong to <span class="math inline">(0, \infty)</span>, since <span class="math display">
\hat{\mu}_i = \mathbb{E}(\sqrt{Y_i})^2 = (\hat{\beta}_1 + \hat{\beta}_2 x_i)^2,</span> in which <span class="math inline">\hat{\beta}_1</span> and <span class="math inline">\hat{\beta}_2</span> are estimated with OLS using <span class="math inline">\sqrt{Y_i}</span> as response.</p>
</div>
</div>
</div>
<ul>
<li><p>The <span class="blue">interpretation</span> of <span class="math inline">\hat{\beta}</span> is less clear, as they refer to the mean of <span class="math inline">\sqrt{Y}_i</span> instead of <span class="math inline">\mathbb{E}(Y_i)</span>.</p></li>
<li><p>This approach is <span class="orange">not compatible</span> with the reasonable assumption <span class="math inline">Y_i \sim \text{Poisson}(\mu_i)</span> and it only valid as an <span class="blue">asymptotic approximation</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>To compare such a model with a similar specification, we also fit another Poisson GLM in which <span class="math display">
Y_i \overset{\text{ind}}{\sim} \text{Poisson}(\mu_i), \qquad \sqrt{\mu_i} = \beta_1 + \beta_2 x_i, \qquad i=1,\dots,14.
</span></p>
</div>
</div>
</div>
</section>
<section id="a-comparison-with-old-tools-ii-1" class="level2">
<h2 class="anchored" data-anchor-id="a-comparison-with-old-tools-ii-1">A comparison with old tools II</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1950"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>The black line is the predicted curve of a <span class="grey">Poisson regression GLM</span> with <span class="gray">logarithmic link</span>. The orange line is the predicted curve of a <span class="orange">linear model</span> with a <span class="orange">square-root transformation</span>. The blue line is the predictive curve of a <span class="blue">Poisson regression GLM</span> with <span class="blue">square-root link</span>.</li>
</ul>
</section>
<section id="the-components-of-a-glm" class="level2">
<h2 class="anchored" data-anchor-id="the-components-of-a-glm">The components of a GLM</h2>
<ul>
<li><span class="orange">Random component</span>. This specifies the probability distribution response variable <span class="math inline">Y_i</span>. The observations <span class="math inline">\bm{y} =(y_1,\dots,y_n)</span> on that distribution are treated as <span class="orange">independent</span>.</li>
</ul>
<ul>
<li><span class="blue">Linear predictor</span>. For a parameter vector <span class="math inline">\bm{\beta} = (\beta_1,\dots,\beta_p)^T</span> and an <span class="math inline">n \times p</span> design matrix <span class="math inline">\bm{X}</span>, the linear predictor is <span class="math inline">\bm{\eta} = \bm{X}\beta</span>. We will also write <span class="math display">
\eta_i = \bm{x}_i^T\beta = x_{i1}\beta_1 + \cdots + x_{ip}\beta_p, \qquad i=1,\dots,n.
</span></li>
</ul>
<ul>
<li><span class="grey">Link function</span>. This is an invertible and differentiable function <span class="math inline">g(\cdot)</span> applied to each component of the <span class="grey">mean</span> <span class="math inline">\mu_i = \mathbb{E}(Y_i)</span> that relates it to the linear predictor: <span class="math display">
g(\mu_i) = \eta_i = \bm{x}_i^T\beta, \qquad \Longrightarrow \qquad \mu_i = g^{-1}(\eta_i) = g^{-1}(\bm{x}_i^T\beta).
</span></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Note that, in general, we <span class="orange">cannot</span> express the response in an additive way <span class="math inline">Y_i = g^{-1}(\eta_i) + \epsilon_i</span>.</p>
</div>
</div>
</div>
</section>
<section id="random-component-of-a-glm" class="level2">
<h2 class="anchored" data-anchor-id="random-component-of-a-glm">Random component of a GLM</h2>
<ul>
<li><p>In GLMs the random variables <span class="math inline">Y_i</span> are <span class="orange">independent</span> and they are distributed according to an <span class="blue">exponential dispersion family</span>, whose definition will be provided in a few slides.</p></li>
<li><p>The <span class="orange">distributions most commonly</span> used in Statistics, such as the normal, binomial, gamma, and Poisson, are exponential family distributions.</p></li>
<li><p>Exponential dispersion families are <span class="orange">characterized</span> by their <span class="blue">mean</span> and <span class="blue">variance</span>. Let <span class="math inline">v(\mu) &gt; 0</span> be a function of the mean, called <span class="blue">variance function</span> and let <span class="math inline">a_i(\phi) &gt;0</span> be functions of an additional unknown parameter <span class="math inline">\phi &gt; 0</span> called <span class="orange">dispersion</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In a GLMs the observations are independent draws from a distribution <span class="math inline">\text{ED}(\mu_i, a_i(\phi)v(\mu_i))</span>: <span class="math display">
Y_i \overset{\text{ind}}{\sim} \text{ED}(\mu_i, a_i(\phi)v(\mu_i)), \qquad \mathbb{E}(Y_i) = \mu_i, \qquad g(\mu_i) = \bm{x}_i^T\beta,
</span> with <span class="math inline">\mu_i \in \mathcal{M}</span>. Moreover, the <span class="orange">variance</span> is connected to the <span class="blue">mean</span> via <span class="math inline">v(\mu)</span>: <span class="math display">
\text{var}(Y_i) = a_i(\phi) v(\mu_i),
</span> where <span class="math inline">a_i(\phi) = \phi / \omega_i</span> and <span class="math inline">\omega_i</span> are <span class="blue">known weights</span>. Special cases are <span class="math inline">a_i(\phi) = \phi</span> and <span class="math inline">a_i(\phi) = 1</span>.</p>
</div>
</div>
</div>
</section>
<section id="notable-examples" class="level2">
<h2 class="anchored" data-anchor-id="notable-examples">Notable examples</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In a <span class="blue">Gaussian linear model</span> we consider the <span class="blue">identity link</span> <span class="math inline">g(\mu) = \mu</span> and let <span class="math display">
Y_i \overset{\text{ind}}{\sim}\text{N}(\mu_i, \sigma^2), \qquad \mu_i = \bm{x}_i^T\beta.
</span> The unknown variance <span class="math inline">\sigma^2 = \phi</span> is called <span class="orange">dispersion</span> in GLMs. The <span class="orange">parameter space</span> is <span class="math inline">\mathcal{M} = \mathbb{R}</span>, whereas <span class="math inline">a_i(\phi) = \phi</span> and the variance function is <span class="blue">constant</span> <span class="math inline">v(\mu) = 1</span> (homoschedasticity).</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In a <span class="blue">binomial regression model</span> with <span class="blue">logit link</span> <span class="math inline">g(\mu) = \text{logit}(\mu)</span> we let <span class="math inline">Y_i = S_i/m_i</span> and <span class="math display">
S_i \overset{\text{ind}}{\sim}\text{Binomial}(m_i, \pi_i),\qquad \mathbb{E}\left(Y_i\right) = \pi_i = \mu_i, \qquad \text{logit}(\mu_i) =  \bm{x}_i^T\beta.
</span> We have <span class="math inline">a_i(\phi) = 1/m_i</span> and <span class="math inline">v(\mu) = \mu(1-\mu)</span>. There is <span class="orange">no dispersion</span> parameter.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In <span class="blue">Poisson regression</span> with <span class="blue">logarithmic link</span> <span class="math inline">g(\mu) = \log(\mu)</span> we let <span class="math display">
Y_i \overset{\text{ind}}{\sim}\text{Poisson}(\mu), \qquad \log(\mu_i) =  \bm{x}_i^T\beta.
</span> We have <span class="math inline">a_i(\phi) = 1</span> and <span class="math inline">v(\mu) = \mu</span>. There is <span class="orange">no dispersion</span> parameter.</p>
</div>
</div>
</div>
</section>
</section>
<section id="exponential-dispersion-families" class="level1 page-columns page-full">
<h1>Exponential dispersion families</h1>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<ul>
<li>Figure 1 of <span class="citation" data-cites="Efron2023">Efron (<a href="#ref-Efron2023" role="doc-biblioref">2023</a>)</span>. <span class="grey">Three level</span> of statistical modeling.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/EF.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:6in"></p>
</figure>
</div>
<ul>
<li><p>The <span class="orange">prime role</span> of <span class="orange">exponential families</span> in the theory of statistical inference was first emphasized by <span class="citation" data-cites="Fisher1934">Fisher (<a href="#ref-Fisher1934" role="doc-biblioref">1934</a>)</span>.</p></li>
<li><p>Most <span class="blue">well-known</span> <span class="blue">distributions</span>—such as Gaussian, Poisson, Binomial, and Gamma—are instances of exponential families.</p></li>
</ul>
</section>
<section id="exponential-dispersion-family-definition" class="level2">
<h2 class="anchored" data-anchor-id="exponential-dispersion-family-definition">Exponential dispersion family: definition</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The <span class="orange">density</span> of <span class="math inline">Y_i</span> belongs to an <span class="blue">exponential dispersion family</span> if it can be written as <span class="math display">
p(y_i; \theta_i, \phi) = \exp\left\{\frac{\theta_i y_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi)\right\},
</span> where <span class="math inline">y_i \in \mathcal{Y} \subseteq \mathbb{R}</span>, <span class="math inline">\theta_i \in \Theta \subseteq\mathbb{R}</span> and <span class="math inline">a_i(\phi) = \phi / \omega_i</span> where <span class="math inline">\omega_i</span> are <span class="blue">known positive weights</span>. The parameter <span class="math inline">\theta_i</span> is called <span class="orange">natural parameter</span> while <span class="math inline">\phi</span> is called <span class="blue">dispersion</span> parameter.</p>
</div>
</div>
</div>
<ul>
<li><p>By specifying the functions <span class="math inline">a_i(\cdot), b(\cdot)</span> and <span class="math inline">c(\cdot)</span> one obtain a particular <span class="orange">parametric model</span>.</p></li>
<li><p>The support <span class="math inline">\mathcal{Y}</span> of <span class="math inline">Y_i</span> does not depend on the parameters <span class="math inline">\phi</span> or <span class="math inline">\theta_i</span> and <span class="math inline">b(\cdot)</span> can be <span class="blue">differentiated</span> infinitely many times. In particular, this is a <span class="orange">regular statistical model</span>.</p></li>
<li><p>As mentioned, special cases are <span class="math inline">a_i(\phi) = \phi</span> and <span class="math inline">a_i(\phi) = 1</span>. When <span class="math inline">a_i(\phi) = 1</span> and <span class="math inline">c(y_i, \phi) = c(y_i)</span> we obtain <span class="math display">
p(y_i; \theta_i) = \exp\left\{\theta_i y_i - b(\theta_i) + c(y_i)\right\},
</span> which is called <span class="orange">natural exponential family</span> of order 1.</p></li>
</ul>
</section>
<section id="mean-and-variance-i" class="level2">
<h2 class="anchored" data-anchor-id="mean-and-variance-i">Mean and variance I 📖</h2>
<ul>
<li><p>Let us consider the <span class="orange">log-likelihood</span> contribution of the <span class="math inline">i</span>th observations, which is defined as <span class="math display">
\ell(\theta_i, \phi; y_i) = \log{p(y_i; \theta_i, \phi)} = \frac{\theta_i y_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi).
</span> If you prefer, this is the log-likelihood when the sample size <span class="math inline">n = 1</span> and we only observe <span class="math inline">Y_i</span>.</p></li>
<li><p>The <span class="blue">score</span> and <span class="orange">hessian</span> functions, namely the first and second derivative over <span class="math inline">\theta_i</span> are <span class="math display">
\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; y_i) = \frac{y_i - b'(\theta_i)}{a_i(\phi)}, \qquad \frac{\partial^2}{\partial \theta_i^2}\ell(\theta_i, \phi; y_i) = \frac{-b''(\theta_i)}{a_i(\phi)}.
</span> where <span class="math inline">b'(\cdot)</span> and <span class="math inline">b''(\cdot)</span> denote the <span class="blue">first</span> and <span class="orange">second</span> derivative of <span class="math inline">b(\cdot)</span>.</p></li>
<li><p>Recall the following <span class="grey">Bartlett identities</span>, valid in any <span class="orange">regular</span> statistical model: <span class="math display">
\begin{aligned}
\mathbb{E}\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right) &amp;= 0, \\
\mathbb{E}\left\{\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right)^2\right\} = \text{var}\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right) &amp;= \mathbb{E}\left(-\frac{\partial^2}{\partial \theta_i^2}\ell(\theta_i, \phi; Y_i)\right).
\end{aligned}
</span></p></li>
</ul>
</section>
<section id="mean-and-variance-ii" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="mean-and-variance-ii">Mean and variance II 📖</h2>
<ul>
<li>Specializing Bartlett identities in <span class="blue">exponential dispersion families</span>, we obtain <span class="math display">
\mathbb{E}\left(\frac{Y_i - b'(\theta_i)}{a_i(\phi)}\right) = 0, \qquad \text{var}\left(\frac{Y_i - b'(\theta_i)}{a_i(\phi)}\right) = \frac{\text{var}(Y_i)}{a_i(\phi)^2} = \frac{b''(\theta_i)}{a_i(\phi)}.
</span> Re-arranging the terms, we finally get the following key result.</li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">Y_i</span> be an exponential dispersion family, identified by the functions <span class="math inline">a_i(\cdot), b(\cdot)</span> and <span class="math inline">c(\cdot)</span>, and with natural parameter <span class="math inline">\theta_i</span>. Then the <span class="orange">mean</span> and the <span class="blue">variance</span> of <span class="math inline">Y_i</span> equal <span class="math display">
\mathbb{E}(Y_i) =  b'(\theta_i), \qquad \text{var}(Y_i) = a_i(\phi) b''(\theta_i).
</span></p>
</div>
</div>
</div>
<ul>
<li><p>The mean <span class="math inline">\mu_i = b'(\theta_i)</span> does <span class="orange">not</span> depend on the <span class="orange">dispersion</span> parameter.</p></li>
<li><p>We have <span class="math inline">b''(\cdot) &gt; 0</span> because <span class="math inline">\text{var}(Y_i)</span>, which means that <span class="math inline">b(\cdot)</span> is a <span class="blue">convex function</span>.</p></li>
<li><p>Moreover, the function <span class="math inline">b'(\theta)</span> is <span class="orange">continuous</span> and <span class="orange">monotone increasing</span> and hence <span class="orange">invertible</span>.</p></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="margin-aside">
<p>The function <span class="math inline">b(\cdot)</span> is related to the moment generating function of <span class="math inline">Y_i</span>. Thus, higher order derivatives of <span class="math inline">b(\cdot)</span> allows the calculations of skewness, kurtosis, etc.</p>
</div></div></section>
<section id="mean-parametrization-variance-function" class="level2">
<h2 class="anchored" data-anchor-id="mean-parametrization-variance-function">Mean parametrization, variance function</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">Y_i</span> be an exponential dispersion family, identified by the functions <span class="math inline">a_i(\cdot), b(\cdot)</span> and <span class="math inline">c(\cdot)</span>, and with <span class="orange">natural parameter</span> <span class="math inline">\theta_i</span>, then <span class="math display">
\mu(\theta_i):= \mu_i = \mathbb{E}(Y_i) = b'(\theta_i).
</span> The function <span class="math inline">\mu(\cdot) : \Theta \to\mathcal{M}</span> is <span class="blue">one-to-one</span> and invertible, that is, a <span class="orange">reparametrization</span> of <span class="math inline">\theta_i</span>. We call <span class="math inline">\mu_i</span> the <span class="blue">mean parametrization</span> of an exponential dispersion family.</p>
</div>
</div>
</div>
<ul>
<li><p>The <span class="orange">inverse</span> relationship, re-obtaining <span class="math inline">\theta_i</span> as a function of <span class="math inline">\mu_i</span>, is denoted with <span class="math display">
\theta_i = \theta(\mu_i) = b'^{-1}(\mu_i).
</span></p></li>
<li><p>Using this notation, we can express the variance of <span class="math inline">Y_i</span> as a function of <span class="math inline">\mu_i</span> as follows <span class="math display">
\text{var}(Y_i) = a_i(\phi)b''(\theta_i) =  a_i(\phi)b''(\theta(\mu_i)) = a_i(\phi)v(\mu_i),
</span> where <span class="math inline">v(\mu_i) := b''(\theta(\mu_i))</span> is the <span class="blue">variance function</span>.</p></li>
<li><p>The domain <span class="math inline">\mathcal{M}</span> and the variance function <span class="math inline">v(\mu)</span> <span class="orange">characterize</span> the function <span class="math inline">b(\cdot)</span> and the entire distribution, for any given <span class="math inline">a_i(\phi)</span>. This justifies the <span class="blue">notation</span> <span class="math inline">Y_i \sim \text{ED}(\mu_i, a_i(\phi)v(\mu_i))</span>.</p></li>
</ul>
</section>
<section id="gaussian-distribution" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-distribution">Gaussian distribution 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_i \sim \text{N}(\mu_i, \sigma^2)</span>. The <span class="blue">density</span> function of <span class="math inline">Y_i</span> can be written as <span class="math display">
\begin{aligned}
p(y_i; \mu_i, \sigma^2) &amp;= \frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(y_i - \mu_i)^2\right\}
\\
&amp;=\exp\left\{\frac{y_i \mu_i - \mu_i^2/2}{\sigma^2}- \frac{\log(2\pi\sigma^2)}{2}-\frac{y_i^2}{2\sigma^2}\right\}
\end{aligned}
</span></p></li>
<li><p>Then, we can recognize the following relationships: <span class="math display">
\theta_i = \theta(\mu_i) = \mu_i, \quad a_i(\phi) = \phi = \sigma^2, \quad b(\theta_i) = \frac{\theta_i^2}{2}, \quad c(y_i, \phi) = - \frac{\log(2\pi\phi)}{2}-\frac{y_i^2}{2\phi}.
</span> In the Gaussian case, the <span class="blue">mean parametrization</span> and the <span class="orange">natural parametrization</span> coincide. Moreover, the <span class="orange">dispersion</span> <span class="math inline">\phi</span> coincides with the <span class="blue">variance</span> <span class="math inline">\sigma^2</span>.</p></li>
<li><p>Using the results we previously discussed, we obtain the well-known relationships <span class="math display">
\mathbb{E}(Y_i) = b'(\theta_i) = \theta_i, \qquad \text{var}(Y_i) = a_i(\phi)b''(\theta_i) = \phi.
</span> The <span class="blue">variance function</span> <span class="math inline">v(\mu_i) = 1</span> is <span class="orange">constant</span>. We will write <span class="math inline">Y_i \sim \text{ED}(\mu_i, \phi)</span> with <span class="math inline">\mu_i \in \mathcal{M} = \mathbb{R}</span>.</p></li>
</ul>
</section>
<section id="poisson-distribution" class="level2">
<h2 class="anchored" data-anchor-id="poisson-distribution">Poisson distribution 📖</h2>
<p>Let <span class="math inline">Y_i \sim \text{Poisson}(\mu_i)</span>. The <span class="blue">pdf</span> function of <span class="math inline">Y_i</span> can be written as <span class="math display">
\begin{aligned}
p(y_i; \mu_i) &amp;= \frac{\mu_i^{y_i} e^{-\mu_i}}{y_i!}=\exp\{y_i \log(\mu_i) - \mu_i - \log(y_i!)\} \\
&amp;=\exp\{y_i \theta_i - e^{\theta_i} - \log(y_i!)\}, \qquad y_i = 0, 1, 2,\dots.
\end{aligned}
</span></p>
<ul>
<li><p>Then, we can recognize the following relationships: <span class="math display">
\begin{aligned}
\theta_i &amp;= \theta(\mu_i) = \log(\mu_i), \quad &amp;&amp;a_i(\phi) = 1, \\
b(\theta_i) &amp;= e^{\theta_i}, \quad &amp;&amp;c(y_i, \phi) = c(y_i) = -\log(y_i!).
\end{aligned}
</span> There is <span class="orange">no dispersion</span> parameter since <span class="math inline">a_i(\phi) = 1</span>.</p></li>
<li><p>Using the results we previously discussed, we obtain the well-known relationships <span class="math display">
\begin{aligned}
\mathbb{E}(Y_i) &amp;= b'(\theta_i) = e^{\theta_i} = \mu_i, \\
\text{var}(Y_i) &amp;= a_i(\phi) b''(\theta_i) = e^{\theta_i} = \mu_i.
\end{aligned}
</span> The <span class="blue">variance function</span> <span class="math inline">v(\mu_i) = \mu_i</span> is <span class="orange">linear</span>. We will write <span class="math inline">Y_i \sim \text{ED}(\mu_i, \mu_i)</span> with <span class="math inline">\mu_i \in (0, \infty)</span>.</p></li>
</ul>
</section>
<section id="gamma-distribution-i" class="level2">
<h2 class="anchored" data-anchor-id="gamma-distribution-i">Gamma distribution I 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_i \sim \text{Gamma}(\alpha, \lambda_i)</span>. The <span class="blue">density</span> function of <span class="math inline">Y_i</span> can be written as <span class="math display">
\begin{aligned}
p(y_i; \alpha, \lambda_i) &amp;= \frac{\lambda_i^\alpha y_i^{\alpha-1} e^{-\lambda_i y_i}}{\Gamma(\alpha)}
\\
&amp;=\exp\left\{\alpha\log{\lambda_i} - \lambda_i y_i + (\alpha-1)\log{y_i}  - \log{\Gamma}(\alpha)\right\} \\
&amp;=\exp\left\{\alpha\left(\log{\lambda_i} - \frac{\lambda_i}{\alpha} y_i\right) + (\alpha-1)\log{y_i}  - \log{\Gamma}(\alpha)\right\} \\
&amp;=\exp\left\{\frac{\theta_i y_i + \log(-\theta_i)}{\phi} - (1/\phi)\log{\phi}+ (1/\phi - 1)\log{y_i}  - \log{\Gamma}(1/\phi)\right\}, \qquad y &gt; 0,\\
\end{aligned}
</span> having defined the <span class="blue">dispersion</span> <span class="math inline">\phi = 1/\alpha</span> and the <span class="orange">natural parameter</span> <span class="math inline">\theta_i = -\lambda_i/\alpha</span>.</p></li>
<li><p>Then, we can recognize the following relationships: <span class="math display">
\begin{aligned}
\quad a_i(\phi) &amp;= \phi, \qquad b(\theta_i) = - \log(-\theta_i), \\
c(y_i, \phi) &amp;= -  (1/\phi)\log{\phi}+ (1/\phi - 1)\log{y_i}  - \log{\Gamma}(1/\phi).
\end{aligned}
</span></p></li>
</ul>
</section>
<section id="gamma-distribution-ii" class="level2">
<h2 class="anchored" data-anchor-id="gamma-distribution-ii">Gamma distribution II 📖</h2>
<ul>
<li><p>Using the results we previously discussed, we obtain the well-known relationships <span class="math display">
\mathbb{E}(Y_i) = b'(\theta_i) = - \frac{1}{\theta_i} = \frac{\alpha}{\lambda_i} = \mu_i, \qquad \text{var}(Y_i) = a_i(\phi)b''(\theta_i) = \frac{\phi}{\theta_i^2} = \frac{\alpha}{\lambda_i^2}.
</span></p></li>
<li><p>At the same time, we can write the <span class="orange">inverse</span> relationship linking <span class="math inline">\theta_i</span> to the <span class="blue">mean</span> as <span class="math display">
\theta_i = \theta(\mu_i) = - \frac{1}{\mu_i}
</span> from which we finally obtain the following <span class="blue">quadratic</span> variance function <span class="math display">
v(\mu_i) = \mu_i^2.
</span></p></li>
<li><p>We will write <span class="math inline">Y_i \sim \text{ED}(\mu_i, \phi\mu_i^2)</span> with <span class="math inline">\mu_i \in (0, \infty)</span>.</p></li>
</ul>
</section>
<section id="binomial-distribution-i" class="level2">
<h2 class="anchored" data-anchor-id="binomial-distribution-i">Binomial distribution I 📖</h2>
<ul>
<li>Let <span class="math inline">S_i \sim \text{Binomial}(m_i, \pi_i)</span>, with <span class="math inline">\pi_i \in (0, 1)</span>. The random variable <span class="math inline">Y_i = S_i/m_i</span> has <span class="blue">density</span> <span class="math display">
\begin{aligned}
p(y_i; m_i, \pi_i) &amp;= \binom{m_i}{m_i y_i}\pi_i^{m_i y_i}(1 - \pi_i)^{m_i - m_i y_i}\\
&amp;=\binom{m_i}{m_i y_i}\left(\frac{\pi_i}{1 - \pi_i}\right)^{m_i y_i}(1 - \pi_i)^{m_i}\\
&amp;=\exp\left\{m_iy_i\log\left(\frac{\pi_i}{1 - \pi_i}\right) + m_i\log(1 - \pi_i) + \log\binom{m_i}{m_i y_i}\right\},
\end{aligned}
</span> for <span class="math inline">y_i \in \{0, 1/m_i, 2/m_2, \dots, m_i/m_i\}</span>. This can be written as <span class="math display">
p(y_i; m_i, \pi_i) =\exp\left\{\frac{y_i\theta_i - \log\{1 + \exp(\theta_i)\}}{1/m_i}+ \log\binom{m_i}{m_i y_i}\right\},
</span> where the <span class="orange">natural parameter</span> is <span class="math inline">\theta_i = \text{logit}(\pi_i) = \log\{\pi/(1-\pi_i)\}</span>.</li>
</ul>
</section>
<section id="binomial-distribution-ii" class="level2">
<h2 class="anchored" data-anchor-id="binomial-distribution-ii">Binomial distribution II 📖</h2>
<ul>
<li><p>Note that <span class="math inline">\mathbb{E}(Y_i) = \mathbb{E}(Z_i / m_i) = \pi_i = \mu_i</span>. This means there <span class="orange">no dispersion</span> parameter <span class="math inline">\phi</span> and <span class="math display">
\theta_i = \text{logit}(\mu_i), \quad a_i(\phi) = \frac{1}{m_i}, \quad b(\theta_i) = \log\{1 + \exp(\theta_i)\}, \quad c(y_i) = \log\binom{m_i}{m_i y_i}.
</span></p></li>
<li><p>Using the general formulas therefore we obtain <span class="math display">
\begin{aligned}
\mathbb{E}(Y_i) &amp;= b'(\theta_i) = \frac{\exp(\theta_i)}{1 + \exp(\theta_i)} = \mu_i, \\
\text{var}(Y_i) &amp;= a_i(\phi)b''(\theta_i) = \frac{1}{m_i}\frac{\exp(\theta_i)}{[1 + \exp(\theta_i)]^2} = \frac{\mu_i (1 - \mu_i)}{m_i},
\end{aligned}
</span> from which we obtain that the <span class="blue">variance function</span> is <span class="math inline">v(\mu_i) = \mu_i(1-\mu_i)</span> is quadratic.</p></li>
<li><p>We will write <span class="math inline">Y_i \sim \text{ED}(\mu_i, \mu_i(1-\mu_i)/m_i)</span> with <span class="math inline">\mu_i \in \mathcal{M} = (0, 1)</span>.</p></li>
</ul>
</section>
<section id="notable-exponential-dispersion-families" class="level2">
<h2 class="anchored" data-anchor-id="notable-exponential-dispersion-families">Notable exponential dispersion families</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th><span class="math inline">\text{N}(\mu_i, \sigma^2)</span></th>
<th><span class="math inline">\text{Gamma}(\alpha, \alpha/\mu_i)</span></th>
<th><span class="math inline">\frac{1}{m_i}\text{Binomial}(m_i, \mu_i)</span></th>
<th><span class="math inline">\text{Poisson}(\mu_i)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="blue">Support</span> <span class="math inline">\mathcal{Y}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">[0, \infty)</span></td>
<td><span class="math inline">\{0, 1/m_i,\dots, 1\}</span></td>
<td><span class="math inline">\mathbb{N}</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\theta_i</span></td>
<td><span class="math inline">\mu_i</span></td>
<td><span class="math inline">- 1/\mu_i</span></td>
<td><span class="math inline">\log\left(\frac{\mu_i}{1 - \mu_i}\right)</span></td>
<td><span class="math inline">\log{\mu_i}</span></td>
</tr>
<tr class="odd">
<td><span class="orange">Parametric space</span> <span class="math inline">\Theta</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">(-\infty, 0)</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
</tr>
<tr class="even">
<td><span class="math inline">b(\theta_i)</span></td>
<td><span class="math inline">\theta_i^2/2</span></td>
<td><span class="math inline">-\log(-\theta_i)</span></td>
<td><span class="math inline">\log\{1 + \exp(\theta_i)\}</span></td>
<td><span class="math inline">\exp(\theta_i)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\phi</span></td>
<td><span class="math inline">\sigma^2</span></td>
<td><span class="math inline">1/\alpha</span></td>
<td><span class="math inline">1</span></td>
<td><span class="math inline">1</span></td>
</tr>
<tr class="even">
<td><span class="math inline">a_i(\phi)</span></td>
<td><span class="math inline">\sigma^2</span></td>
<td><span class="math inline">1/\alpha</span></td>
<td><span class="math inline">1/m_i</span></td>
<td><span class="math inline">1</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\mathcal{M}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">(0, \infty)</span></td>
<td><span class="math inline">(0, 1)</span></td>
<td><span class="math inline">(0, \infty)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">v(\mu_i)</span></td>
<td><span class="math inline">1</span></td>
<td><span class="math inline">\mu_i^2</span></td>
<td><span class="math inline">\mu_i(1-\mu_i)</span></td>
<td><span class="math inline">\mu_i</span></td>
</tr>
</tbody>
</table>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The list of exponential dispersion families does not end here. Other examples are the <span class="blue">inverse Gaussian</span>, the <span class="orange">negative binomial</span> and <span class="grey">hyperbolic secant</span> distributions.</p>
</div>
</div>
</div>
</section>
<section id="link-functions-and-canonical-link" class="level2">
<h2 class="anchored" data-anchor-id="link-functions-and-canonical-link">Link functions and canonical link</h2>
<ul>
<li><p>To complete the GLM specification, we need to choose a <span class="blue">link function</span> <span class="math inline">g(\cdot)</span> such that: <span class="math display">
g(\mu_i) = \bm{x}_i^T\beta, \qquad \theta_i = \theta(\mu_i) \quad \Longrightarrow \quad \theta_i = \theta(g^{-1}(\bm{x}_i^T\beta)).
</span></p></li>
<li><p>It is fairly natural to consider a <span class="blue">monotone</span> and <span class="orange">differentiable</span> link function <span class="math inline">g(\cdot) : \mathcal{M} \to \mathbb{R}</span> so that the inverse <span class="math inline">g^{-1}(\cdot) : \mathbb{R} \to \mathcal{M}</span>. This ensures that the predictions are well-defined. <span class="math display">
\mathbb{E}(Y_i) = g^{-1}(\bm{x}_i^T\beta) \in \mathcal{M}.
</span></p></li>
<li><p>For example, in <span class="blue">binary regression</span> any continuous <span class="orange">cumulative distribution function</span> for <span class="math inline">g^{-1}(\cdot)</span> leads to a good link function, such as <span class="math inline">g(\cdot) = \Phi(\cdot)</span> (probit) or <span class="math inline">g^{-1}(\eta_i) = e^{\eta_i}/(1 + e^{\eta_i})</span> (logistic).</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The following link is called <span class="orange">canonical link</span> and it is implied by the distribution: <span class="math display">
g(\mu_i) = \theta(\mu_i) \quad \Longrightarrow \quad \theta_i = \bm{x}_i^T\beta.
</span></p>
</div>
</div>
</div>
<ul>
<li>The <span class="blue">identity</span> link is canonical for the <span class="blue">Gaussian</span>, the <span class="orange">logarithm</span> is canonical for the <span class="orange">Poisson</span>, the <span class="grey">logit</span> is canonical for the Binomial and the <span class="blue">reciprocal</span> is canonical for the <span class="blue">Gamma</span>.</li>
</ul>
<!-- - In a Gamma GLM, the reciprocal canonical link does not map $\bm{x}_i^T\beta$ into positive values, therefore the logarithmic link is sometimes preferred.  -->
</section>
</section>
<section id="likelihood-quantities" class="level1">
<h1>Likelihood quantities</h1>
<section id="likelihood-function" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-function">Likelihood function</h2>
<ul>
<li><p>Let <span class="math inline">Y_i \overset{\text{ind}}{\sim}\text{ED}(\mu_i, a_i(\phi)v(\mu_i))</span> be the response variable of a GLM, with <span class="math inline">g(\mu_i) = \bm{x}_i^T\beta</span>. The <span class="blue">joint distribution</span> of the responses <span class="math inline">\bm{Y} = (Y_1,\dots,Y_n)</span> is <span class="math display">
p(\bm{y}; \beta, \phi) = \prod_{i=1}^np(y_i; \beta, \phi) = \prod_{i=1}^n \exp\left\{\frac{y_i\theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi)\right\}.
</span> with <span class="math inline">\theta_i = \theta(\mu_i) = \theta(g^{-1}(\bm{x}_i^T\beta))</span>.</p></li>
<li><p>The <span class="orange">log-likelihood</span> function therefore is <span class="math display">
\ell(\beta, \phi) = \sum_{i=1}^n\frac{y_i\theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi).
</span></p></li>
</ul>
<ul>
<li>In general, there is <span class="orange">no sufficient statistic</span> with dimension smaller than <span class="math inline">n</span>.</li>
</ul>
<!-- ## ☠️ - Sufficient statistics and canonical link -->
<!-- - Consider the [canonical link]{.blue}, namely $\theta(\mu_i) = g(\mu_i)$ so that $\theta_i = \bm{x}_i^T\beta$. Then the log-likelihood function [simplifies]{.orange}: -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \ell(\beta, \phi) &= \sum_{i=1}^n\frac{y_i\bm{x}_i^T\beta - b(\bm{x}_i^T\beta)}{a_i(\phi)} + c(y_i, \phi) \\ -->
<!-- &= \left[\beta_1\sum_{i=1}^n\frac{x_{i1}y_i}{a_i(\phi)} + \cdots + \beta_p\sum_{i=1}^n\frac{x_{ip}y_i}{a_i(\phi)} \right] - \sum_{i=1}^n\frac{b(\bm{x}_i^T\beta)}{a_i(\phi)} + c(y_i, \phi). -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- If $\phi$ were [known]{.blue}, say $a_i(\phi) = 1$ or $a_i(\phi) = 1/\omega_i$, then -->
<!-- $$ -->
<!-- \left(\sum_{i=1}^n\frac{1}{a_i(\phi)}x_{i1}y_i, \dots, \sum_{i=1}^n\frac{1}{a_i(\phi)}x_{ip}y_i \right), -->
<!-- $$ -->
<!-- is (minimal) [sufficient]{.blue} of dimension $p \le n$ for inference on $\beta$.  -->
<!-- . . . -->
<!-- - In [logistic regression]{.blue} for binary observations and [Poisson regression]{.orange} with logarithmic link we have $a_i(\phi) = 1$. The [sufficient]{.blue} statistic is $\bm{X}^T\bm{y} = \sum_{i=1}^n\bm{x}_iy_i = \left(\sum_{i=1}^nx_{i1}y_i, \dots, \sum_{i=1}^nx_{ip}y_i \right).$ -->
</section>
<section id="likelihood-equations-i" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-equations-i">Likelihood equations I 📖</h2>
<ul>
<li><p>To conduct inference using the <span class="orange">classical theory</span> (as in <em>Statistica II</em>), we need to consider the first and second derivative of the log-likelihood, that is, the <span class="blue">score function</span> <span class="math display">
\ell_*(\beta;\phi) := \frac{\partial}{\partial \beta}\ell(\beta, \phi),
</span> and the <span class="orange">observed information matrix</span> <span class="math inline">\bm{J}</span>, whose elements are <span class="math display">
j_{rs} = - \frac{\partial}{\partial \beta_r}\frac{\partial}{\partial \beta_s}\ell(\beta, \phi), \qquad r, s=1,\dots,p.
</span></p></li>
<li><p>These quantities have a <span class="blue">simple expression</span> in the end, but getting there requires quite a <span class="orange">bit of calculus</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let us focus on the estimation of <span class="math inline">\beta</span>, assuming for now that <span class="math inline">\phi</span> is a <span class="blue">known parameter</span>, as is the case in binomial or Poisson regression.</p>
<p>This assumption is not restrictive, even when <span class="math inline">\phi</span> is actually unknown. In fact, we will show that the maximum likelihood estimate <span class="math inline">\hat{\beta}</span> does not depend on <span class="math inline">\phi</span>, and that <span class="math inline">\beta</span> and <span class="math inline">\phi</span> are <span class="orange">orthogonal</span>.</p>
</div>
</div>
</div>
</section>
<section id="likelihood-equations-ii" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-equations-ii">Likelihood equations II 📖</h2>
<ul>
<li><p>Let us begin by noting that <span class="math display">
\ell_r(\beta;\phi) = \frac{\partial}{\partial \beta_r}\ell(\beta, \phi) =  \sum_{i=1}^n\frac{1}{a_i(\phi)} \left(y_i \frac{\partial \theta_i}{\partial \beta_r} - \frac{\partial b(\theta_i)}{\partial \beta_r} \right), \qquad r = 1,\dots,p.
</span> Such an expression can be <span class="blue">simplified</span> because <span class="math display">
\frac{\partial b(\theta_i)}{\partial \beta_r} = b'(\theta_i)\frac{\partial \theta_i}{\partial \beta_r} = \mu_i\frac{\partial \theta_i}{\partial \beta_r},
</span> which implies that the score function will have the following <span class="orange">structure</span>: <span class="math display">
\frac{\partial}{\partial \beta_r}\ell(\beta, \phi) = \sum_{i=1}^n\frac{1}{a_i(\phi)}(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r}, \qquad r=1,\dots,p.
</span></p></li>
<li><p>Recall that <span class="math inline">a_i(\phi) = \phi/\omega_i</span>, hence the <span class="blue">maximum likelihood estimator</span> is obtained by solving: <span class="math display">
\textcolor{red}{\cancel{\frac{1}{\phi}}}\sum_{i=1}^n\omega_i(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r} = 0, \qquad r=1,\dots,p.
</span></p></li>
</ul>
</section>
<section id="likelihood-equations-iii" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-equations-iii">Likelihood equations III 📖</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">f(x)</span> be a function with <span class="orange">inverse</span> <span class="math inline">g(x) = f^{-1}(x)</span> and <span class="blue">first derivative</span> <span class="math inline">f'(x)</span>. Then <span class="math display">
\frac{\partial g}{\partial{x}} = [f^{-1}]'(x) = \frac{1}{f'(f^{-1}(x))}.
</span></p>
</div>
</div>
</div>
<ul>
<li>Recall that <span class="math inline">g(\mu_i) = \bm{x}_i^T\beta = \eta_i</span> and that <span class="math inline">\theta_i = \theta(\mu_i)</span> is the inverse of <span class="math inline">\mu(\theta_i)</span>. As an <span class="blue">application</span> of the above <span class="blue">lemma</span>: <span class="math display">
\frac{\partial \theta_i}{\partial \mu_i} = \theta'(\mu_i) = \frac{1}{\mu'(\theta(\mu_i))}= \frac{1}{b''(\theta(\mu_i))} = \frac{1}{v(\mu_i)},
</span> Moreover, since we <span class="math inline">\mu_i = g^{-1}(\eta_i)</span> we obtain <span class="math display">
\frac{\partial \mu_i}{\partial \eta_i} = \frac{1}{g'(g^{-1}(\eta_i))} = \frac{1}{g'(\mu_i)}.
</span></li>
<li>Summing up, the <span class="orange">chain rule of derivation</span> for <span class="blue">composite functions</span> gives: <span class="math display">
\frac{\partial \theta_i}{\partial \beta_r} = \frac{\partial \theta_i}{\partial \mu_i} \frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \eta_i}{\partial\beta_r} = \frac{1}{v(\mu_i)}\frac{1}{g'(\mu_i)}x_{ir}, \qquad r=1,\dots,p.
</span></li>
</ul>
</section>
<section id="likelihood-equations-iv" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-equations-iv">Likelihood equations IV 📖</h2>
<ul>
<li>Combining all the above equations, we obtain an explicit formula for the <span class="blue">score function</span> <span class="math display">
\frac{\partial}{\partial \beta_r}\ell(\beta, \phi)  = \frac{1}{\phi}\sum_{i=1}^n \omega_i \frac{(y_i - \mu_i)}{v(\mu_i)}\frac{x_{ir}}{g'(\mu_i)} = \sum_{i=1}^n \frac{(y_i - \mu_i)}{\text{var}(Y_i)}\frac{x_{ir}}{g'(\mu_i)}, \qquad r=1,\dots,p.
</span></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The <span class="blue">maximum likelihood estimator</span> solves the <span class="orange">likelihood equations</span>: <span class="math display">
\sum_{i=1}^n \omega_i \frac{(y_i - \mu_i)}{v(\mu_i)}\frac{x_{ir}}{g'(\mu_i)} = 0, \qquad r=1,\dots,p,
</span> which <span class="orange">do not depend</span> on <span class="math inline">\phi</span>. In <span class="blue">matrix notation</span> <span class="math display">
\bm{D}^T \bm{V}^{-1}(\bm{y} - \bm{\mu}) = \bm{0},
</span> where <span class="math inline">\bm{V} = \text{diag}(v(\mu_1)/\omega_1,\dots,v(\mu_n)/\omega_n)</span> and <span class="math inline">\bm{D}</span> is an <span class="math inline">n \times p</span> matrix whose elements are <span class="math display">
d_{ir} = \frac{\partial \mu_i}{\partial \beta_r} =\frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \eta_i}{\partial\beta_r} =\frac{1}{g'(\mu_i)}x_{ir}, \qquad i=1,\dots,n, \quad r=1,\dots,p.
</span></p>
</div>
</div>
</div>
</section>
<section id="canonical-link-simplifications" class="level2">
<h2 class="anchored" data-anchor-id="canonical-link-simplifications">Canonical link: simplifications 📖</h2>
<ul>
<li>When using the <span class="orange">canonical link</span> <span class="math inline">\theta(\mu_i) = g(\mu_i)</span> significant simplifications arise, because <span class="math display">
\frac{\partial \theta_i}{\partial \mu_i}= \frac{1}{v(\mu_i)} = g'(\mu_i) \quad \Longrightarrow\quad v(\mu_i)g'(\mu_i) = 1.
</span> Thus, plugging-in this equality in the former equations, gives: <span class="math display">
\frac{\partial \theta_i}{\partial \beta_r} = x_{ir}, \qquad r=1,\dots,p,
</span> which is not surprising, because the canonical link implies <span class="math inline">\theta_i = x_{i1}\beta_1 + \cdots + x_{ip}\beta_p</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The <span class="orange">likelihood equations</span> under the <span class="blue">canonical link</span> are <span class="math display">
\sum_{i=1}^n \omega_i (y_i - \mu_i)x_{ir} = 0, \qquad r=1,\dots,p.
</span> Let <span class="math inline">\bm{\Omega} = \text{diag}(\omega_1,\dots,\omega_n)</span>, then in <span class="blue">matrix notation</span> we have <span class="math inline">\bm{X}^T\bm{\Omega}(\bm{y} - \bm{\mu}) = \bm{0}</span>. The equations simplify even further when the weights are constant, i.e.&nbsp;<span class="math inline">\bm{\Omega} = I_n</span>, yielding <span class="math inline">\bm{X}^T(\bm{y} - \bm{\mu}) = \bm{0}</span>.</p>
</div>
</div>
</div>
</section>
<section id="examples-of-estimating-equations" class="level2">
<h2 class="anchored" data-anchor-id="examples-of-estimating-equations">Examples of estimating equations</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">Y_i \sim \text{ED}(\mu_i, \phi)</span> with <span class="math inline">g(\mu_i) = \mu_i</span>, namely the <span class="orange">Gaussian</span> linear model with the <span class="grey">identity</span> (canonical) link. The likelihood equations are <span class="math display">
\bm{X}^T(\bm{y} - \bm{X}\beta) = \bm{0},
</span> which are also called <span class="orange">normal equations</span>. Their solution over <span class="math inline">\beta</span> is the OLS <span class="math inline">\hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}</span>.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">Y_i \sim \text{ED}(\mu_i, \phi/\omega_i)</span> with <span class="math inline">g(\mu_i) = \mu_i</span>, namely the <span class="orange">Gaussian</span> linear model with the <span class="grey">identity</span> (canonical) link and <span class="blue">heteroschedastic errors</span>. The likelihood equations are <span class="math display">
\bm{X}^T\bm{\Omega}(\bm{y} - \bm{X}\beta) = \bm{0},
</span> Their solution over <span class="math inline">\beta</span> is the weighted least square estimator <span class="math inline">\hat{\beta}_\text{wls} = (\bm{X}^T\bm{\Omega}\bm{X})^{-1}\bm{X}^T\bm{\Omega}\bm{y}</span>.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">Y_i \sim \text{ED}(\mu_i, \mu_i)</span> with <span class="math inline">g(\mu_i) = \log{\mu_i}</span>, namely a <span class="orange">Poisson</span> regression model with the <span class="grey">logarithmic</span> (canonical) link. The likelihood equations can be solved <span class="orange">numerically</span> <span class="math display">
\bm{X}^T(\bm{y} - \bm{\mu}) = \bm{0}, \qquad \bm{\mu} = (e^{\bm{x}_1^T\beta}, \dots,  e^{\bm{x}_n^T\beta}).
</span></p>
</div>
</div>
</div>
</section>
<section id="example-beetles-data" class="level2">
<h2 class="anchored" data-anchor-id="example-beetles-data">Example: <code>Beetles</code> data</h2>
<ul>
<li><p>Using the <code>Beetles</code> data, we specified a <span class="blue">binomial logistic</span> regression model for the counts <span class="math inline">m_i Y_i \sim \text{Binomial}(m_i, \pi_i)</span> with mean <span class="math inline">\mathbb{E}(Y_i) = \pi_i = \exp(\beta_1 + \beta_2 x_i)/(1 + \exp(\beta_1 + \beta_2 x_i))</span>.</p></li>
<li><p>The <span class="orange">maximum likelihood estimate</span> <span class="math inline">(\hat{\beta}_1, \hat{\beta}_2)</span> is the value solving simultaneously: <span class="math display">
\sum_{i=1}^n m_i y_i =  \sum_{i=1}^n m_i \frac{\exp(\beta_1 + \beta_2x_i)}{1 + \exp(\beta_1 + \beta_2x_i)}, \quad
\text{and}\quad \sum_{i=1}^n m_i x_i y_i =  \sum_{i=1}^n m_i x_i \frac{\exp(\beta_1 + \beta_2x_i)}{1 + \exp(\beta_1 + \beta_2x_i)}.
</span> Unfortunately, there is <span class="orange">no closed form</span> solution.</p></li>
<li><p>In our case, we have that <span class="math display">
\sum_{i=1}^n m_i y_i = 291, \qquad \sum_{i=1}^n m_i x_i y_i = 532.2083.
</span></p></li>
<li><p>With these values, we can use the <span class="orange">numerical</span> algorithm IRLS to solve the above system, obtaining <span class="math display">
\hat{\beta} = (\hat{\beta}_1, \hat{\beta_2}) = (-60.717, 34.270).
</span></p></li>
</ul>
</section>
<section id="example-beetles-data-1" class="level2">
<h2 class="anchored" data-anchor-id="example-beetles-data-1">Example: <code>Beetles</code> data</h2>
<ul>
<li>The <span class="blue">predicted response</span> can be computed by using the formula <span class="math display">
\hat{\mu}_i = \frac{\exp(\hat{\beta}_1 + \hat{\beta}_2x_i)}{1 + \exp(\hat{\beta}_1 + \hat{\beta}_2x_i)} = \frac{\exp(-60.717 + 34.270 x_i)}{1 + \exp(-60.717 + 34.270 x_i)}, \qquad i=1,\dots, 8.
</span></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 16%">
<col style="width: 19%">
<col style="width: 28%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;"><span class="math inline">m_i</span></th>
<th style="text-align: right;"><code>deaths</code> (<span class="math inline">S_i</span>)</th>
<th style="text-align: right;"><code>logdose</code> (<span class="math inline">x_i</span>)</th>
<th style="text-align: right;"><span class="math inline">Y_i = S_i / m_i</span></th>
<th style="text-align: right;"><span class="math inline">\hat{\mu}_i</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">59</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">1.691</td>
<td style="text-align: right;">0.102</td>
<td style="text-align: right;">0.059</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">1.724</td>
<td style="text-align: right;">0.217</td>
<td style="text-align: right;">0.164</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">1.755</td>
<td style="text-align: right;">0.290</td>
<td style="text-align: right;">0.362</td>
</tr>
<tr class="even">
<td style="text-align: right;">56</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">1.784</td>
<td style="text-align: right;">0.500</td>
<td style="text-align: right;">0.605</td>
</tr>
<tr class="odd">
<td style="text-align: right;">63</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">1.811</td>
<td style="text-align: right;">0.825</td>
<td style="text-align: right;">0.795</td>
</tr>
<tr class="even">
<td style="text-align: right;">59</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">1.837</td>
<td style="text-align: right;">0.898</td>
<td style="text-align: right;">0.903</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">1.861</td>
<td style="text-align: right;">0.984</td>
<td style="text-align: right;">0.955</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">1.884</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.979</td>
</tr>
</tbody>
</table>
<ul>
<li>The predicted values and the data <span class="math inline">Y_i</span> were also shown in a <span class="blue">plot</span> at the <a href="./un_B_slides.html#/beetles-data-fitted-model">beginning of this unit</a>.</li>
</ul>
</section>
<section id="example-aids-data" class="level2">
<h2 class="anchored" data-anchor-id="example-aids-data">Example: <code>Aids</code> data</h2>
<ul>
<li><p>In the <code>Aids</code> data, we specified a <span class="blue">Poisson</span> regression model with <span class="math inline">\mathbb{E}(Y_i) = \exp(\beta_1 + \beta_2 x_i)</span>.</p></li>
<li><p>The maximum likelihood estimate <span class="math inline">(\hat{\beta}_1, \hat{\beta}_2)</span> <span class="orange">solve</span> simultaneously: <span class="math display">
\sum_{i=1}^n y_i =  \sum_{i=1}^n \exp(\beta_1 + \beta_2x_i), \quad
\text{and}\quad \sum_{i=1}^n x_i y_i =  \sum_{i=1}^n x_i\exp(\beta_1 + \beta_2 x_i).
</span></p></li>
<li><p>This system does <span class="orange">not</span> always admits a <span class="orange">solution</span>. This happens, for example, in the <span class="blue">extreme case</span> <span class="math inline">\sum_{i=1}^ny_i = 0</span>, occurring when all counts equal zero.</p></li>
</ul>
<ul>
<li><p>Using the <code>Aids</code> data we have <span class="math inline">\sum_{i=1}^ny_i = 217</span> and <span class="math inline">\sum_{i=1}^nx_i y_i = 2387</span>. Via <span class="orange">numerical methods</span> we solve the above system of equations and we obtain <span class="math inline">\hat{\beta}_1 = 0.304</span> and <span class="math inline">\hat{\beta}_2 = 0.259</span>.</p></li>
<li><p>The estimated mean values are <span class="math inline">\hat{\mu}_i = \exp(0.304 + 0.259 x_i)</span> and in particular the mean for the <span class="blue">next period</span> is <span class="math display">
\hat{\mu}_{i+1} = \exp(0.304 + 0.259 (x_i +1)) = \exp(0.259) \hat{\mu}_i = 1.296 \hat{\mu}_i.
</span> In other words, the estimated number of deaths increases by about <span class="math inline">30\%</span> every trimester.</p></li>
</ul>
</section>
<section id="example-aids-data-1" class="level2">
<h2 class="anchored" data-anchor-id="example-aids-data-1">Example: <code>Aids</code> data</h2>
<div class="small-table">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><code>deaths</code> (<span class="math inline">Y_i</span>)</th>
<th style="text-align: right;"><code>period</code> (<span class="math inline">x_i</span>)</th>
<th style="text-align: right;"><span class="math inline">\hat{\mu}_i</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1983-1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1.755</td>
</tr>
<tr class="even">
<td style="text-align: left;">1984-1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">2.274</td>
</tr>
<tr class="odd">
<td style="text-align: left;">1985-1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">2.946</td>
</tr>
<tr class="even">
<td style="text-align: left;">1986-1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">3.817</td>
</tr>
<tr class="odd">
<td style="text-align: left;">1983-2</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">4.945</td>
</tr>
<tr class="even">
<td style="text-align: left;">1984-2</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">6.407</td>
</tr>
<tr class="odd">
<td style="text-align: left;">1985-2</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">8.301</td>
</tr>
<tr class="even">
<td style="text-align: left;">1986-2</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">10.755</td>
</tr>
<tr class="odd">
<td style="text-align: left;">1983-3</td>
<td style="text-align: right;">23</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">13.934</td>
</tr>
<tr class="even">
<td style="text-align: left;">1984-3</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">18.052</td>
</tr>
<tr class="odd">
<td style="text-align: left;">1985-3</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">23.389</td>
</tr>
<tr class="even">
<td style="text-align: left;">1986-3</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">30.302</td>
</tr>
<tr class="odd">
<td style="text-align: left;">1983-4</td>
<td style="text-align: right;">37</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">39.259</td>
</tr>
<tr class="even">
<td style="text-align: left;">1984-4</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">50.863</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>The predicted values and the data <span class="math inline">Y_i</span> were also shown in a <span class="blue">plot</span> at the <a href="./un_B_slides.html#/aids-data-fitted-model">beginning of this unit</a>.</li>
</ul>
</section>
<section id="observed-and-expected-information-i" class="level2">
<h2 class="anchored" data-anchor-id="observed-and-expected-information-i">Observed and expected information I 📖</h2>
<ul>
<li><p>Let us first consider the negative derivative of the score function, that is the <span class="orange">observed information matrix</span> <span class="math inline">\bm{J}</span> with entries: <span class="math display">
\begin{aligned}
j_{rs} &amp;=  -\frac{\partial}{\partial \beta_s}\left[\frac{\partial}{\partial \beta_r}\ell(\beta, \phi)\right] = -\frac{\partial}{\partial \beta_s}\sum_{i=1}^n\frac{1}{a_i(\phi)}(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r} \\
&amp;=\sum_{i=1}^n\frac{1}{a_i(\phi)}\left[\frac{\partial\mu_i}{\partial \beta_s}\frac{\partial\theta_i}{\partial \beta_r} - (y_i - \mu_i) \frac{\partial^2\theta_i}{\partial \beta_r \partial \beta_s}\right], \qquad r,s = 1,\dots,p.
\end{aligned}
</span></p></li>
<li><p>Let <span class="math inline">\bm{I} = \mathbb{E}(\bm{J})</span> be the <span class="math inline">p \times p</span> <span class="blue">Fisher information matrix</span> associated with <span class="math inline">\beta</span>, whose elements are <span class="math display">
i_{rs} = \mathbb{E}(j_{rs}) = \mathbb{E}\left(- \frac{\partial}{\partial \beta_r}\frac{\partial}{\partial \beta_s}\ell(\beta, \phi)\right), \qquad r,s = 1,\dots,p.
</span></p></li>
<li><p>Thus, the Fisher information matrix substantially simplifies because <span class="math inline">\mathbb{E}(Y_i) = \mu_i</span>, obtaining: <span class="math display">
i_{rs} = \sum_{i=1}^n\frac{1}{a_i(\phi)}\frac{\partial\mu_i}{\partial \beta_s}\frac{\partial\theta_i}{\partial \beta_r}, \qquad r,s = 1,\dots,p.
</span></p></li>
</ul>
</section>
<section id="observed-and-expected-information-ii" class="level2">
<h2 class="anchored" data-anchor-id="observed-and-expected-information-ii">Observed and expected information II 📖</h2>
<ul>
<li>In the previous slides we already computed the explicit values of these derivatives: <span class="math display">
\frac{\partial\mu_i}{\partial \beta_s} = \frac{x_{is}}{g'(\mu_i)}, \qquad \frac{\partial\theta_i}{\partial \beta_r} = \frac{x_{is}}{v(\mu_i) g'(\mu_i)}.
</span></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Combining the above equations, we obtain that the <span class="orange">Fisher information</span> <span class="math inline">\bm{I}</span> of a GLM has entries <span class="math display">
i_{rs} = \frac{1}{\phi}\sum_{i=1}^n \omega_i \frac{x_{ir} x_{is}}{v(\mu_i)g'(\mu_i)^2} = \sum_{i=1}^n \frac{x_{ir}x_{is}}{\text{var}(Y_i) g'(\mu_i)^2}, \qquad r,s = 1,\dots,p.
</span> In <span class="blue">matrix notation</span>, we have that <span class="math display">
\bm{I} = \bm{X}^T\bm{W}\bm{X},
</span> where <span class="math inline">\bm{W} = \text{diag}(w_1,\dots,w_n)</span> and <span class="math inline">w_i</span> are <span class="blue">weights</span> such that <span class="math display">
w_i = \frac{1}{\phi}\frac{\omega_i}{v(\mu_i) g'(\mu_i)^2} = \frac{1}{\text{var}(Y_i) g'(\mu_i)^2}, \qquad i=1,\dots,n.
</span></p>
</div>
</div>
</div>
</section>
<section id="canonical-link-simplifications-1" class="level2">
<h2 class="anchored" data-anchor-id="canonical-link-simplifications-1">Canonical link: simplifications 📖</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Under the <span class="orange">canonical link</span> we have that <span class="math inline">\theta_i = x_{i1}\beta_1 + \cdots +\beta_p x_{ip}</span>, which means that <span class="math display">
\frac{\partial^2\theta_i}{\partial \beta_r \partial \beta_s} = 0 \qquad \Longrightarrow \qquad i_{rs} = j_{rs} = \sum_{i=1}^n\frac{1}{a_i(\phi)}\frac{\partial\mu_i}{\partial \beta_s}\frac{\partial\theta_i}{\partial \beta_r}.
</span> The <span class="blue">observed information</span> <span class="math inline">\bm{J}</span> is <span class="orange">non-stochastic</span>, which means that observed information and expected (Fisher) information coincide, that is <span class="math inline">i_{rs} = j_{rs}</span> and <span class="math inline">\bm{I} = \bm{J}</span>.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Under the <span class="orange">canonical link</span>, we also have the simplifications <span class="math inline">1/v(\mu_i) = g'(\mu_i)</span>, yielding <span class="math display">
i_{rs}  = \frac{1}{\phi}\sum_{i=1}^n \omega_i v(\mu_i)x_{ir} x_{is}, \qquad r,s = 1,\dots,p.
</span> In <span class="blue">matrix notation</span>, we have that <span class="math inline">\bm{I} = \bm{X}^T\bm{W}\bm{X}</span> with weights <span class="math display">
w_i = \frac{1}{\phi} \omega_iv(\mu_i) = \frac{v(\mu_i)}{a_i(\phi)}, \qquad i=1,\dots,n.
</span></p>
</div>
</div>
</div>
</section>
<section id="further-considerations" class="level2">
<h2 class="anchored" data-anchor-id="further-considerations">Further considerations</h2>
<ul>
<li>The <span class="orange">observed</span> and <span class="blue">expected</span> information matrices <span class="math inline">\bm{J}</span> and <span class="math inline">\bm{I}</span>, as well as weights <span class="math inline">\bm{W}</span>, <span class="blue">depend</span> on <span class="math inline">\beta</span> and <span class="math inline">\phi</span>. We write <span class="math inline">\hat{\bm{J}}</span>, <span class="math inline">\hat{\bm{I}}</span> and <span class="math inline">\hat{\bm{W}}</span> to indicate that <span class="math inline">\beta</span> and <span class="math inline">\phi</span> have been estimated with <span class="math inline">\hat{\beta}</span> and <span class="math inline">\hat{\phi}</span>.</li>
</ul>
<ul>
<li>If <span class="math inline">\bm{X}</span> has <span class="blue">full rank</span> and <span class="math inline">g'(\mu) \neq 0</span>, then <span class="math inline">\bm{I}</span> is <span class="orange">positive definite</span> for any value of <span class="math inline">\beta</span> and <span class="math inline">\phi</span>.</li>
</ul>
<ul>
<li><p>Under the <span class="orange">canonical link</span>, we have <span class="math inline">\bm{J} = \bm{I}</span>, and both matrices are <span class="orange">positive definite</span> if <span class="math inline">\text{rk}(\bm{X}) = p</span>.</p></li>
<li><p>This implies that the log-likelihood function is <span class="orange">concave</span> because its second derivative is negative definite, so any <span class="orange">solution</span> to the <span class="orange">estimating equations</span> is also a <span class="blue">global optimum</span>.</p></li>
</ul>
<ul>
<li>The Fisher information matrix could be computed exploiting <span class="grey">Bartlett identity</span>, namely <span class="math display">
i_{rs} = \mathbb{E}\left[\left(\frac{\partial}{\partial \beta_r}\ell(\beta, \phi)\right)\left(\frac{\partial}{\partial \beta_s}\ell(\beta, \phi)\right)\right], \qquad r,s = 1,\dots,p.</span> as in <span class="citation" data-cites="Agresti2015">Agresti (<a href="#ref-Agresti2015" role="doc-biblioref">2015</a>)</span>. Of course, the final result <span class="orange">coincide</span> with ours.</li>
</ul>
</section>
<section id="orthogonality-of-beta-and-psi" class="level2">
<h2 class="anchored" data-anchor-id="orthogonality-of-beta-and-psi">☠️ - Orthogonality of <span class="math inline">\beta</span> and <span class="math inline">\psi</span></h2>
<ul>
<li>Let us now consider the case in which <span class="math inline">\phi</span> is <span class="blue">unknown</span> so that <span class="math inline">a_i(\phi) = \phi/\omega_i</span>. We obtain: <span class="math display">
j_{r \phi} = - \frac{\partial}{\partial \beta_r}\frac{\partial}{\partial \phi}\ell(\beta, \phi) = \frac{1}{\phi^2}\sum_{i=1}^n \omega_i(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r}, \qquad r = 1,\dots,p.
</span> whose <span class="orange">expected value</span> is <span class="math inline">i_{r\phi} = \mathbb{E}(j_{r\phi}) = 0</span> since <span class="math inline">\mathbb{E}(Y_i) = \mu_i</span>.</li>
</ul>
<ul>
<li>This means the Fisher information matrix accounting for <span class="math inline">\phi</span> takes the form: <span class="math display">
\begin{pmatrix}
\bm{I} &amp; \bm{0} \\
\bm{0} &amp; i_{\phi \phi}
\end{pmatrix} \qquad\Longrightarrow\qquad  \begin{pmatrix}
\bm{I} &amp; \bm{0} \\
\bm{0} &amp; i_{\phi \phi}
\end{pmatrix}^{-1} = \begin{pmatrix}
\bm{I}^{-1} &amp; \bm{0} \\
\bm{0} &amp; 1 /i_{\phi \phi}
\end{pmatrix}
</span> where <span class="math inline">[\bm{I}]_{rs} = i_{rs}</span> are the elements associated to <span class="math inline">\beta</span> as before.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The parameters <span class="math inline">\beta</span> and <span class="math inline">\phi</span> are <span class="orange">orthogonal</span> and their estimates are <span class="blue">asymptotically independent</span>.</p>
<p>Moreover, the matrices <span class="math inline">\bm{I}</span> and <span class="math inline">\bm{I}^{-1}</span> are sufficient for inference on <span class="math inline">\beta</span> and there is no need to compute <span class="math inline">i_{\phi \phi}</span>. Moreover, the maximum likelihood <span class="math inline">\hat{\beta}</span> can also be computed without knowing <span class="math inline">\phi</span>.</p>
</div>
</div>
</div>
<!-- - In a GLM with $p = 2$ the Fisher information matrix, evaluated on a given $\beta,\phi$, looks like: -->
<!-- $$ -->
<!-- \begin{pmatrix} -->
<!-- i_{11} & i_{12} &  i_{1\phi} \\ -->
<!-- i_{21} & i_{22} &  i_{2\phi} \\ -->
<!-- i_{\phi 1} & i_{\phi2 } &  i_{\phi\phi} \\ -->
<!-- \end{pmatrix}= \begin{pmatrix} -->
<!-- 10.39 & -1.22 & 0 \\  -->
<!-- -1.22 & 10.39 & 0 \\  -->
<!-- 0 & 0 & 10 \\  -->
<!--   \end{pmatrix} \Longrightarrow \begin{pmatrix} -->
<!-- i_{11} & i_{12} &  i_{1\phi} \\ -->
<!-- i_{21} & i_{22} &  i_{2\phi} \\ -->
<!-- i_{\phi 1} & i_{\phi2 } &  i_{\phi\phi} \\ -->
<!-- \end{pmatrix}^{-1}= \begin{pmatrix} -->
<!-- 0.098 & 0.011 & 0 \\  -->
<!-- 0.011 & 0.098 & 0 \\  -->
<!-- 0 & 0 & 0.1 \\  -->
<!--   \end{pmatrix} -->
<!-- $$ -->
<!-- ```{r} -->
<!-- set.seed(1234) -->
<!-- XtX <- crossprod(matrix(rnorm(20), 10, 2)) -->
<!-- XtX <- rbind(cbind(XtX, 0),0) -->
<!-- XtX[3, 3] <- 10 -->
<!-- # xtable::xtable(XtX, digits = 2) -->
<!-- # xtable::xtable(solve(XtX), digits = 3) -->
<!-- ``` -->
</section>
</section>
<section id="irls-algorithm" class="level1">
<h1>IRLS algorithm</h1>
<section id="numerical-methods-for-maximum-likelihood-estimation" class="level2">
<h2 class="anchored" data-anchor-id="numerical-methods-for-maximum-likelihood-estimation">Numerical methods for maximum likelihood estimation</h2>
<ul>
<li><p>In general, the estimating equations of a GLM <span class="math display">
\bm{D}^T \bm{V}^{-1}(\bm{y} - \bm{\mu}) = \bm{0}
</span> cannot be solved in <span class="blue">closed form</span> and we need to rely on <span class="orange">numerical methods</span>.</p></li>
<li><p>An <span class="orange">iterative</span> method means that we start the <span class="blue">algorithm</span> with a candidate value <span class="math inline">\beta^{(1)}</span> (<span class="grey">initialization</span>). Then, at the step <span class="math inline">t</span> we update <span class="math display">
\beta^{(t+1)} = \texttt{update}(\beta^{(t)}), \qquad t=1,2,\dots
</span></p></li>
<li><p>The algorithm <span class="orange">stops</span> whenever a certain criteria is met, e.g.&nbsp;when <span class="math inline">||\beta^{(t+1)} - \beta^{(t)}|| &lt; \epsilon</span>, where <span class="math inline">\epsilon</span> is sometimes called <span class="grey">tolerance</span>. We say it reached <span class="blue">convergence</span>.</p></li>
</ul>
<ul>
<li><p>The <span class="blue">iteratively re-weighted least squares</span> (IRLS) algorithm became very popular after being proposed by <span class="citation" data-cites="Nelder1972">Nelder and Wedderburn (<a href="#ref-Nelder1972" role="doc-biblioref">1972</a>)</span> and is currently implemented in <strong>R</strong>.</p></li>
<li><p>The IRLS algorithm can be used for any GLM, has a clear geometric interpretation, and often delivers good performance. It can be seen as a <span class="orange">variant</span> of <span class="blue">Newton-Raphson</span>.</p></li>
</ul>
</section>
<section id="newton-raphson-algorithm-i" class="level2">
<h2 class="anchored" data-anchor-id="newton-raphson-algorithm-i">Newton-Raphson algorithm I</h2>
<ul>
<li><p>In the Newton-Raphson algorithm, we consider a second-order <span class="blue">Taylor expansion</span> of the log-likelihood <span class="math inline">\ell(\beta) = \ell(\beta,\phi)</span> centered in <span class="math inline">\beta^{(t)}</span>, namely: <span class="math display">
\ell(\beta) \approx \ell(\beta^{(t)}) + \ell_*(\beta^{(t)})^T(\beta - \beta^{(t)}) - \frac{1}{2}(\beta -
\beta^{(t)})^T\bm{J}^{(t)}
</span> where <span class="math inline">\ell_*(\beta^{(t)})</span> is the <span class="orange">score function</span> and <span class="math inline">\bm{J}^{(t)}</span> is the <span class="blue">observed information</span>, evaluated at <span class="math inline">\beta^{(t)}</span>.</p></li>
<li><p>In other words, we <span class="orange">approximate</span> the log-likelihood <span class="math inline">\ell(\beta)</span> with a <span class="blue">parabola</span>. This gives the <span class="grey">approximate likelihood equations</span>: <span class="math display">
\ell_*(\beta^{(t)}) - \bm{J}^{(t)}(\beta - \beta^{(t)}) = \bm{0}.
</span></p></li>
<li><p>Solving the equation above gives the following <span class="orange">updates</span>: <span class="math display">
\beta^{(t+1)} = \hat{\beta}^{(t)} + (\bm{J}^{(t)})^{-1}\ell_*(\beta^{(t)}), \qquad t=1,2,\dots
</span></p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The Netwon-Raphson method essentially considers a series of <span class="orange">parabolic approximations</span> to the log-likelihood, each time evaluating the <span class="blue">point of maximum</span>.</p>
</div>
</div>
</div>
</section>
<section id="newton-raphson-algorithm-ii" class="level2">
<h2 class="anchored" data-anchor-id="newton-raphson-algorithm-ii">Newton-Raphson algorithm II</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/NR.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:5in"></p>
</figure>
</div>
<ul>
<li>Figure taken from <span class="citation" data-cites="Agresti2015">Agresti (<a href="#ref-Agresti2015" role="doc-biblioref">2015</a>)</span>.</li>
</ul>
</section>
<section id="iteratively-re-weighted-least-squares-i" class="level2">
<h2 class="anchored" data-anchor-id="iteratively-re-weighted-least-squares-i">Iteratively re-weighted least squares I 📖</h2>
<ul>
<li>The matrix <span class="math inline">\bm{J}^{(t)}</span> is not always invertible, therefore the algorithm may crash. To remedy this, we replace it with the <span class="blue">expected information</span> <span class="math inline">\bm{I}^{(t)}</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In the <span class="blue">iteratively re-weighted least squares</span> (IRLS) algorithm, we consider the updates: <span class="math display">
\beta^{(t+1)} = \beta^{(t)} + (\bm{I}^{(t)})^{-1}\ell_*(\beta^{(t)}), \qquad t=1,2,\dots
</span> This method is also called <span class="orange">Fisher scoring</span>.</p>
</div>
</div>
</div>
<ul>
<li>The above formula can <span class="blue">simplified</span> a bit. First, we rewrite the <span class="orange">score</span> as <span class="math display">
\frac{\partial}{\partial \beta_r}\ell(\beta, \phi)  = \frac{1}{\phi}\sum_{i=1}^n \omega_i \frac{(y_i - \mu_i)}{v(\mu_i)}\frac{x_{ir}}{g'(\mu_i)} = \sum_{i=1}^nx_{ir} w_i (y_i - \mu_i)g'(\mu_i),
</span> where the <span class="orange">weights</span> were defined as <span class="math inline">w_i = \omega_i / (\phi v(\mu_i) g'(\mu_i)^2)</span>. In <span class="blue">matrix notation</span> we will write: <span class="math display">
\ell_*(\beta^{(t)}) = \bm{X}^T\bm{W}^{(t)}\bm{u}^{(t)}, \qquad \bm{I}^{(t)} = \bm{X}^T\bm{W}^{(t)}\bm{X},
</span> where <span class="math inline">\bm{u}^{(t)} =(u_1^{t},\dots,u_n^{(t)})^T</span> and <span class="math inline">u_i^{(t)} = (y_i - \mu_i^{(t)})g'(\mu_i^{(t)})</span> for <span class="math inline">i=1,\dots,n</span>.</li>
</ul>
</section>
<section id="iteratively-re-weighted-least-squares-ii" class="level2">
<h2 class="anchored" data-anchor-id="iteratively-re-weighted-least-squares-ii">Iteratively re-weighted least squares II 📖</h2>
<ul>
<li>Exploiting the former formulas, we can write the IRLS update as follows <span class="math display">
\beta^{(t+1)} = \beta^{(t)} + (\bm{X}^T\bm{W}^{(t)}\bm{X})^{-1}\bm{X}^T\bm{W}^{(t)}\bm{u}^{(t)}.
</span> Now <span class="orange">multiply</span> both sides by <span class="math inline">(\bm{X}^T\bm{W}^{(t)}\bm{X})</span>, <span class="blue">simplify</span> and <span class="grey">re-arrange</span> the resulting terms. This gives the following formula.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In the <span class="blue">iteratively re-weighted least squares</span> (IRLS) algorithm, we consider the updates: <span class="math display">
\beta^{(t+1)} = (\bm{X}^T\bm{W}^{(t)}\bm{X})^{-1}\bm{X}^T\bm{W}^{(t)}\bm{z}^{(t)}, \qquad t=1,2,\dots,
</span> where <span class="math inline">\bm{z}^{(t)} = (z_1^{(t)}, \dots, z_n^{(t)})</span> is called <span class="orange">pseudo-response</span> whose elements are defined as <span class="math display">
z_i^{(t)} = \bm{x}_i^T\hat{\beta}^{(t)} + u_i^{(t)}  =  \bm{x}_i^T\hat{\beta}^{(t)} + (y_i - \mu_i^{(t)})g'(\mu_i^{(t)}),\qquad i=1,\dots,n.
</span> Hence, each update can be interpreted as the solution of a <span class="grey">weighted least square</span> problem: <span class="math display">
\beta^{(t+1)} = \arg\min_{\beta \in \mathbb{R}^p} \: (\bm{z}^{(t)} - \bm{X}\beta)^T\bm{W}^{(t)}(\bm{z}^{(t)} - \bm{X}\beta).
</span></p>
</div>
</div>
</div>
</section>
<section id="iteratively-re-weighted-least-squares-iii" class="level2">
<h2 class="anchored" data-anchor-id="iteratively-re-weighted-least-squares-iii">Iteratively re-weighted least squares III 📖</h2>
<ul>
<li>The IRLS updates does not depend on the choice of <span class="math inline">\phi</span>, because it <span class="orange">cancels</span> in the multiplications, as we would expect.</li>
</ul>
<ul>
<li><p>The pseudo-responses have a <span class="blue">nice interpretation</span>, because they can be interpreted as a linear approximation of the transformed responses: <span class="math display">
g(y_i) \approx g(\mu_i) + (y_i - \mu_i)g'(\mu_i) = \eta_i + (y_i - \mu_i)g'(\mu_i) = z_i.
</span></p></li>
<li><p>Based on this approximation, a good <span class="blue">initialization</span> is <span class="math display">
\bm{W}^{(1)} = I_n, \qquad z_i^{(1)} = g(y_i), \qquad \Longrightarrow \qquad \beta^{(2)} = (\bm{X}^T\bm{X})^{-1}\bm{X}^Tg(\bm{y}),
</span> the <span class="orange">least square</span> solution for the transformed data. To avoid boundary issues, sometimes the data are perturbed, as we did in Binomial regression.</p></li>
</ul>
</section>
<section id="example-irls-for-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="example-irls-for-logistic-regression">Example: IRLS for logistic regression</h2>
<ul>
<li>Consider a <span class="blue">logistic regression</span> model for <span class="orange">proportions</span> <span class="math inline">Y_i \in \{0, 1/m_i, \dots,1\}</span> with probability of success <span class="math inline">\pi_i = \mu_i</span> and trials <span class="math inline">m_i</span>.</li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
IRLS algorithm for logistic regression
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><span class="grey">Initialize</span> <span class="math inline">\bm{\beta}^{(1)} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\text{logit}(\tilde{\bm{y}})</span> where <span class="math inline">\text{logit}(\tilde{\bm{y}})</span> is the <span class="orange">empirical logit</span> transform.</p></li>
<li><p>For <span class="math inline">t=1,2, \dots</span> until <span class="orange">convergence</span> compute: <span class="math display">
\beta^{(t+1)} = (\bm{X}^T\bm{W}^{(t)}\bm{X})^{-1}\bm{X}^T\bm{W}^{(t)}\bm{z}^{(t)},
</span> where the weights in <span class="math inline">\bm{W}^{(t)}</span> equals <span class="math inline">w_i^{(t)} = m_i \pi_i^{(t)} (1 - \pi_i^{(t)})</span> and the <span class="blue">pseudo-responses</span> <span class="math inline">\bm{z}^{(t)}</span> are <span class="math display">
z_i^{(t)} = \bm{x}_i^T\beta^{(t)} + \frac{y_i - \pi_i^{(t)}}{ \pi_i^{(t)}(1-\pi_i^{(t)})}, \qquad i=1,\dots,n,
</span> with probabilities <span class="math inline">\pi_i^{(t)} = \exp(\bm{x}_i^T\beta^{(t)})/(1 + \exp(\bm{x}_i^T\beta^{(t)}))</span> for <span class="math inline">i=1,\dots,n</span>.</p></li>
</ul>
</div>
</div>
</section>
<section id="estimation-of-the-dispersion-phi" class="level2">
<h2 class="anchored" data-anchor-id="estimation-of-the-dispersion-phi">Estimation of the dispersion <span class="math inline">\phi</span></h2>
<ul>
<li><p>In some GLMs, such as the Gaussian and the Gamma, there is a <span class="orange">dispersion parameter</span> <span class="math inline">\phi</span> that we need to estimate.</p></li>
<li><p>Instead of the maximum likelihood, because of <span class="orange">numerical instabilities</span> and <span class="blue">lack of robustness</span> it is typically preferred a <span class="orange">method of moments</span> estimator. If <span class="math inline">\mu_i</span> were known, the estimator <span class="math display">
\frac{1}{n}\sum_{i=1}^n\omega_i\frac{(y_i - \mu_i)^2}{v(\mu_i)}
</span> would be <span class="orange">unbiased</span> for <span class="math inline">\phi</span>, because <span class="math inline">\mathbb{E}\{(Y_i - \mu_i)^2\} = (\phi/\omega_i) v(\mu_i)</span>. This motivates the estimator <span class="math display">
\hat{\phi} = \frac{1}{n - p}\sum_{i=1}^n \omega_i\frac{(y_i - \hat{\mu}_i)^2}{v(\hat{\mu}_i)}, \qquad \hat{\mu}_i = g^{-1}(\bm{x}_i^T\hat{\beta}).
</span></p></li>
<li><p>This is a <span class="blue">consistent</span> estimator of <span class="math inline">\phi</span> as long as <span class="math inline">\hat{\beta}</span> is consistent.</p></li>
</ul>
<ul>
<li>When <span class="math inline">g(\mu_i) = \mu_i</span> is the <span class="blue">identity link</span> and <span class="math inline">v(\mu_i) = \omega_i = 1</span>, this coincides with the usual unbiased estimator <span class="math inline">s^2</span> of <span class="math inline">\sigma^2</span> for a Gaussian linear model.</li>
</ul>
</section>
</section>
<section id="inference-and-hypothesis-testing" class="level1 page-columns page-full">
<h1>Inference and hypothesis testing</h1>
<section id="asymptotic-distribution-of-hatbeta" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-distribution-of-hatbeta">Asymptotic distribution of <span class="math inline">\hat{\beta}</span></h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The <span class="orange">asymptotic distribution</span> of the <span class="blue">maximum likelihood estimator</span> is <span class="math display">
\hat{\beta} \, \dot{\sim} \, \text{N}_p\left(\beta, (\bm{X}^T\bm{W}\bm{X})^{-1}\right),
</span> for large values of <span class="math inline">n</span> and under mild regularity conditions on <span class="math inline">\bm{X}</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>Under correct specification and mild conditions on <span class="math inline">\bm{X}</span>, the maximum likelihood estimator is <span class="orange">asymptotically unbiased</span> and with known asymptotic <span class="blue">variance</span> <span class="math display">
\mathbb{E}(\hat{\beta} - \beta) \approx 0, \qquad \text{var}(\hat{\beta}) \approx (\bm{X}^T\bm{W}\bm{X})^{-1}.
</span></p></li>
<li><p>In practice, since <span class="math inline">\bm{W}</span> depends on <span class="math inline">\beta</span> and <span class="math inline">\phi</span>, we rely on the following approximation <span class="math display">
\widehat{\text{var}}(\hat{\beta}) = (\bm{X}^T\hat{\bm{W}}\bm{X})^{-1},
</span> where we plugged in the estimates <span class="math inline">\hat{\beta}</span> and <span class="math inline">\hat{\phi}</span> into <span class="math inline">\bm{W}</span> obtaining <span class="math inline">\hat{\bm{W}}</span>. The <span class="orange">standard errors</span> are: <span class="math display">
\texttt{Std. Error} = [\widehat{\text{se}}(\hat{\beta})]_j = \sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}}
</span></p></li>
</ul>
<!-- - Note that the asymptotic variance implicitly takes into account [heteroschedasticity]{.orange}.  -->
</section>
<section id="example-beetles-data-2" class="level2">
<h2 class="anchored" data-anchor-id="example-beetles-data-2">Example: <code>Beetles</code> data</h2>
<ul>
<li><p>Using the <code>Beetles</code> data, we specified a <span class="blue">binomial logistic</span> regression model for the counts <span class="math inline">m_i Y_i \sim \text{Binomial}(m_i, \pi_i)</span> with mean <span class="math inline">\mu_i = \exp(\beta_1 + \beta_2 x_i)/(1 + \exp(\beta_1 + \beta_2 x_i))</span>.</p></li>
<li><p>We previously <span class="orange">estimated</span> <span class="math inline">\hat{\beta} = (-60.717, 34.270)</span>. This means that the <span class="blue">weights</span> are estimated as <span class="math display">
\hat{\bm{W}} =\text{diag}(m_1\hat{\mu}_1(1 - \hat{\mu}_1),\dots,m_n \hat{\mu}_n(1 - \hat{\mu}_n)) = \text{diag}(3.255, 8.227, \dots, 1.231).
</span> from which we obtain the <span class="orange">estimated</span> <span class="blue">Fisher information matrix</span>: <span class="math display">
\bm{X}^T\hat{\bm{W}}\bm{X} = \begin{pmatrix}
\sum_{i=1}^nm_i\hat{\mu}_i(1 - \hat{\mu}_i) &amp; \sum_{i=1}^n x_im_i\hat{\mu}_i(1 - \hat{\mu}_i)\\
\sum_{i=1}^n x_im_i\hat{\mu}_i(1 - \hat{\mu}_i) &amp; \sum_{i=1}^n x_i^2m_i\hat{\mu}_i(1 - \hat{\mu}_i)
\end{pmatrix} = \begin{pmatrix}
58.484 &amp; 104.011\\
104.011 &amp; 185.095
\end{pmatrix}.
</span></p></li>
<li><p>Hence, the <span class="blue">estimated covariance</span> matrix of the maximum likelihood estimator is <span class="math display">
\widehat{\text{var}}(\hat{\beta}) = (\bm{X}^T\hat{\bm{W}}\bm{X})^{-1} = \begin{pmatrix}
26.840 &amp; -15.082 \\
-15.082 &amp; 8.481
\end{pmatrix}.
</span></p></li>
<li><p>Therefore the <span class="orange">estimated standard errors</span> are <span class="math display">
[\widehat{\text{se}}(\hat{\beta})]_j = \sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}} \quad\Longrightarrow \quad \widehat{\text{se}}(\hat{\beta}) = (5.181, 2.912).
</span></p></li>
</ul>
</section>
<section id="example-aids-data-2" class="level2">
<h2 class="anchored" data-anchor-id="example-aids-data-2">Example: <code>Aids</code> data</h2>
<ul>
<li><p>In the <code>Aids</code> data, we specified a <span class="blue">Poisson</span> regression model with <span class="math inline">\mathbb{E}(Y_i) = \exp(\beta_1 + \beta_2 x_i)</span> and estimated <span class="math inline">\hat{\beta} = (0.304, 0.259)</span>.</p></li>
<li><p>This means that the weights are estimated as <span class="math display">
\hat{\bm{W}} = \text{diag}(\hat{\mu}_1,\dots,\hat{\mu}_n) = \text{diag}(1.755, \dots, 50.863).
</span> from which we obtain the <span class="orange">estimated</span> <span class="blue">Fisher information matrix</span>: <span class="math display">
\bm{X}^T\hat{\bm{W}}\bm{X} = \begin{pmatrix}
\sum_{i=1}^n\hat{\mu}_i &amp; \sum_{i=1}^n x_i\hat{\mu}_i\\
\sum_{i=1}^n x_i\hat{\mu}_i &amp; \sum_{i=1}^n x_i^2\hat{\mu}_i
\end{pmatrix} = \begin{pmatrix}
217 &amp; 2387\\
2387 &amp; 28279.05
\end{pmatrix}.
</span></p></li>
<li><p>Hence, the <span class="blue">estimated covariance</span> matrix of the maximum likelihood estimator is <span class="math display">
\widehat{\text{var}}(\hat{\beta}) = (\bm{X}^T\hat{\bm{W}}\bm{X})^{-1} = \begin{pmatrix}
0.06445 &amp; -0.00544 \\
-0.00544 &amp; 0.00049
\end{pmatrix}.
</span></p></li>
<li><p>Therefore the <span class="orange">estimated standard errors</span> are <span class="math display">
[\widehat{\text{se}}(\hat{\beta})]_j = \sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}} \quad\Longrightarrow \quad \widehat{\text{se}}(\hat{\beta}) = (0.254, 0.022).
</span></p></li>
</ul>
</section>
<section id="wald-test-and-confidence-intervals" class="level2">
<h2 class="anchored" data-anchor-id="wald-test-and-confidence-intervals">Wald test and confidence intervals</h2>
<ul>
<li><p>Consider the <span class="blue">hypothesis</span> <span class="math inline">H_0: \beta_j = \beta_0</span> against the <span class="orange">alternative</span> <span class="math inline">H_1: \beta_j \neq \beta_0</span>. The <span class="blue">Wald test statistic</span> <span class="math inline">z_j</span>, rejecting the hypothesis for large values of <span class="math inline">|z_j|</span> is: <span class="math display">
\texttt{z value} = z_j = \frac{\hat{\beta}_j - \beta_0}{[\widehat{\text{se}}(\hat{\beta})]_j} = \frac{\hat{\beta}_j - \beta_0}{\sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}}} \, \dot{\sim}\,\text{N}(0, 1).
</span> which is <span class="orange">approximately</span> distributed as a <span class="blue">standard normal</span> under <span class="math inline">H_0</span>.</p></li>
<li><p>The <span class="blue">p-value</span> is defined in the usual way, namely <span class="math display">
\alpha_\text{obs} = \mathbb{P}(Z \ge |z_j|) = 2 (1 - \Phi(|z_j|)), \qquad Z \sim \text{N}(0, 1).
</span></p></li>
</ul>
<ul>
<li>By inverting the the Wald test, we obtain the associated <span class="blue">confidence interval</span> <span class="math display">
\hat{\beta}_j \pm z_{1 - \alpha/2} \sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}}.
</span> of <span class="orange">approximate</span> level <span class="math inline">1-\alpha</span>, where <span class="math inline">z_{1-\alpha/2}</span> is the quantile of a standard Gaussian.</li>
</ul>
</section>
<section id="comparison-with-the-gaussian-linear-model" class="level2">
<h2 class="anchored" data-anchor-id="comparison-with-the-gaussian-linear-model">Comparison with the Gaussian linear model</h2>
<ul>
<li><p>In a classical <span class="blue">Gaussian linear model</span> the weight matrix is <span class="math inline">\bm{W} = \sigma^2 I_n</span>, therefore <span class="math display">
\hat{\beta} \sim \text{N}_p\left(\beta, \sigma^2(\bm{X}^T\bm{X})^{-1}\right).
</span></p></li>
<li><p>The <span class="blue">Wald statistic</span> <span class="math inline">z_j</span> <span class="orange">specializes</span> to <span class="math display">
z_j = \frac{\hat{\beta}_j - \beta_0}{[\widehat{\text{se}}(\hat{\beta})]_j} = \frac{\hat{\beta}_j - \beta_0}{s \sqrt{[(\bm{X}^T\bm{X})^{-1}]_{jj}}},
</span> which is the usual test statistic considered, e.g., in the output of <code>lm</code> in <strong>R</strong>.</p></li>
<li><p>However, in the Gaussian case there is <span class="orange">no need</span> of <span class="orange">approximations</span>. The distribution of <span class="math inline">z_j</span> is a <span class="grey">Student</span>’ <span class="math inline">t_{n-p}</span> under <span class="math inline">H_0</span>, which indeed converges to a <span class="math inline">\text{N}(0, 1)</span> for large values of <span class="math inline">n</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In GLMs we use procedures that are <span class="orange">approximate</span> rather than <span class="blue">exact</span>. Of course, whenever an exact result is known, we should use it.</p>
</div>
</div>
</div>
</section>
<section id="example-beetles-data-3" class="level2">
<h2 class="anchored" data-anchor-id="example-beetles-data-3">Example: <code>Beetles</code> data</h2>
<ul>
<li>The Wald test is the <span class="orange">default</span> choice in <strong>R</strong> for checking the hypotheses <span class="math inline">H_0 : \beta_j = 0</span>. In the <code>Beetles</code> data we get the following familiar summary:</li>
</ul>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>
z test of coefficients:

            Estimate Std. Error z value  Pr(&gt;|z|)    
(Intercept) -60.7175     5.1807 -11.720 &lt; 2.2e-16 ***
logdose      34.2703     2.9121  11.768 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<ul>
<li><p>Many of the above quantities (estimates and standard errors) have been obtained before.</p></li>
<li><p>In this case, we <span class="orange">reject</span> the null hypothesis that <span class="math inline">\beta_2 = 0</span>. Indeed, even from the scatterplot there was evidence of a relationship between the <code>deaths</code> proportion and the <code>logdose</code>.</p></li>
</ul>
<ul>
<li>For completeness, we also compute the associated <span class="blue">Wald confidence intervals</span>, which are:</li>
</ul>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>                2.5 %    97.5 %
(Intercept) -70.87144 -50.56347
logdose      28.56265  39.97800</code></pre>
</div>
</div>
</section>
<section id="example-aids-data-3" class="level2">
<h2 class="anchored" data-anchor-id="example-aids-data-3">Example: <code>Aids</code> data</h2>
<ul>
<li>The Wald tests for checking the hypotheses <span class="math inline">H_0 : \beta_j = 0</span> in the <code>Aids</code> data are provided below.</li>
</ul>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>
z test of coefficients:

            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 0.303655   0.253867  1.1961   0.2317    
period      0.258963   0.022238 11.6448   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<ul>
<li>In this case, we <span class="orange">reject</span> the null hypothesis that <span class="math inline">\beta_2 = 0</span> because the <span class="blue">p-value</span> <span class="math inline">\texttt{Pr(&gt;|z|)} \approx 0</span>. Again, this is not very surprising: the number of <code>deaths</code> was clearly increasing over time.</li>
</ul>
<ul>
<li>The associated <span class="blue">Wald confidence intervals</span> are:</li>
</ul>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>                 2.5 %    97.5 %
(Intercept) -0.1939158 0.8012249
period       0.2153764 0.3025494</code></pre>
</div>
</div>
</section>
<section id="general-hypothesis-testing" class="level2">
<h2 class="anchored" data-anchor-id="general-hypothesis-testing">General hypothesis testing</h2>
<ul>
<li>Suppose we wish to test <span class="blue">multiple parameters</span> at the same time. Let us organize the parameters into <span class="blue">two blocks</span>: <span class="math display">
\beta = \begin{pmatrix}\beta_A \\ \beta_B \end{pmatrix}, \qquad \beta_A = \begin{pmatrix}\beta_1 \\ \vdots \\ \beta_{p_0} \end{pmatrix}, \quad \beta_B = \begin{pmatrix}\beta_{p_0+1} \\ \vdots \\ \beta_p \end{pmatrix},
</span> where <span class="math inline">q = p - p_0</span> is the number of <span class="orange">constrained parameters</span>. We want to <span class="blue">test the hypothesis</span>: <span class="math display">
H_0: \beta_B = \beta_0 \qquad \text{against}\qquad H_1: \beta_B \neq \beta_0.</span></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>A common case is <span class="math inline">H_0: \beta_B = 0</span> (<span class="grey">nested models</span>), where we compare the <span class="orange">reduced model</span> <span class="math inline">M_0</span> against the <span class="blue">full model</span> <span class="math inline">M_1</span>. We verify if <span class="orange">all</span> the <span class="math inline">q</span> variables associated with <span class="math inline">\beta_B</span> can be omitted.</p>
<p>The case <span class="math inline">q = 1</span>, that is <span class="math inline">\beta_B = \beta_p</span> with <span class="math inline">H_0: \beta_p = 0</span> corresponds to the previously considered situation where we test if a <span class="orange">specific coefficient</span>, say <span class="math inline">\beta_p</span>, is non-zero.</p>
</div>
</div>
</div>
</section>
<section id="testing-hypothesis-in-glms-i" class="level2">
<h2 class="anchored" data-anchor-id="testing-hypothesis-in-glms-i">Testing hypothesis in GLMs I</h2>
<ul>
<li><p>There are <span class="orange">three classical tests</span> that we could consider for such a testing problem: the <span class="orange">Wald test</span> <span class="math inline">W_e</span>, the <span class="grey">Rao-score test</span> <span class="math inline">W_u</span>, and the <span class="blue">log-likelihood ratio test</span> <span class="math inline">W</span>.</p></li>
<li><p>All these tests <span class="orange">reject</span> the null hypothesis for <span class="blue">large values</span> of the <span class="blue">statistic</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Wald test (general case)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\hat{\beta} = (\hat{\beta}_A, \hat{\beta}_B)</span> be the <span class="blue">unrestricted</span> maximum likelihood, the quantity <span class="math display">
W_e = (\hat{\beta}_B - \beta_0)^T \:\widehat{\text{var}}(\hat{\beta}_B)^{-1} \:(\hat{\beta}_B - \beta_0),
</span> is called <span class="blue">Wald test</span>. Here <span class="math inline">\widehat{\text{var}}(\hat{\beta}_B)</span> is the appropriate <span class="grey">block</span> of <span class="math inline">(\bm{X}\hat{\bm{W}}\bm{X})^{-1}</span> and <span class="math inline">\hat{\bm{W}}</span> is estimated using <span class="math inline">\hat{\beta}</span> and <span class="math inline">\hat{\phi}</span>. Under <span class="math inline">H_0</span> this quantity is <span class="orange">approximately</span> distributed as <span class="math display">
W_e \: \dot{\sim} \; \chi^2_q,
</span> a <span class="math inline">\chi^2</span> distribution with <span class="math inline">q</span> <span class="blue">degrees of freedom</span>. The <span class="blue">p-value</span> is <span class="math inline">\texttt{Pr(&gt;Chi)} = \mathbb{P}(W_e &gt; w_\text{e, obs})</span>.</p>
</div>
</div>
<ul>
<li>Clearly, in the <span class="math inline">q = 1</span> case we recover the Wald statistic with <span class="math inline">z_j^2 = W_e</span>.</li>
</ul>
<!-- - The Wald statistic is the [simplest]{.blue}, but it has some [drawbacks]{.orange}. For example, it depends on the parametrization and it is sometimes problematic for small $n$. -->
</section>
<section id="log-likelihood-ratio-test" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="log-likelihood-ratio-test">Log-likelihood ratio test</h2>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Log-likelihood ratio test (LRT)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\hat{\beta} = (\hat{\beta}_A, \hat{\beta}_B)</span> be the <span class="blue">unrestricted</span> maximum likelihood and let <span class="math inline">\hat{\beta}_0 = (\hat{\beta}_{A,0}, \textcolor{red}{\beta_0})</span> the <span class="orange">restricted</span> maximum likelihood estimate. The quantity <span class="math display">
W = 2 [\ell(\hat{\beta}; \hat{\phi}) - \ell(\hat{\beta}_0; \hat{\phi})],
</span> is called <span class="blue">log-likelihood ratio</span> test (LRT). Under <span class="math inline">H_0</span> this quantity is <span class="orange">approximately</span> distributed as <span class="math display">
W \: \dot{\sim} \; \chi^2_q,
</span> a <span class="math inline">\chi^2</span> distribution with <span class="math inline">q</span> <span class="blue">degrees of freedom</span>. The <span class="blue">p-value</span> is <span class="math inline">\texttt{Pr(&gt;Chi)} =\mathbb{P}(W &gt; w_\text{obs})</span>.</p>
</div>
</div>
<ul>
<li><p>When testing <span class="math inline">H_0 : \beta_B = 0</span>, we separately fit the <span class="blue">full model</span>, obtaining <span class="math inline">\hat{\beta}</span>, and the <span class="orange">reduced model</span>, obtaining <span class="math inline">\hat{\beta}_0 = (\hat{\beta}_{A,0}, 0)</span>. Then, we compare their log-likelihoods: <span class="math inline">\ell(\hat{\beta}; \hat{\phi}) - \ell(\hat{\beta}_0; \hat{\phi})</span>.</p></li>
<li><p>The LRT is the <span class="orange">default</span> in <strong>R</strong> for comparing <span class="blue">nested models</span>.</p></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="margin-aside">
<p>When the dispersion parameter <span class="math inline">\phi</span> is unknown, a <span class="orange">variant</span> uses <span class="blue">separate estimates</span> <span class="math inline">\hat{\phi}</span>, based on <span class="math inline">\hat{\beta}</span>, and <span class="math inline">\hat{\phi}_0</span>, based on <span class="math inline">\hat{\beta}_0</span>. The <code>anova</code> <strong>R</strong> command uses a single <span class="math inline">\hat{\phi}</span>, as described above.</p>
</div></div></section>
<section id="score-or-rao-test" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="score-or-rao-test">Score or Rao test</h2>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Rao-score test
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\hat{\beta} = (\hat{\beta}_A, \hat{\beta}_B)</span> be the <span class="blue">unrestricted</span> maximum likelihood and let <span class="math inline">\hat{\beta}_0 = (\hat{\beta}_{A,0}, \textcolor{red}{\beta_0})</span> the <span class="orange">restricted</span> maximum likelihood estimate. Moreover, let <span class="math display">
\ell_B(\beta;\phi) = \frac{\partial}{\partial \beta_B}\ell(\beta, \phi),
</span> namely the <span class="grey">block</span> of the score function associated with <span class="math inline">\beta_B</span>. The quantity <span class="math display">
W_u = \ell_B(\hat{\beta}_0; \hat{\phi})^T\: \widetilde{\text{var}}(\hat{\beta}_B) \: \ell_B(\hat{\beta}_0; \hat{\phi}),
</span> is called <span class="blue">Rao-score test</span>. Here <span class="math inline">\widetilde{\text{var}}(\hat{\beta}_B)</span> is the appropriate <span class="grey">block</span> of <span class="math inline">(\bm{X}\tilde{\bm{W}}\bm{X})^{-1}</span> where <span class="math inline">\tilde{\bm{W}}</span> is estimated using the <span class="orange">restricted</span> <span class="math inline">\hat{\beta}_0</span>. Under <span class="math inline">H_0</span> this quantity is <span class="orange">approximately</span> distributed as <span class="math display">
W_u \: \dot{\sim} \; \chi^2_q,
</span> a <span class="math inline">\chi^2</span> distribution with <span class="math inline">q</span> <span class="blue">degrees of freedom</span>. The <span class="blue">p-value</span> is <span class="math inline">\texttt{Pr(&gt;Chi)} =\mathbb{P}(W_u &gt; w_\text{u, obs})</span>.</p>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="margin-aside">
<p>The Rao-score test arguably the <span class="orange">less common</span>. When <span class="math inline">\phi</span> is unknown, there are several variants depending on how it is estimated.</p>
</div></div></section>
<section id="a-graphical-representation-when-p-1" class="level2">
<h2 class="anchored" data-anchor-id="a-graphical-representation-when-p-1">A graphical representation when <span class="math inline">p = 1</span></h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/tests.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:4in"></p>
</figure>
</div>
<ul>
<li>Figure taken from Azzalini (1996). This is also the <span class="grey">cover</span> of the <span class="orange">book</span>!</li>
</ul>
</section>
<section id="three-asymptotically-equivalent-tests" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="three-asymptotically-equivalent-tests">Three asymptotically equivalent tests</h2>
<div class="callout callout-style-simple callout-warning no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The Wald test, the Score test and the log-likelihood ratio test are <span class="blue">asymptotically equivalent</span>, that is, these tests give the same <span class="blue">same number</span> for large values of <span class="math inline">n</span>. We have that <span class="math display">
W_e = W + o_p(1), \qquad W_u = W + o_p(1),
</span> where <span class="math inline">o_p(1)</span> is a quantity that goes to <span class="math inline">0</span> in probability as <span class="math inline">n \to \infty</span>.</p>
</div>
</div>
</div>
<ul>
<li>When <span class="math inline">q = 1</span>, we can also <span class="blue">invert</span> <span class="math inline">W_e</span>, <span class="math inline">W_u</span> and <span class="math inline">W</span> tests over <span class="math inline">\beta_0</span> to obtain the corresponding <span class="orange">confidence interval</span>. This is often done <span class="orange">numerically</span> for <span class="math inline">W_u</span> and <span class="math inline">W</span>.</li>
</ul>
<ul>
<li><p>The Wald test depends on the parametrization. When considering a transformation of <span class="math inline">\beta</span>, the variance must be adjusted using the derivative of the transformation (<span class="blue">delta method</span>).<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p></li>
<li><p>On the other hand, both the LRT and the score are <span class="blue">invariant</span>, and therefore we can simply <span class="orange">transform</span> the <span class="orange">extremes</span> of the <span class="orange">original interval</span> without further corrections.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Transforming the extremes of Wald confidence interval “works” in the sense that it produces a valid confidence interval, but it is <span class="orange">not</span> the Wald interval in the trasformed scale.</p></div></div></section>
<section id="comparison-with-the-gaussian-linear-model-1" class="level2">
<h2 class="anchored" data-anchor-id="comparison-with-the-gaussian-linear-model-1">Comparison with the Gaussian linear model</h2>
<div class="callout callout-style-simple callout-warning no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In the Gaussian linear model all tests are <span class="blue">equivalent</span> if <span class="math inline">\phi = \sigma^2</span> is <span class="orange">known</span>. We have <span class="math display">
W = W_e = W_u =  \frac{||\bm{Y} - \bm{X}\hat{\beta}_0||^2 - ||\bm{Y} - \bm{X}\hat{\beta}||^2}{\sigma^2} \sim \chi^2_q.
</span> The <span class="math inline">\chi^2_q</span> distribution is <span class="orange">exact</span> and not an approximation thanks to Cochran theorem.</p>
</div>
</div>
</div>
<ul>
<li>Consider the <span class="blue">log-likelihood ratio</span> for testing <span class="math inline">H_0: \beta_B = \beta_0</span>. Suppose <span class="math inline">\sigma^2</span> is <span class="orange">unknown</span>, then: <span class="math display">
\begin{aligned}
W &amp;= 2 [\ell(\hat{\beta}; \hat{\phi}) - \ell(\hat{\beta}_0; \hat{\phi})] = \frac{||\bm{Y} - \bm{X}\hat{\beta}_0||^2 - ||\bm{Y} - \bm{X}\hat{\beta}||^2}{\hat{\phi}} = q \frac{(||\bm{Y} - \bm{X}\hat{\beta}_0||^2 - ||\bm{y} - \bm{Y}\hat{\beta}||^2)/q}{||\bm{y} - \bm{X}\hat{\beta}||^2/(n - p)} \\
&amp; = q F,
\end{aligned}
</span> where <span class="math inline">F \sim F_{q, n - p}</span> is the usual <span class="orange">Snedecor</span>’s F. Indeed <span class="math inline">qF</span> is <span class="orange">approximately</span> distributed as <span class="math inline">\chi^2_q</span> for large values of <span class="math inline">n</span>.</li>
</ul>
<!-- - In Gaussian linear models we should not use the asymptotic approximation, because the [exact distribution]{.orange} is known! -->
<ul>
<li>The quantities <span class="math inline">W_e, W_u</span>, and <span class="math inline">W</span> are the natural extension of the F-statistic for GLMs. They are <span class="orange">approximately</span> distributed as <span class="math inline">\chi^2_q</span> with <span class="math inline">q</span> <span class="blue">degrees of freedom</span>.</li>
</ul>
</section>
<section id="example-beetles-data-4" class="level2">
<h2 class="anchored" data-anchor-id="example-beetles-data-4">Example: <code>Beetles</code> data</h2>
<ul>
<li>We would like to use the <span class="blue">Wald</span>, the <span class="orange">Rao-score</span> and the <span class="grey">log-likelihood ratio</span> tests to verify the hypothesis <span class="math inline">H_0: \beta_2 = 0</span>, that is the relevance of <code>logdose</code> in predicting the response.</li>
</ul>
<ul>
<li>In this case, we have <span class="math inline">q = 1</span> (<span class="math inline">\texttt{Df}</span>) because there is only one parameter under scrutiny.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 61%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Test for the hypothesis <span class="math inline">H_0 : \beta_2 = 0</span></th>
<th style="text-align: right;"><span class="math inline">\texttt{Chi}</span></th>
<th style="text-align: right;"><span class="math inline">\texttt{Df}</span></th>
<th style="text-align: right;"><span class="math inline">\texttt{Pr(&gt;Chi)}</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W_e</span> - <span class="blue">Wald test</span></td>
<td style="text-align: right;">138.488</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;"><span class="math inline">\approx</span> 0</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">W_u</span> - <span class="orange">Rao-score test</span></td>
<td style="text-align: right;">227.580</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;"><span class="math inline">\approx</span> 0</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W</span> - <span class="grey">Log-likelihood ratio test</span></td>
<td style="text-align: right;">272.970</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;"><span class="math inline">\approx</span> 0</td>
</tr>
</tbody>
</table>
<ul>
<li><p>As one may expect, the test values are not identical. Here the sample size is <span class="math inline">n = 8</span>, which is definitely not a big number, therefore we are far from the <span class="blue">asymptotic regime</span>.</p></li>
<li><p>However, the practical conclusions are identical. All tests strongly <span class="orange">reject</span> the null hypothesis.</p></li>
</ul>
<ul>
<li>We previously obtained the <span class="blue">Wald statistic</span> <span class="math inline">z_j</span> and indeed <span class="math inline">z_j^2 = 11.76811^2 = 138.488 = W_e</span>.</li>
</ul>
</section>
<section id="example-beetles-data-5" class="level2">
<h2 class="anchored" data-anchor-id="example-beetles-data-5">Example: <code>Beetles</code> data</h2>
<ul>
<li>Any statistical test can be <span class="orange">inverted</span>, namely we find all the values <span class="math inline">\beta_0</span> such that we do <span class="blue">not reject the null hypothesis</span>. This generates a <span class="blue">confidence interval</span>.</li>
</ul>
<ul>
<li><p>For the Wald test, the inversion is done analytically, producing the “usual” confidence interval.</p></li>
<li><p>For the Rao-score and the log-likelihood ratio we need numerical procedures.</p></li>
<li><p>In the <code>Beetles</code> data, the three tests produce the following confidence intervals for <span class="math inline">\beta_2</span>, associated to <code>logdose</code>.</p></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Confidence intervals for <span class="math inline">\beta_2</span> at a <span class="math inline">95\%</span> level</th>
<th style="text-align: right;">2.5%</th>
<th style="text-align: right;">97.5%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W_e</span> - <span class="blue">Wald test</span></td>
<td style="text-align: right;">28.563</td>
<td style="text-align: right;">39.978</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">W_u</span> - <span class="orange">Rao-score test</span></td>
<td style="text-align: right;">28.588</td>
<td style="text-align: right;">39.957</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W</span> - <span class="grey">Log-likelihood ratio test</span></td>
<td style="text-align: right;">28.854</td>
<td style="text-align: right;">40.301</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Wald interval was also computed before. The three tests produce <span class="blue">nearly identical</span> intervals.</p></li>
<li><p>Wald is always symmetric around <span class="math inline">\hat{\beta}_j</span>, whereas Rao and the log-likelihood ratio are typically <span class="orange">asymmetric</span>, depending on the shape of the likelihood function.</p></li>
</ul>
</section>
<section id="example-aids-data-4" class="level2">
<h2 class="anchored" data-anchor-id="example-aids-data-4">Example: <code>Aids</code> data</h2>
<ul>
<li>Let us know perform the same analysis for the <code>Aids</code> data. Again, we test the null hypothesis <span class="math inline">H_0 : \beta_2 = 0</span>, which is the relevance of <code>period</code> in predicting the response.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 61%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Test for the hypothesis <span class="math inline">H_0 : \beta_2 = 0</span></th>
<th style="text-align: right;"><span class="math inline">\texttt{Chi}</span></th>
<th style="text-align: right;"><span class="math inline">\texttt{Df}</span></th>
<th style="text-align: right;"><span class="math inline">\texttt{Pr(&gt;Chi)}</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W_e</span> - <span class="blue">Wald test</span></td>
<td style="text-align: right;">135.602</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;"><span class="math inline">\approx</span> 0</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">W_u</span> - <span class="orange">Rao-score test</span></td>
<td style="text-align: right;">163.586</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;"><span class="math inline">\approx</span> 0</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W</span> - <span class="grey">Log-likelihood ratio test</span></td>
<td style="text-align: right;">178.551</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;"><span class="math inline">\approx</span> 0</td>
</tr>
</tbody>
</table>
<ul>
<li>As before, despite their numerical differences, all the tests <span class="orange">reject</span> the null hypothesis. We previously obtained the <span class="blue">Wald statistic</span> <span class="math inline">z_j = 11.645</span> and indeed <span class="math inline">z_j^2 = 11.645^2 = 135.6 = W_e</span>.</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Confidence intervals for <span class="math inline">\beta_2</span> at a <span class="math inline">95\%</span> level</th>
<th style="text-align: right;">2.5%</th>
<th style="text-align: right;">97.5%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W_e</span> - <span class="blue">Wald test</span></td>
<td style="text-align: right;">0.2154</td>
<td style="text-align: right;">0.3025</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">W_u</span> - <span class="orange">Rao-score test</span></td>
<td style="text-align: right;">0.2155</td>
<td style="text-align: right;">0.3025</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W</span> - <span class="grey">Log-likelihood ratio test</span></td>
<td style="text-align: right;">0.2165</td>
<td style="text-align: right;">0.3037</td>
</tr>
</tbody>
</table>
</section>
<section id="example-aids-data-5" class="level2">
<h2 class="anchored" data-anchor-id="example-aids-data-5">Example: <code>Aids</code> data</h2>
<ul>
<li><p>We are actually interested in a confidence interval for the quantity <span class="math inline">100\times(\exp(\beta_2) - 1)</span>, which is the <span class="blue">percentage increase</span> of <code>deaths</code> after each period.</p></li>
<li><p>Thanks to <span class="blue">invariance property</span> of the Rao-score and the log-likelihood ratio tests, we can simply <span class="orange">transform</span> the <span class="blue">original intervals</span> for <span class="math inline">\beta_2</span>.</p></li>
<li><p>If the extremes of the log-likelihood ratio interval are <span class="math inline">C_\text{low}, C_\text{high}</span>, then the new interval is <span class="math display">
[100\times(\exp(C_\text{low}) - 1), \:100\times(\exp(C_\text{high}) - 1)].
</span> and similarly for the Rao-score case. These are reported below.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 61%">
<col style="width: 19%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Confidence intervals for <span class="math inline">100[\exp(\beta_2)-1]</span> at a <span class="math inline">95\%</span> level</th>
<th style="text-align: right;">2.5%</th>
<th style="text-align: right;">97.5%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W_u</span> - <span class="orange">Rao-score test</span></td>
<td style="text-align: right;">24.04</td>
<td style="text-align: right;">35.32</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">W</span> - <span class="grey">Log-likelihood ratio test</span></td>
<td style="text-align: right;">24.17</td>
<td style="text-align: right;">35.49</td>
</tr>
</tbody>
</table>
<ul>
<li><p>The <span class="orange">average</span> percentage increase is between <span class="math inline">24\%</span> and <span class="math inline">35\%</span> each <code>period</code>, with a <span class="math inline">95\%</span> <span class="blue">confidence</span>.</p></li>
<li><p>These confidence intervals are always <span class="orange">positive</span>, which is desirable because they are percentages.</p></li>
</ul>
</section>
<section id="example-aids-data-6" class="level2">
<h2 class="anchored" data-anchor-id="example-aids-data-6">Example: <code>Aids</code> data</h2>
<ul>
<li><p>In the Wald case, we cannot simply transform the extremes of the intervals. Indeed, that would lead to a <span class="blue">valid</span> confidence interval that is not anymore of Wald type (<em>Lo sbagliato</em> 🥃).</p></li>
<li><p>Instead, we first need to <span class="orange">adjust</span> the <span class="orange">variance</span> according to the <span class="blue">delta method</span>, obtaining <span class="math display">
\widehat{\text{var}}\{100[\exp(\hat{\beta}_2)-1]\} = 100^2\exp(2 \hat{\beta}_2) \text{var}(\hat{\beta}_2) = 8.301184.
</span></p></li>
<li><p>The Wald confidence interval for <span class="math inline">100[\exp(\hat{\beta}_2)-1]</span> therefore is <span class="math display">
100[\exp(\hat{\beta}_2)-1] \pm z_{1-\alpha/2}\widehat{\text{se}}\{100[\exp(\hat{\beta}_2)-1]\}.
</span></p></li>
</ul>
<div class="cell styled-output">

</div>
<table class="caption-top table">
<colgroup>
<col style="width: 61%">
<col style="width: 19%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Confidence intervals for <span class="math inline">100[\exp(\beta_2)-1]</span> at a <span class="math inline">95\%</span> level</th>
<th style="text-align: right;">2.5%</th>
<th style="text-align: right;">97.5%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W_e</span> - <span class="blue">Wald test</span></td>
<td style="text-align: right;">23.91</td>
<td style="text-align: right;">35.21</td>
</tr>
<tr class="even">
<td style="text-align: left;">“<em>Lo sbagliato</em>” - transformed Wald</td>
<td style="text-align: right;">24.03</td>
<td style="text-align: right;">35.33</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Whenever there are restrictions on the parametric space, as in this case, Wald is typically <span class="orange">problematic</span>. Here, it <span class="blue">could</span> lead to <span class="orange">negative values</span>, which is absurd.</p>
</div>
</div>
</div>
</section>
</section>
<section id="deviance-model-checking-residuals" class="level1 page-columns page-full">
<h1>Deviance, model checking, residuals</h1>
<section id="deviance-some-intuitions" class="level2">
<h2 class="anchored" data-anchor-id="deviance-some-intuitions">Deviance: some intuitions</h2>
<ul>
<li><p>In a Gaussian linear model, we called <span class="orange">deviance</span> the residual sum of squares, that is <span class="math display">
D(\bm{y}; \hat{\bm{\mu}}) = \sum_{i=1}^n(y_i - \bm{x}_i^T\hat{\beta})^2 = \sum_{i=1}^n(y_i - \hat{\mu}_i)^2.
</span></p></li>
<li><p>The residual sum of squares <span class="math inline">D(\bm{y}; \hat{\bm{\mu}})</span> is a <span class="blue">goodness of fit</span> measure. The lower the deviance, the higher the quality of the predictions.</p></li>
</ul>
<ul>
<li><p>When <span class="math inline">\sigma^2</span> is <span class="orange">known</span>, the distribution of the <span class="blue">scaled deviance</span> is <span class="math display">
\frac{D(\bm{Y}; \hat{\bm{\mu}})}{\sigma^2} = \frac{1}{\sigma^2}\sum_{i=1}^n(Y_i - \bm{x}_i^T\hat{\beta})^2  \sim \chi^2_{n - p}.
</span></p></li>
<li><p>When <span class="math inline">\sigma^2</span> is known, the difference of scaled deviances of two nested models is: <span class="math display">
W = \frac{D(\bm{Y}; \hat{\bm{\mu}}_0) - D(\bm{Y}; \hat{\bm{\mu}})}{\sigma^2} = \frac{||\bm{Y} - \bm{X}\hat{\beta}_0||^2 - ||\bm{Y} - \bm{X}\hat{\beta}||^2}{\sigma^2} \sim \chi^2_q.
</span></p></li>
<li><p>The natural question is: what is a natural <span class="blue">generalization</span> of the deviance for GLMs?</p></li>
</ul>
</section>
<section id="example-beetles-data-saturated-model" class="level2">
<h2 class="anchored" data-anchor-id="example-beetles-data-saturated-model">Example: <code>Beetles</code> data, saturated model</h2>
<ul>
<li>Let us consider again the <code>Beetles</code> data and the predictions <span class="math inline">\hat{\mu}_i</span>, based on <span class="math inline">p = 2</span> parameters. These predictions are not perfect but that may be due to chance.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 16%">
<col style="width: 19%">
<col style="width: 28%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;"><span class="math inline">m_i</span></th>
<th style="text-align: right;"><code>deaths</code> (<span class="math inline">S_i</span>)</th>
<th style="text-align: right;"><code>logdose</code> (<span class="math inline">x_i</span>)</th>
<th style="text-align: right;"><span class="math inline">Y_i = S_i / m_i</span></th>
<th style="text-align: right;"><span class="math inline">\hat{\mu}_i</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">59</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">1.691</td>
<td style="text-align: right;">0.102</td>
<td style="text-align: right;">0.059</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">1.724</td>
<td style="text-align: right;">0.217</td>
<td style="text-align: right;">0.164</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">1.755</td>
<td style="text-align: right;">0.290</td>
<td style="text-align: right;">0.362</td>
</tr>
<tr class="even">
<td style="text-align: right;">56</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">1.784</td>
<td style="text-align: right;">0.500</td>
<td style="text-align: right;">0.605</td>
</tr>
<tr class="odd">
<td style="text-align: right;">63</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">1.811</td>
<td style="text-align: right;">0.825</td>
<td style="text-align: right;">0.795</td>
</tr>
<tr class="even">
<td style="text-align: right;">59</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">1.837</td>
<td style="text-align: right;">0.898</td>
<td style="text-align: right;">0.903</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">1.861</td>
<td style="text-align: right;">0.984</td>
<td style="text-align: right;">0.955</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">1.884</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.979</td>
</tr>
</tbody>
</table>
<ul>
<li>The <span class="orange">empirical proportions</span> <span class="math inline">s_i / m_i</span> can be seen as <span class="blue">estimates</span> of the <span class="blue">most flexible model</span>, in which every observation <span class="math inline">Y_i</span> has its own mean <span class="math inline">\mu_i</span>. We call it <span class="orange">saturated model</span> because <span class="math inline">p = n</span>.</li>
</ul>
</section>
<section id="saturated-model" class="level2">
<h2 class="anchored" data-anchor-id="saturated-model">Saturated model</h2>
<ul>
<li><p>Let us express the <span class="orange">log-likelihood</span> of a GLM as a function of the mean <span class="math inline">\bm{\mu} = (\mu_1,\dots,\mu_n)</span>.</p></li>
<li><p>When evaluated in the maximum likelihood, this gives: <span class="math display">
\ell_\mathcal{M}(\hat{\bm{\mu}},\phi) = \sum_{i=1}^n\omega_i\frac{y_i\theta(\hat{\mu}_i) - b(\theta(\hat{\mu}_i))}{\phi} + c(y_i, \phi).
</span> The maximum likelihood for each <span class="math inline">\mu_i</span> is <span class="orange">restricted</span>, in the sense that depends on the <span class="math inline">p</span> parameters of the <span class="blue">linear predictor</span> <span class="math inline">\bm{x}_i^T\beta</span> through the link function <span class="math inline">g(\mu_i) = \bm{x}_i^T\beta</span>.</p></li>
</ul>
<ul>
<li><p>In the <span class="blue">saturated model</span> the means <span class="math inline">\mu_i</span> are <span class="orange">unrestricted</span>: each parameter is estimated separately, giving the maximum likelihood estimate <span class="math inline">\hat{\mu}_{i, \text{sat}} = y_i</span>. This happens whenever <span class="math inline">p = n</span>.</p></li>
<li><p>When evaluated in the maximum, the log-likelihood of the saturated model is <span class="math display">
\ell_\mathcal{M}(\bm{y},\phi) = \sum_{i=1}^n\omega_i\frac{y_i\theta(y_i) - b(\theta(y_i))}{\phi} + c(y_i, \phi).
</span></p></li>
<li><p>The saturated model is the <span class="blue">most complex model</span> we can think of.</p></li>
</ul>
</section>
<section id="deviance" class="level2">
<h2 class="anchored" data-anchor-id="deviance">Deviance</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The <span class="blue">deviance</span> of a GLM is defined as <span class="math display">
\begin{aligned}
D(\bm{y}; \hat{\bm{\mu}}) &amp;:=\phi W =\phi \: 2[\ell_\mathcal{M}(\bm{y},\phi)  - \ell_\mathcal{M}(\hat{\bm{\mu}},\phi)] \\
&amp;=2\sum_{i=1}^n\omega_i\left\{y_i [\theta(y_i) - \theta(\hat{\mu}_i)] - [b(\theta(y_i)) - b(\theta(\hat{\mu}_i))]\right\}.
\end{aligned}
</span> The quantity <span class="math inline">D(\bm{y}; \hat{\bm{\mu}})/\phi</span> is called <span class="blue">scaled deviance</span> and it corresponds to a <span class="orange">log-likelihood ratio test</span> <span class="math inline">W</span> in which the current model is tested against the saturated model.</p>
</div>
</div>
</div>
<ul>
<li><p>By definition, the deviance is <span class="orange">positive</span>: <span class="math inline">D(\bm{y}; \hat{\bm{\mu}}) \ge 0</span>, because <span class="math inline">\ell_\mathcal{M}(\bm{y},\phi) \ge \ell_\mathcal{M}(\hat{\bm{\mu}},\phi)</span>.</p></li>
<li><p>The deviance of the saturated model is <span class="math inline">D(\bm{y}; \bm{y}) = 0</span>.</p></li>
</ul>
<ul>
<li><p>The deviance describes a <span class="blue">lack of fit</span>: the higher the deviance, the poorer the fit.</p></li>
<li><p>It measures the discrepancy between the saturated model and a model using <span class="math inline">p &lt; n</span> parameters.</p></li>
</ul>
<ul>
<li>The deviance is a function of <span class="math inline">\bm{\mu}</span>, therefore its definition does not depend on the link function <span class="math inline">g(\cdot)</span>.</li>
</ul>
</section>
<section id="deviance-and-log-likelihood-ratio-test" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="deviance-and-log-likelihood-ratio-test">Deviance and log-likelihood ratio test</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let us consider two <span class="grey">nested models</span> <span class="math inline">M_0 \subset M_1</span>. The <span class="orange">reduced model</span> <span class="math inline">M_0</span> has <span class="math inline">p_0</span> parameters and predictions <span class="math inline">\hat{\bm{\mu}}_0</span>. The <span class="blue">full model</span> <span class="math inline">M_1</span> has <span class="math inline">p</span> parameters <span class="math inline">\hat{\bm{\mu}}_1</span>.</p>
<p>The <span class="blue">log-likelihood ratio test</span> <span class="math inline">W</span> for testing model <span class="math inline">M_0</span> against model <span class="math inline">M_1</span><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> can be written as <span class="math display">
W = 2 [\ell_\mathcal{M}(\hat{\bm{\mu}}, \hat{\phi})  - \ell_\mathcal{M}(\hat{\bm{\mu}}_0, \hat{\phi})] = \frac{D(\bm{Y}; \hat{\bm{\mu}}_0) - D(\bm{Y}; \hat{\bm{\mu}})}{\hat{\phi}} \: \dot{\sim} \: \chi^2_q.
</span> where <span class="math inline">q = p - p_0</span> are the degrees of freedom.</p>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;More formally, we should say that we are testing the hypothesis <span class="math inline">H_0: \beta_B = \bm{0}</span> against the alternative <span class="math inline">H_1: \beta_B \neq \bm{0}</span>. I hope you can tolerate this slight linguistic abuse.</p></div></div><ul>
<li><p>The log-likelihood ratio can be interpreted as a difference of scaled deviances. This explains why it is popular in GLMs for comparing nested models.</p></li>
<li><p>This is also strong parallelism with the Gaussian linear model.</p></li>
</ul>
</section>
<section id="the-null-model" class="level2">
<h2 class="anchored" data-anchor-id="the-null-model">The null model</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let us consider a model <span class="math inline">M_\text{null}</span> with <span class="blue">no covariates</span> and one parameter (<span class="math inline">p =1</span>), i.e.&nbsp;the <span class="blue">intercept</span>. The predicted values are all equals to <span class="math display">
\hat{\bm{\mu}}_\text{null} = (g^{-1}(\hat{\beta}_1), \dots, g^{-1}(\hat{\beta}_1)).
</span> We call <span class="math inline">M_\text{null}</span> the <span class="blue">null model</span> and <span class="math inline">D(\bm{y}; \hat{\bm{\mu}}_\text{null})</span> the <span class="orange">null deviance</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>The null model is the “opposite” of the saturated model. It is the <span class="orange">simplest</span> among all models and the one having the <span class="blue">highest deviance</span>.</p></li>
<li><p>Indeed, the following inequalities hold: <span class="math display">
0 = D(\bm{y}; \bm{y}) \le D(\bm{y}; \hat{\bm{\mu}}) \le D(\bm{y}; \hat{\bm{\mu}}_\text{null}).
</span></p></li>
<li><p>It is sometimes useful to test the current model against the null model: <span class="math display">
W = \frac{D(\bm{Y}; \hat{\bm{\mu}}_\text{null}) - D(\bm{Y}; \hat{\bm{\mu}})}{\hat{\phi}} \: \dot{\sim} \: \chi^2_{p-1}.
</span> If the <span class="math inline">H_0</span> is not rejected, it means all the <span class="orange">covariates</span> are regarded as <span class="orange">irrelevant</span>.</p></li>
</ul>
</section>
<section id="pearson-x2-statistic" class="level2">
<h2 class="anchored" data-anchor-id="pearson-x2-statistic">Pearson <span class="math inline">X^2</span> statistic</h2>
<ul>
<li><p>The deviance is a <span class="blue">log-likelihood ratio</span> test between a given model and the saturated model, rescaled by <span class="math inline">\phi</span>.</p></li>
<li><p>Hence, we may consider another test, like the <span class="orange">Rao-Score</span>, to obtain an alternative definition.</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">W_u</span> be the <span class="blue">Rao-Score</span> test comparing model <span class="math inline">M</span> with the <span class="orange">saturated model</span>. Then, it holds: <span class="math display">
\phi W_u = X^2=\sum_{i=1}^n \omega_i\frac{(y_i - \hat{\mu}_i)^2}{v(\hat{\mu}_i)},
</span> which is known as generalized <span class="blue">Pearson chi-squared statistic</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>Karl Pearson introduced <span class="math inline">X^2</span> in 1900 for testing various hypotheses using the chi-squared distribution, such as the hypothesis of independence in contingency tables.</p></li>
<li><p>Since <span class="math inline">W_u</span> and <span class="math inline">W</span> are <span class="blue">asymptotically equivalent</span>, so will be the chi-squared statistic <span class="math inline">X^2</span> and the deviance <span class="math inline">D(\bm{Y}; \hat{\bm{\mu}})</span> for large values of <span class="math inline">n</span>.</p></li>
</ul>
</section>
<section id="deviance-of-a-gaussian-linear-model" class="level2">
<h2 class="anchored" data-anchor-id="deviance-of-a-gaussian-linear-model">Deviance of a Gaussian linear model</h2>
<ul>
<li>In the classical <span class="blue">Gaussian linear model</span>, we have that <span class="math inline">\theta_i = \mu_i</span> and <span class="math inline">b(\theta_i) = \theta_i^2</span>. Thus <span class="math inline">\theta(y_i) = y_i</span> and <span class="math display">
\begin{aligned}
D(\bm{y}; \hat{\bm{\mu}}) &amp;=2\sum_{i=1}^n\omega_i\left\{y_i [\theta(y_i) - \theta(\hat{\mu}_i)] - [b(\theta(y_i)) - b(\theta(\hat{\mu}_i))]\right\} \\
&amp;=2\sum_{i=1}^n\{y_i(y_i - \hat{\mu}_i) - y_i^2/2 + \hat{\mu}_i^2/2\}\\
&amp;= \sum_{i=1}^n(y_i^2 - 2y_i\hat{\mu_i} + \hat{\mu}_i^2)= \sum_{i=1}^n(y_i - \hat{\mu}_i)^2.
\end{aligned}
</span></li>
<li>In the Gaussian case, the deviance is the <span class="orange">residuals sum of squares</span> and <span class="math inline">D(\bm{y}; \hat{\bm{\mu}}) = X^2</span>.</li>
</ul>
<ul>
<li>Note that the <span class="blue">null deviance</span> is obtained with <span class="math inline">\hat{\bm{\mu}}_\text{null} = (\bar{y},\dots,\bar{y})</span> so that <span class="math display">
D(\bm{y}; \hat{\bm{\mu}}_\text{null})= \sum_{i=1}^n(y_i - \bar{y})^2,
</span> namely the so-called <span class="blue">total deviance</span>.</li>
</ul>
</section>
<section id="deviance-of-a-poisson-model" class="level2">
<h2 class="anchored" data-anchor-id="deviance-of-a-poisson-model">Deviance of a Poisson model</h2>
<ul>
<li><p>Let us consider a <span class="blue">Poisson regression</span> model, that is <span class="math inline">\theta_i = \log{\mu_i}</span> and <span class="math inline">b(\theta_i) = \exp(\theta_i) = \mu_i</span>. Then <span class="math inline">\theta(y_i) = \log{y_i}</span> and <span class="math display">
\begin{aligned}
D(\bm{y}; \hat{\bm{\mu}}) &amp;=2\sum_{i=1}^n\{y_i(\log{y_i} - \log{\hat{\mu}_i}) - y_i + \hat{\mu}_i\}\\
&amp;= 2\sum_{i=1}^n\{y_i\log(y_i/\hat{\mu}_i) - y_i + \hat{\mu}_i\},
\end{aligned}
</span> with the <span class="grey">convention</span> that <span class="math inline">y_i\log(y_i/\hat{\mu}_i) = 0</span> whenever <span class="math inline">y_i = 0</span>.</p></li>
<li><p>The <span class="math inline">X^2</span> statistic in this case has a very simple form <span class="math display">
X^2 = \sum_{i=1}^n \frac{(y_i - \hat{\mu}_i)^2}{\hat{\mu}_i} = \sum_{i=1}^n\frac{(\textsf{observed}_i - \textsf{fitted}_i)^2}{\textsf{fitted}_i}.
</span> As discussed in <span class="citation" data-cites="Salvan2020">Salvan et al. (<a href="#ref-Salvan2020" role="doc-biblioref">2020</a>)</span>, Example 2.12, this can be seen as a <span class="orange">quadratic approximation</span> of the <span class="blue">deviance</span>, which is valid for large values of <span class="math inline">n</span>.</p></li>
</ul>
</section>
<section id="deviance-of-a-binomial-model-i" class="level2">
<h2 class="anchored" data-anchor-id="deviance-of-a-binomial-model-i">Deviance of a binomial model I</h2>
<ul>
<li><p>Let us consider a <span class="blue">Binomial regression</span> model <span class="math inline">m_i Y_i = S_i \sim \text{Bimonial}(m_i, \pi_i)</span> with <span class="math inline">\mu_i = \pi_i</span>. Then <span class="math display">
\ell_\mathcal{M}(\bm{\hat{\mu}}) = \sum_{i=1}^n\{m_i y_i \log{(\hat{\mu}_i)} + m_i(1 - y_i)\log{(1-\hat{\mu}_i)}\}.
</span> Therefore, under the convention <span class="math inline">x\log(x) = 0</span> as before, the deviance is <span class="math display">
\begin{aligned}
D(\bm{y}; \hat{\bm{\mu}}) &amp;=2\sum_{i=1}^nm_i\left\{y_i\log\left(\frac{y_i}{\hat{\mu}_i}\right)  + (1- y_i)\log\left(\frac{1 - y_i}{1 - \hat{\mu}_i}\right)\right\}\\
&amp;= 2\sum_{i=1}^n\left\{m_i y_i\log\left(\frac{m_i y_i}{m_i \hat{\mu}_i}\right)  + (m_i- m_i y_i)\log\left(\frac{m_i - m_i y_i}{m_i - m_i\hat{\mu}_i}\right)\right\}.
\end{aligned}
</span></p></li>
<li><p>The quantities <span class="math inline">m_i y_i</span> and <span class="math inline">m_i - m_i y_i</span> can be interpreted as the number of observed <span class="blue">successes</span> and <span class="orange">failures</span>, respectively. Similarly, <span class="math inline">m_i\hat{\mu}_i</span> and <span class="math inline">m_i - m_i\hat{\mu}_i</span> represent their <span class="grey">predictions</span>. Hence, we can write <span class="math display">
D(\bm{y}; \hat{\bm{\mu}}) = 2\sum_{j=1}^{2n}\textsf{observed}_j\, \log\left(\frac{\textsf{observed}_j}{\textsf{fitted}_j}\right)
</span></p></li>
</ul>
</section>
<section id="deviance-of-a-binomial-model-ii" class="level2">
<h2 class="anchored" data-anchor-id="deviance-of-a-binomial-model-ii">Deviance of a binomial model II</h2>
<ul>
<li><p>The <span class="math inline">X^2</span> statistic of a binomial model, recalling that <span class="math inline">v(\mu_i) = \mu_i(1 - \mu_i)</span>, equals to <span class="math display">
\begin{aligned}
X^2&amp;= \sum_{i=1}^{n} \frac{(y_i - \hat{\mu}_i)^2}{\hat{\mu}_i(1-\hat{\mu}_i)/m_i} \\
&amp; = \sum_{i=1}^{n} \frac{(m_i y_i - m_i\hat{\mu}_i)^2}{m_i\hat{\mu}_i} + \sum_{i=1}^{n} \frac{[(m_i - m_iy_i) - (m_i - m_i \hat{\mu}_i)]^2}{m_i -m_i \hat{\mu}_i}.
\end{aligned}
</span> The second representation follows after some <span class="grey">algebra</span>.</p></li>
<li><p>The second equation shows that we can write <span class="math display">
X^2 = \sum_{j=1}^{2n} \frac{(\textsf{observed}_j - \textsf{fitted}_j)^2}{\textsf{fitted}_j}.
</span></p></li>
<li><p>As already mentioned, the <span class="math inline">X^2</span> statistic can be seen as a <span class="orange">quadratic approximation</span> of the <span class="blue">deviance</span>.</p></li>
</ul>
</section>
<section id="deviance-as-goodness-of-fit-measure-i" class="level2">
<h2 class="anchored" data-anchor-id="deviance-as-goodness-of-fit-measure-i">Deviance as goodness of fit measure I</h2>
<ul>
<li><p>The deviance is useful a <span class="orange">descriptive measure</span> for the <span class="blue">goodness of fit</span>.</p></li>
<li><p>It is <span class="orange">tempting</span> to use the deviance as a formal <span class="blue">statistical test</span>, to verify if the current model is adequate compared to the saturated model.</p></li>
</ul>
<ul>
<li><p>Suppose <span class="math inline">\phi</span> were <span class="blue">known</span>, then in the Gaussian case we would have <span class="math inline">D(\bm{y}; \hat{\bm{\mu}})/\phi \sim \chi^2_{n-p}</span>, which would allow us to check the adequacy of the model.</p></li>
<li><p>Unfortunately, whenever <span class="math inline">\hat{\phi}</span> is <span class="orange">estimated</span> we obtain <span class="math inline">D(\bm{y}; \hat{\bm{\mu}})/\hat{\phi} \approx n - p</span>, and exactly <span class="math inline">n-p</span> in the Gaussian case, so this strategy can not be used.</p></li>
</ul>
<ul>
<li><p>On the other hand, for example in Poisson e binomial regression, we have <span class="math inline">\phi = 1</span>. Hence, the intuition tells us that, at least approximately, we should have <span class="math display">
D(\bm{y}; \hat{\bm{\mu}}) \; \dot \sim \; \chi^2_{n-p}.
</span></p></li>
<li><p>Unfortunately, this is <span class="orange">not</span> the <span class="orange">case</span>: the saturated model is an “irregular case” in that the number of parameters <span class="math inline">p = n</span> grows with the sample size.</p></li>
<li><p>The usual “large <span class="math inline">n</span>” approximation does not hold in general, e.g.&nbsp;because <span class="math inline">\chi^2_{n-p}</span> itself depends on <span class="math inline">n</span>.</p></li>
</ul>
</section>
<section id="deviance-as-goodness-of-fit-measure-ii" class="level2">
<h2 class="anchored" data-anchor-id="deviance-as-goodness-of-fit-measure-ii">Deviance as goodness of fit measure II</h2>
<ul>
<li>Despite these bad news, it turns out that in some special cases, the <span class="math inline">\chi^2_{n-p}</span> approximation is still valid even for fixed values of <span class="math inline">n</span>.</li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Then the <span class="blue">deviance</span> <span class="math inline">D(\bm{y}; \hat{\bm{\mu}})</span> and the <span class="math inline">X^2</span> <span class="blue">Pearson statistic</span> are approximately distributed as a <span class="math inline">\chi^2_{n-p}</span> in the following cases:</p>
<ul>
<li>In binomial regression, for large values of <span class="math inline">m_i</span> (small-dispersion asymptotics);</li>
<li>In Poisson regression, for large values of the estimated means <span class="math inline">\hat{\mu}_i</span> (say <span class="math inline">\hat{\mu}_i \ge 5</span>);</li>
<li>When <span class="math inline">\phi</span> is known and <span class="math inline">\phi \rightarrow 0</span> (small-dispersion asymptotics).</li>
</ul>
</div>
</div>
</div>
<ul>
<li><p><span class="blue">Small-dispersion asymptotics</span> describe an alternative limiting regime in which the variance of the observations tends to <span class="math inline">0</span>.</p></li>
<li><p>In binomial regression the <span class="math inline">X^2</span> statistic converges to <span class="math inline">\chi^2_{n-p}</span> more quickly than the deviance and has a <span class="blue">more trustworthy</span> p-value when some expected success or failure totals are less than about five.</p></li>
<li><p>The <span class="math inline">\chi^2_{n-p}</span> approximation is <span class="orange">very poor</span> for binary regression, i.e.&nbsp;when <span class="math inline">m_i = 1</span>.</p></li>
</ul>
</section>
<section id="on-pseudo-r2" class="level2">
<h2 class="anchored" data-anchor-id="on-pseudo-r2">On pseudo-<span class="math inline">R^2</span></h2>
<ul>
<li><p>There exist several generalizations of the <span class="math inline">R^2</span> statistic for linear models, called <span class="blue">pseudo-<span class="math inline">R^2</span></span> (e.g.&nbsp;McFadden, Cox &amp; Snell, Nagelkerke, Tjur, etc.).</p></li>
<li><p>These indices are <span class="orange">difficult to interpret</span> and could mislead those accustomed with standard <span class="math inline">R^2.</span> A pseudo-<span class="math inline">R^2 \approx 0.4</span> may indicate a nearly perfect fit (i.e.&nbsp;<code>Beetles</code> data), which is confusing.</p></li>
<li><p>On top of this, these pseudo-<span class="math inline">R^2</span> produce different answers depending on the <span class="blue">aggregation</span> of the data.</p></li>
</ul>
<ul>
<li><p>The recommendation is to rely on indices tailored for the data at hand, such as the ROC curve for binary data, or the correlation between <span class="math inline">\bm{y}</span> and <span class="math inline">\hat{\bm{\mu}}</span>.</p></li>
<li><p>The residual <span class="blue">deviance</span> is also a useful tool, especially for comparing models.</p></li>
</ul>
<!-- - It is not a coincidence that none of our textbooks @Agresti2015, @Salvan2020, @Azzalini2008, @McCullagh1989, nor the `glm` R function, even mention pseudo-$R^2$.  -->
<ul>
<li>Pseudo-<span class="math inline">R^2</span> are often shown by default in other software, such as SAS or SPSS.</li>
</ul>
</section>
<section id="example-beetles-data-output-of-summary" class="level2">
<h2 class="anchored" data-anchor-id="example-beetles-data-output-of-summary">Example: <code>Beetles</code> data, output of <code>summary</code></h2>
<ul>
<li>This is how the <code>summary</code> of a GLM looks like. It is very similar to the <code>summary</code> of <code>lm</code>. At this stage of the course, you should be able to understand almost everything.</li>
</ul>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = cbind(deaths, m - deaths) ~ logdose, family = "binomial", 
    data = Beetles)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -60.717      5.181  -11.72   &lt;2e-16 ***
logdose       34.270      2.912   11.77   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 284.202  on 7  degrees of freedom
Residual deviance:  11.232  on 6  degrees of freedom
AIC: 41.43

Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<ul>
<li><p><code>Null deviance</code> corresponds to the <span class="blue">null deviance</span> <span class="math inline">D(\bm{y}; \hat{\bm{\mu}}_\text{null})</span>.</p></li>
<li><p><code>Residual deviance</code> corresponds to the <span class="orange">deviance</span> <span class="math inline">D(\bm{y}; \hat{\bm{\mu}})</span> of the current model.</p></li>
</ul>
</section>
<section id="example-beetles-data-output-of-anova" class="level2">
<h2 class="anchored" data-anchor-id="example-beetles-data-output-of-anova">Example: <code>Beetles</code> data, output of <code>anova</code></h2>
<ul>
<li><code>anova(model0, model1)</code> computes <span class="blue">log-likelihood ratio test</span> comparing two nested models: the <span class="orange">reduced</span> model <span class="math inline">M_0</span> with <span class="math inline">p_0</span> parameters and the <span class="blue">full</span> model <span class="math inline">M_1</span> with <span class="math inline">p</span> parameters.</li>
</ul>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Deviance Table

Model 1: cbind(deaths, m - deaths) ~ 1
Model 2: cbind(deaths, m - deaths) ~ logdose
  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
1         7    284.202                          
2         6     11.232  1   272.97 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<ul>
<li><p><code>Resid Df</code> are the <span class="blue">degrees of freedom</span> of the <span class="orange">deviances</span>, that is <span class="math inline">n - p_0</span> and <span class="math inline">n-p</span>, respectively.</p></li>
<li><p><code>Resid. Dev</code> are the <span class="blue">deviances</span> of the <span class="orange">reduced</span> model <span class="math inline">D(\bm{y}; \hat{\bm{\mu}}_0)</span> and the <span class="blue">full</span> model <span class="math inline">D(\bm{y}; \hat{\bm{\mu}})</span>. In this example, the reduced model is also the <span class="orange">null model</span>.</p></li>
<li><p><code>Df</code> refers to the <span class="blue">degrees of freedom</span> <span class="math inline">q = p- p_0</span> of the test, which is <span class="math inline">q = 1</span> in this case.</p></li>
<li><p><code>Deviance</code> indicates the <span class="blue">change in deviance</span>, that is <span class="math inline">\phi W = D(\bm{y}; \hat{\bm{\mu}}_0) - D(\bm{y}; \hat{\bm{\mu}})</span>.</p></li>
<li><p><code>Pr(&gt;Chi)</code> is the <span class="blue">p-value</span> of the log-likelihood ratio test <span class="math inline">W</span>.</p></li>
</ul>
</section>
<section id="example-beetles-data-goodness-of-fit" class="level2">
<h2 class="anchored" data-anchor-id="example-beetles-data-goodness-of-fit">Example: <code>Beetles</code> data, goodness of fit</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="2000"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p>The <span class="blue">deviance</span> equals <span class="math inline">D(\bm{y}; \hat{\bm{\mu}}) = 11.232</span>, with <span class="math inline">n - p = 8 - 2 = 6</span> degrees of freedom. The observed <span class="math inline">X^2</span> <span class="orange">Pearson statistic</span> equals <span class="math inline">10.027</span>.</p></li>
<li><p>Using the <span class="math inline">X^2</span> statistic with <span class="math inline">6</span> degrees of freedom, we obtain the p-value <span class="math inline">\mathbb{P}(X^2 &gt; 10.027) = 0.124</span>, as pictured above, which can be interpreted as a <span class="orange">slight</span> lack of fit.</p></li>
</ul>
</section>
<section id="example-aids-data-output-of-summary" class="level2">
<h2 class="anchored" data-anchor-id="example-aids-data-output-of-summary">Example: <code>Aids</code> data, output of <code>summary</code></h2>
<ul>
<li>Below is shown the <code>summary</code> of the Poisson regression model with the <code>Aids</code> data.</li>
</ul>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = deaths ~ period, family = "poisson", data = Aids)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.30365    0.25387   1.196    0.232    
period       0.25896    0.02224  11.645   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 208.754  on 13  degrees of freedom
Residual deviance:  30.203  on 12  degrees of freedom
AIC: 86.949

Number of Fisher Scoring iterations: 5</code></pre>
</div>
</div>
</section>
<section id="example-aids-data-output-of-anova-and-lrtest" class="level2">
<h2 class="anchored" data-anchor-id="example-aids-data-output-of-anova-and-lrtest">Example: <code>Aids</code> data, output of <code>anova</code> and <code>lrtest</code></h2>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Deviance Table

Model 1: deaths ~ 1
Model 2: deaths ~ period
  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
1        13    208.754                          
2        12     30.203  1   178.55 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The use of the term <code>deviance</code> to indicate the <span class="blue">difference</span> between two deviances is a bit <span class="orange">misleading</span>. I do not know the the reason, but the <code>lrtest</code> function of the <code>lmtest</code> package changed it.</p>
</div>
</div>
</div>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>Likelihood ratio test

Model 1: deaths ~ 1
Model 2: deaths ~ period
  #Df   LogLik Df  Chisq Pr(&gt;Chisq)    
1   1 -130.750                         
2   2  -41.475  1 178.55  &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
</section>
<section id="example-aids-data-goodness-of-fit" class="level2">
<h2 class="anchored" data-anchor-id="example-aids-data-goodness-of-fit">Example: <code>Aids</code> data, goodness of fit</h2>
<ul>
<li><p>The <span class="blue">deviance</span> equals <span class="math inline">D(\bm{y}; \hat{\bm{\mu}}) = 30.203</span>, with <span class="math inline">n - p = 14 - 2 = 12</span> degrees of freedom. The observed <span class="math inline">X^2</span> <span class="orange">Pearson statistic</span> equals <span class="math inline">29.92</span>.</p></li>
<li><p>Using the <span class="math inline">X^2</span> statistic with <span class="math inline">12</span> degrees of freedom, we obtain the p-value <span class="math inline">\mathbb{P}(X^2 &gt; 29.92) = 0.0028</span>, therefore <span class="orange">rejecting</span> the hypothesis that this model has a comparable fit with the saturated model.</p></li>
</ul>
<ul>
<li><p>The <span class="math inline">X^2</span> goodness of fit test indicates a potential issue with the model, but it does not explain <span class="blue">why</span>.</p></li>
<li><p>A few remedies could be:</p>
<ol type="a">
<li>Choosing a different <span class="orange">link function</span>;</li>
<li>Including an <span class="blue">additional covariate</span> (if available), and/or considering non-linear <span class="orange">transformations</span> of the available ones;</li>
<li>Choosing a different <span class="orange">distribution</span> instead of the Poisson, such as the negative binomial;</li>
<li>Accounting for <span class="blue">overdispersion</span> using quasi-likelihoods, that is, estimating <span class="math inline">\phi</span> from the data rather than fixing it to <span class="math inline">\phi = 1</span>.</li>
</ol></li>
</ul>
<ul>
<li>It turns out that selecting the link function <span class="math inline">g(\mu_i) = \sqrt{\mu_i}</span> yields a much better fit with <span class="math inline">X^2 = 17.09</span>, whose p-value is <span class="math inline">0.146</span>. This is not the only possible solution.</li>
</ul>
</section>
<section id="residuals" class="level2">
<h2 class="anchored" data-anchor-id="residuals">Residuals</h2>
<ul>
<li>Linear models have an additive structure <span class="math inline">y_i = \bm{x}_i^T\beta + \epsilon_i</span> therefore the residuals can be estimated as <span class="math display">
r_i = y_i - \hat{\mu}_i.
</span> We call these the <span class="blue">response residuals</span>.</li>
</ul>
<ul>
<li><p>GLMs do not have an additive decomposition, therefore we need define a good <span class="orange">generalization</span> of <span class="orange">residuals</span>. There are at least <span class="math inline">2</span> alternatives: Pearson and deviance residuals.</p></li>
<li><p>Ideally, we would like residuals to have approximately <span class="math inline">0</span> mean and unitary variance, but these properties will not hold exactly.</p></li>
</ul>
<ul>
<li>The analysis of the residuals is very helpful for identifying any misspecification as well as hinting the solution. In particular, it is useful for instance to:
<ol type="a">
<li>Choosing the correct variance function <span class="math inline">v(\mu_i)</span>, i.e.&nbsp;the correct response distribution;</li>
<li>Choosing the correct link function;</li>
<li>Identifying latent patterns, often an indication of an omitted variable;</li>
<li>Identifying potential outliers and leverage points.</li>
</ol></li>
</ul>
</section>
<section id="pearson-residuals" class="level2">
<h2 class="anchored" data-anchor-id="pearson-residuals">Pearson residuals</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>For a GLM with variance function <span class="math inline">v(\mu_i)</span>, we call <span class="blue">Pearson residuals</span> the following quantities: <span class="math display">
r_{i, P} = \frac{y_i - \hat{\mu}_i}{\sqrt{v(\hat{\mu}_i)/\omega_i}}, \qquad i=1,\dots,n.
</span></p>
</div>
</div>
</div>
<ul>
<li><p>Pearson residuals rescale the response residuals, accounting for <span class="orange">heteroschedasticity</span>.</p></li>
<li><p>Pearson residuals own their name to the fact that the <span class="math inline">X^2</span> statistic is obtained as <span class="math display">
X^2 = \sum_{i=1}^n r_{i, P}^2 = \sum_{i=1}^n \omega_i \frac{(y_i - \hat{\mu}_i)^2}{v(\mu_i)}.
</span> Moreover, the dispersion parameter, when present, can be estimated as <span class="math display">
\hat{\phi} = \frac{1}{n - p}\sum_{i=1}^n r_{i, P}^2.
</span></p></li>
</ul>
</section>
<section id="deviance-residuals" class="level2">
<h2 class="anchored" data-anchor-id="deviance-residuals">Deviance residuals</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The deviance of a GLM can be written as a sum of individual contributions <span class="math inline">D(\bm{y}; \hat{\bm{\mu}}) = \sum_{i=1}^n d_i</span> and <span class="math display">
d_i = \omega_i\left\{y_i [\theta(y_i) - \theta(\hat{\mu}_i)] - [b(\theta(y_i)) - b(\theta(\hat{\mu}_i))]\right\}, \qquad i=1,\dots,n.
</span> We call <span class="blue">deviance residuals</span> the following quantities: <span class="math display">
r_{i, D} = \text{sign}(y_i - \hat{\mu}_i) \sqrt{d_i}, \qquad i=1,\dots,n.
</span></p>
</div>
</div>
</div>
<ul>
<li><p>By definition, the deviance is obtained as <span class="math display">
D(\bm{y}; \hat{\bm{\mu}}) = \sum_{i=1}^n r_{i, D}^2.
</span></p></li>
<li><p>Deviance residuals are the <span class="orange">default</span> choice in the <code>residuals</code> <strong>R</strong> function.</p></li>
<li><p>Pearson residuals are an asymptotic approximation of deviance residuals, therefore these two quantities are often very similar in practice.</p></li>
</ul>
</section>
<section id="a-weighted-projection-matrix" class="level2">
<h2 class="anchored" data-anchor-id="a-weighted-projection-matrix">A weighted projection matrix</h2>
<ul>
<li>In linear models, we considered the <span class="blue">hat matrix</span> <span class="math inline">\bm{H} = \bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T</span>, whose diagonal elements <span class="math inline">h_i</span> are called <span class="orange">leverages</span> and it holds <span class="math inline">\text{var}(r_i) = \sigma^2(1 - h_i)</span>.</li>
</ul>
<ul>
<li><p>Several arguments can be invoked to justify the following <span class="blue">weighted hat matrix</span> in GLMs <span class="math display">
\bm{H}_W = \bm{W}^{1/2}\bm{X}(\bm{X}^T\bm{W}\bm{X})^{-1}\bm{X}\bm{W}^{1/2}.
</span> This matrix is <span class="blue">symmetric</span> <span class="math inline">(\bm{H}_W = \bm{H}_W^T)</span> and <span class="orange">idempotent</span> (<span class="math inline">\bm{H}_W^2 = \bm{H}_W</span>), i.e.&nbsp;a projection matrix.</p></li>
<li><p>We denote with <span class="math inline">h_{i,W}</span> the diagonal elements of <span class="math inline">\bm{H}_W</span>, which are the <span class="blue">leverages</span> of a GLM. In practice <span class="math inline">\bm{W}</span> is estimated from the data, therefore the leverages will depend on the <span class="orange">response</span>.</p></li>
<li><p>It can be shown, as in <span class="citation" data-cites="Agresti2015">Agresti (<a href="#ref-Agresti2015" role="doc-biblioref">2015</a>)</span>, Section 4.4.5, that <span class="math display">
\text{var}(y_i - \hat{\mu}_i) \approx \phi/\omega_i v(\mu_i)(1 - h_{i, W}) \quad \Longrightarrow \quad \text{var}(r_{i,P}) \approx \phi(1 - h_{i, W}).
</span></p></li>
</ul>
<ul>
<li>A technical but deeper discussion about <span class="math inline">\bm{H}_W</span> can be found in the Appendix of Chapter 4 of <span class="citation" data-cites="Agresti2015">Agresti (<a href="#ref-Agresti2015" role="doc-biblioref">2015</a>)</span>, where stronger analogies with linear models are shown.</li>
</ul>
</section>
<section id="standardized-residuals" class="level2">
<h2 class="anchored" data-anchor-id="standardized-residuals">Standardized residuals</h2>
<ul>
<li><p>In analogy of what has been done for GLMs, we can consider the <span class="blue">standardized</span> version of Pearson and deviance residuals.</p></li>
<li><p><span class="blue">Standardized Pearson residuals</span> are defined as <span class="math display">
\tilde{r}_{i, P} = \frac{r_{i,P}}{\sqrt{\hat{\phi}(1 - \hat{h}_{i, W})}} = \frac{y_i - \hat{\mu_i}}{\sqrt{\hat{\phi}/\omega_iv(\hat{\mu}_i)(1 - \hat{h}_{i, W})}}, \qquad i=1,\dots,n,
</span> where <span class="math inline">\hat{\phi}</span> is an estimate of <span class="math inline">\phi</span> (if unknown) and <span class="math inline">\hat{h}_{i, W}</span> is an estimate of the leverages.</p></li>
<li><p><span class="blue">Standardized deviance residuals</span> are defined as <span class="math display">
\tilde{r}_{i, D} = \frac{r_{i, D}}{\sqrt{\hat{\phi} (1 - \hat{h}_{i, W})}}, \qquad i=1,\dots,n.
</span></p></li>
<li><p>We can also obtain an <span class="orange">approximate Cook’s distance</span> by considering <span class="math display">
c_i = \tilde{r}_{i,P}^2 \frac{\hat{h}_{i, W}}{p(1 - \hat{h}_{i,W})}, \qquad i=1,\dots,n.
</span></p></li>
</ul>
</section>
<section id="on-q-q-plots-and-other-practicalities" class="level2">
<h2 class="anchored" data-anchor-id="on-q-q-plots-and-other-practicalities">On Q-Q plots and other practicalities</h2>
<ul>
<li><p>It is sometimes recommended to check the normality of the Pearson/deviance residuals using Q-Q plots. Such a plot is also provided in <strong>R</strong>.</p></li>
<li><p>Indeed, for example under small dispersion asymptotics or other specific, Pearson residuals are <span class="orange">approximately</span> Gaussian.</p></li>
</ul>
<ul>
<li><p>However, these conditions are often not met. For example, in binary data the response <span class="math inline">y_i \in \{0, 1\}</span> can only assume two values and the residuals will not be Gaussian distributed, even for large <span class="math inline">n</span>.</p></li>
<li><p>Actually, the analysis for the residuals in <span class="blue">binary data</span> do <span class="orange">not provide useful information</span>; see e.g. <span class="citation" data-cites="Salvan2020">Salvan et al. (<a href="#ref-Salvan2020" role="doc-biblioref">2020</a>)</span>, Section 3.6.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The analysis of residuals in GLMs is often useful but should be taken <em>cum grano salis</em>, particularly when dealing with discrete responses that take only a few distinct values.</p>
<p>Overreliance on residual analysis can encourage automatic decisions rather than thoughtful, critical judgment. A good model should not be discarded solely on the basis of a “bad” diagnostic plot.</p>
</div>
</div>
</div>
</section>
<section id="on-identifying-and-removing-outliers" class="level2">
<h2 class="anchored" data-anchor-id="on-identifying-and-removing-outliers">On identifying and removing outliers</h2>
<ul>
<li><p>The analysis of the residuals can also help in identifying outliers and influence points. However, we must be careful in drawing conclusions.</p></li>
<li><p>An outlier might be detected as such simply as the consequence of <span class="orange">model misspecification</span>, e.g.&nbsp;an omitted variable.</p></li>
<li><p>In the vast majority of cases, the presence of outliers should be carefully dealt with by carefully modifying the model.</p></li>
<li><p>There are instances in which outliers are actually <span class="blue">contaminated data points</span> (e.g.&nbsp;<code>age = -3</code>). If there is strong and contextual evidence that this might be the case, then these points should be <span class="orange">removed</span>. Otherwise, removing data points is a <span class="orange">bad practice</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In statistical modelling, we wish to find a model that fits our data, not a dataset that is aligned with our prescribed model.</p>
<p>Discarding observations until the hypoteshes are reasonable it skews the overall analysis and does not answer any meaningful scientific or business question.</p>
</div>
</div>
</div>
</section>
<section id="example-beetles-data-6" class="level2">
<h2 class="anchored" data-anchor-id="example-beetles-data-6">Example: <code>Beetles</code> data</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-33-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="2500"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p>A formal test already confirmed that there are <span class="orange">no</span> noticeable <span class="orange">differences</span> between this model and the saturated model. The analysis of the residuals confirms it.</p></li>
<li><p>Deviance residuals and Pearson residuals are very similar, as expected.</p></li>
</ul>
</section>
<section id="example-beetles-data-7" class="level2">
<h2 class="anchored" data-anchor-id="example-beetles-data-7">Example: <code>Beetles</code> data</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-34-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="2000"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>The Cook’s distance also confirms that there are not strong influence points.</li>
</ul>
</section>
<section id="example-aids-data-7" class="level2">
<h2 class="anchored" data-anchor-id="example-aids-data-7">Example: <code>Aids</code> data</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-35-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="2500"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p>From both residuals plots it is evident that three observations are highly underestimated, while the others are slightly underestimated.</p></li>
<li><p>The lack of fit can be solved, in this case, by using a different link function.</p></li>
</ul>
</section>
<section id="example-aids-data-8" class="level2">
<h2 class="anchored" data-anchor-id="example-aids-data-8">Example: <code>Aids</code> data</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-36-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="2500"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p>We estimated a Poisson regression model with a <span class="blue">non-canonical link</span> function <span class="math inline">g(\mu_i) = \sqrt{\mu_i}</span>.</p></li>
<li><p>This yields a much <span class="blue">better fit</span>, as we previously discussed. The residuals are also better behaved.</p></li>
</ul>
</section>
</section>
<section id="model-selection" class="level1">
<h1>Model selection</h1>
<section id="model-selection-process" class="level2">
<h2 class="anchored" data-anchor-id="model-selection-process">Model selection process</h2>
<ul>
<li><p><span class="blue">Model selection</span> for GLMs faces the same issues as for linear models.</p></li>
<li><p>The selection process becomes more difficult as the number of explanatory variables <span class="math inline">p</span> increases, because of the growth in possible effects and interactions. There are two competing goals:</p>
<ol type="a">
<li>The model should be complex enough to <span class="blue">fit the data well</span>;</li>
<li>On the other hand, it should <span class="orange">smooth rather than overfit</span> the data and ideally remain <span class="blue">simple to interpret</span>.</li>
</ol></li>
</ul>
<ul>
<li><p>Most research studies are designed to answer certain questions, which guide the choice.</p></li>
<li><p><span class="blue">Confirmatory analyses</span> use a restricted set of models, e.g.&nbsp;for testing a study hypothesis about an effect by comparing models <span class="orange">with and without</span> that effect.</p></li>
<li><p><span class="blue">Exploratory studies</span>, instead, search among possible models which may provide clues about the structure of effects or can raise questions for future research.</p></li>
</ul>
<ul>
<li>In either case, it is helpful first to study the <span class="blue">marginal effect</span> of each predictor. Use <span class="orange">descriptive statistics</span> and a scatterplot matrix to get a feel for those effects.</li>
</ul>
</section>
<section id="automatic-model-selection" class="level2">
<h2 class="anchored" data-anchor-id="automatic-model-selection">Automatic model selection</h2>
<ul>
<li><p>With <span class="math inline">p</span> <span class="blue">explanatory variables</span>, the number of potential models is the huge number <span class="math display">
\sum_{k=1}^p \binom{p}{k} = 2^p.
</span></p></li>
<li><p><span class="blue">Best subset selection</span> identifies the model that optimizes an <span class="blue">information criterion</span> e.g.&nbsp;AIC or BIC, which are defined as <span class="math display">
\textsf{AIC} = - 2\ell(\hat{\beta}) + 2k, \qquad\textsf{BIC} = - 2\ell(\hat{\beta}) + k\log{n},
</span> where <span class="math inline">k</span> is the number of parameters in the model.</p></li>
<li><p>Best subset selection is <span class="orange">computationally intensive</span> when <span class="math inline">p</span> is large, to the extent that it is not even feasible in most cases, but approximations such as <span class="blue">forward</span> and <span class="blue">backward</span> selection are possible.</p></li>
</ul>
<ul>
<li><p>In <span class="blue">exploratory studies</span>, these methods are useful if applied <span class="blue">cautiously</span>.</p></li>
<li><p>As we shall discuss, an excess of automatism may lead to good predictive performance, but it may fail in making the model simple or interpretable.</p></li>
</ul>
</section>
<section id="stepwise-procedures-forward-and-backward-selection" class="level2">
<h2 class="anchored" data-anchor-id="stepwise-procedures-forward-and-backward-selection">Stepwise procedures: forward and backward selection</h2>
<ul>
<li><p><span class="blue">Forward selection</span> adds terms sequentially. At each stage it selects the term giving the greatest improvement in terms of <span class="orange">deviance</span> or other goodness of fit measures.</p></li>
<li><p>The process stops when further additions do not improve the it, according to statistical significance (i.e.&nbsp;a log-likelihood ratio test) or a criterion for judging the model fit (such as the AIC or BIC).</p></li>
<li><p>A stepwise variation of this procedure rechecks, at each stage, whether terms added at previous stages are still needed.</p></li>
</ul>
<ul>
<li><p><span class="orange">Backward elimination</span> begins with a complex model and sequentially removes terms.</p></li>
<li><p>At each stage, it selects the term whose removal has the least damaging effect on the model, such as the largest p-value in a test or the least deterioration in a criterion for judging the model fit.</p></li>
<li><p>The process stops when any further deletion leads to a poorer it.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Whenever possible, i.e.&nbsp;when <span class="math inline">p</span> is not too large, we recommend <span class="blue">manually</span> performing each stage of forward or backward procedures and <span class="orange">avoid fully automatic procedures</span>.</p>
</div>
</div>
</div>
</section>
<section id="comments-on-forwardbackward-selection-i" class="level2">
<h2 class="anchored" data-anchor-id="comments-on-forwardbackward-selection-i">Comments on forward/backward selection I</h2>
<ul>
<li><p>An <span class="blue">interaction term</span> should not be included without its <span class="orange">main effects</span>.</p></li>
<li><p>For <span class="blue">qualitative predictors</span> with <span class="math inline">&gt;2</span> categories: add/drop the <span class="orange">entire variable</span>, not just one indicator. Otherwise, results depend on the reference category used in coding.</p></li>
</ul>
<ul>
<li><p>Some statisticians prefer <span class="blue">backward elimination</span> over <span class="orange">forward selection</span>. It is safer to delete terms from an overly complex model than to <span class="orange">add</span> to an overly simple one.</p></li>
<li><p>Forward selection based on significance tests:</p>
<ol type="a">
<li>May stop prematurely if a test has <span class="blue">low power</span>.<br>
</li>
<li>Early-stage comparisons often involve <span class="orange">inadequate models</span>, making tests <span class="blue">questionable</span>.</li>
</ol></li>
</ul>
<ul>
<li><p>Neither backward nor forward strategies guarantee a <span class="blue">meaningful model</span>.</p></li>
<li><p>Evaluating many terms increases risk of <span class="blue">chance findings</span>. If true effects are <span class="orange">weak</span>, the largest sample effect likely <span class="blue">overestimates</span> the truth.</p></li>
</ul>
<ul>
<li>Use of standard significance tests in selection lacks <span class="orange">theoretical justification</span>. Distribution of minimum or maximum <span class="math inline">p</span>-values at each stage is not the same as the distribution of a <span class="blue">pre-selected variable</span>. This issue is called <span class="blue">multiple testing</span> and leads to <span class="orange">overconfident</span> conclusions.</li>
</ul>
</section>
<section id="comments-on-forwardbackward-selection-ii" class="level2">
<h2 class="anchored" data-anchor-id="comments-on-forwardbackward-selection-ii">Comments on forward/backward selection II</h2>
<ul>
<li><p>Statistical significance is not the same as <span class="blue">practical significance</span>; do not rely only on significance tests.</p></li>
<li><p>The price to pay for adding an irrelevant variable is an increase in <span class="blue">variance</span> of the estimates. The price to pay for dropping a relevant variable is an increase in <span class="orange">bias</span>.</p></li>
</ul>
<ul>
<li>It is possible to include variables central to the study goals even if <span class="orange">not significant</span>:
<ul>
<li>It enables comparisons with other studies where the effect is significant, perhaps because of a larger sample size;</li>
<li>If the variable is a potential <span class="orange">confounder</span>, i.e.&nbsp;possibly relevant for predicting the response, but not of direct interest, including it in the model may help to reduce bias in estimating relevant effects of key explanatory variables.</li>
</ul></li>
</ul>
<ul>
<li>Do not keep variables just because they are <span class="blue">significant</span>.
<ul>
<li>As an example, consider an adjusted <span class="math inline">R^2 = 0.39</span> in a linear model with interactions vs.&nbsp;<span class="math inline">0.38</span> without. The simpler model may be preferable being more interpretable.</li>
</ul></li>
</ul>
<ul>
<li>Algorithmic selection methods are no substitute for careful thought in model building.</li>
</ul>
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Agresti2015" class="csl-entry" role="listitem">
Agresti, A. (2015), <em><span class="nocase">Foundations of Linear and Generalized Linear Models</span></em>, Wiley.
</div>
<div id="ref-Azzalini2008" class="csl-entry" role="listitem">
Azzalini, A. (2008), <em>Inferenza statistica</em>, Springer Verlag.
</div>
<div id="ref-Efron2023" class="csl-entry" role="listitem">
Efron, B. (2023), <em><span class="nocase">Exponential Families in Theory and Practice</span></em>, Cambridge University Press.
</div>
<div id="ref-Fisher1934" class="csl-entry" role="listitem">
Fisher, R. A. (1934), <span>“<span class="nocase">Two new properties of mathematical likelihood</span>,”</span> <em>Proceedings of the Royal Society of London. Series A</em>, 144, 285–307.
</div>
<div id="ref-Nelder1972" class="csl-entry" role="listitem">
Nelder, J. A., and Wedderburn, R. W. M. (1972), <span>“<span class="nocase">Generalized linear models</span>,”</span> <em>Journal of the Royal Statistical Society. Series A: Statistics in Society</em>, 135, 370–384.
</div>
<div id="ref-Salvan2020" class="csl-entry" role="listitem">
Salvan, A., Sartori, N., and Pace, L. (2020), <em>Modelli lineari generalizzati</em>, Springer.
</div>
</div>
</section>
</section>


</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<script src="un_B_files/libs/quarto-html/zenscroll-min.js"></script>
</body></html>