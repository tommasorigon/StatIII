<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tommaso Rigon">

<title>Linear models and misspecification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="un_A_files/libs/clipboard/clipboard.min.js"></script>
<script src="un_A_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="un_A_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="un_A_files/libs/quarto-html/popper.min.js"></script>
<script src="un_A_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="un_A_files/libs/quarto-html/anchor.min.js"></script>
<link href="un_A_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="un_A_files/libs/quarto-html/quarto-syntax-highlighting-2dd8675203c32411a5a6bac4cfca86ce.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="un_A_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="un_A_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="un_A_files/libs/bootstrap/bootstrap-f461a50d3c54b5bcd96bf8a6f9e0f9b8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-full">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#homepage" id="toc-homepage" class="nav-link active" data-scroll-target="#homepage">Homepage</a></li>
  <li><a href="#the-modeling-process" id="toc-the-modeling-process" class="nav-link" data-scroll-target="#the-modeling-process">The modeling process</a>
  <ul class="collapse">
  <li><a href="#car-data-diesel-or-gas" id="toc-car-data-diesel-or-gas" class="nav-link" data-scroll-target="#car-data-diesel-or-gas">Car data (<span class="blue">diesel</span> or <span class="orange">gas</span>)</a></li>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear regression</a></li>
  <li><a href="#regression-models" id="toc-regression-models" class="nav-link" data-scroll-target="#regression-models">Regression models</a></li>
  <li><a href="#linear-models" id="toc-linear-models" class="nav-link" data-scroll-target="#linear-models">Linear models</a></li>
  <li><a href="#matrix-notation" id="toc-matrix-notation" class="nav-link" data-scroll-target="#matrix-notation">Matrix notation</a></li>
  <li><a href="#linear-regression-estimation-i" id="toc-linear-regression-estimation-i" class="nav-link" data-scroll-target="#linear-regression-estimation-i">Linear regression: estimation I</a></li>
  <li><a href="#linear-regression-estimation-ii" id="toc-linear-regression-estimation-ii" class="nav-link" data-scroll-target="#linear-regression-estimation-ii">Linear regression: estimation II</a></li>
  <li><a href="#linear-regression-inference" id="toc-linear-regression-inference" class="nav-link" data-scroll-target="#linear-regression-inference">Linear regression: inference</a></li>
  <li><a href="#linear-regression-diagnostic" id="toc-linear-regression-diagnostic" class="nav-link" data-scroll-target="#linear-regression-diagnostic">Linear regression: diagnostic</a></li>
  <li><a href="#leverages-outliers-and-influence-points" id="toc-leverages-outliers-and-influence-points" class="nav-link" data-scroll-target="#leverages-outliers-and-influence-points">Leverages, outliers and influence points</a></li>
  <li><a href="#a-first-model-estimated-coefficients" id="toc-a-first-model-estimated-coefficients" class="nav-link" data-scroll-target="#a-first-model-estimated-coefficients">A first model: estimated coefficients</a></li>
  <li><a href="#a-first-model-fitted-values" id="toc-a-first-model-fitted-values" class="nav-link" data-scroll-target="#a-first-model-fitted-values">A first model: fitted values</a></li>
  <li><a href="#a-first-model-graphical-diagnostics" id="toc-a-first-model-graphical-diagnostics" class="nav-link" data-scroll-target="#a-first-model-graphical-diagnostics">A first model: graphical diagnostics</a></li>
  <li><a href="#comments-and-criticisms" id="toc-comments-and-criticisms" class="nav-link" data-scroll-target="#comments-and-criticisms">Comments and criticisms</a></li>
  <li><a href="#linear-models-and-non-linear-patterns" id="toc-linear-models-and-non-linear-patterns" class="nav-link" data-scroll-target="#linear-models-and-non-linear-patterns">Linear models and non-linear patterns</a></li>
  <li><a href="#second-model-fitted-values" id="toc-second-model-fitted-values" class="nav-link" data-scroll-target="#second-model-fitted-values">Second model: fitted values</a></li>
  <li><a href="#second-model-graphical-diagnostics" id="toc-second-model-graphical-diagnostics" class="nav-link" data-scroll-target="#second-model-graphical-diagnostics">Second model: graphical diagnostics</a></li>
  <li><a href="#comments-and-criticisms-1" id="toc-comments-and-criticisms-1" class="nav-link" data-scroll-target="#comments-and-criticisms-1">Comments and criticisms</a></li>
  <li><a href="#a-third-model-additional-variables" id="toc-a-third-model-additional-variables" class="nav-link" data-scroll-target="#a-third-model-additional-variables">A third model: additional variables</a></li>
  <li><a href="#a-third-model-graphical-diagnostics" id="toc-a-third-model-graphical-diagnostics" class="nav-link" data-scroll-target="#a-third-model-graphical-diagnostics">A third model: graphical diagnostics</a></li>
  <li><a href="#comments-and-criticisms-2" id="toc-comments-and-criticisms-2" class="nav-link" data-scroll-target="#comments-and-criticisms-2">Comments and criticisms</a></li>
  </ul></li>
  <li><a href="#misspecification-and-remedies" id="toc-misspecification-and-remedies" class="nav-link" data-scroll-target="#misspecification-and-remedies">Misspecification and remedies</a>
  <ul class="collapse">
  <li><a href="#assumptions-and-misspecification" id="toc-assumptions-and-misspecification" class="nav-link" data-scroll-target="#assumptions-and-misspecification">Assumptions and misspecification</a></li>
  <li><a href="#robust-estimation-and-assumptions" id="toc-robust-estimation-and-assumptions" class="nav-link" data-scroll-target="#robust-estimation-and-assumptions">Robust estimation and assumptions</a></li>
  <li><a href="#non-normality-of-the-errors-i" id="toc-non-normality-of-the-errors-i" class="nav-link" data-scroll-target="#non-normality-of-the-errors-i">Non-normality of the errors I üìñ</a></li>
  <li><a href="#non-normality-of-the-errors-ii" id="toc-non-normality-of-the-errors-ii" class="nav-link" data-scroll-target="#non-normality-of-the-errors-ii">Non-normality of the errors II</a></li>
  <li><a href="#heteroschedasticity-of-the-errors-i" id="toc-heteroschedasticity-of-the-errors-i" class="nav-link" data-scroll-target="#heteroschedasticity-of-the-errors-i">Heteroschedasticity of the errors I üìñ</a></li>
  <li><a href="#heteroschedasticity-of-the-errors-ii" id="toc-heteroschedasticity-of-the-errors-ii" class="nav-link" data-scroll-target="#heteroschedasticity-of-the-errors-ii">Heteroschedasticity of the errors II</a></li>
  <li><a href="#weighted-least-squares-i" id="toc-weighted-least-squares-i" class="nav-link" data-scroll-target="#weighted-least-squares-i">Weighted least squares I üìñ</a></li>
  <li><a href="#weighted-least-squares-ii" id="toc-weighted-least-squares-ii" class="nav-link" data-scroll-target="#weighted-least-squares-ii">Weighted least squares II üìñ</a></li>
  <li><a href="#variable-transformations" id="toc-variable-transformations" class="nav-link" data-scroll-target="#variable-transformations">Variable transformations</a></li>
  <li><a href="#box-cox-transform" id="toc-box-cox-transform" class="nav-link" data-scroll-target="#box-cox-transform">Box-Cox transform</a></li>
  <li><a href="#box-cox-transform-derivation-i" id="toc-box-cox-transform-derivation-i" class="nav-link" data-scroll-target="#box-cox-transform-derivation-i">Box-Cox transform: derivation I üìñ</a></li>
  <li><a href="#box-cox-transform-derivation-ii" id="toc-box-cox-transform-derivation-ii" class="nav-link" data-scroll-target="#box-cox-transform-derivation-ii">Box-Cox transform: derivation II üìñ</a></li>
  <li><a href="#box-cox-transform-for-the-auto-dataset" id="toc-box-cox-transform-for-the-auto-dataset" class="nav-link" data-scroll-target="#box-cox-transform-for-the-auto-dataset">Box-Cox transform for the auto dataset</a></li>
  <li><a href="#a-fourth-model-graphical-diagnostics" id="toc-a-fourth-model-graphical-diagnostics" class="nav-link" data-scroll-target="#a-fourth-model-graphical-diagnostics">A fourth model: graphical diagnostics</a></li>
  <li><a href="#variance-stabilizing-transformations-i" id="toc-variance-stabilizing-transformations-i" class="nav-link" data-scroll-target="#variance-stabilizing-transformations-i">Variance stabilizing transformations I üìñ</a></li>
  <li><a href="#variance-stabilizing-transformations-ii" id="toc-variance-stabilizing-transformations-ii" class="nav-link" data-scroll-target="#variance-stabilizing-transformations-ii">Variance stabilizing transformations II üìñ</a></li>
  <li><a href="#limitations-of-variable-transformations-i" id="toc-limitations-of-variable-transformations-i" class="nav-link" data-scroll-target="#limitations-of-variable-transformations-i">Limitations of variable transformations I</a></li>
  <li><a href="#limitations-of-variable-transformations-ii" id="toc-limitations-of-variable-transformations-ii" class="nav-link" data-scroll-target="#limitations-of-variable-transformations-ii">Limitations of variable transformations II</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="un_A_slides.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></nav>
</div>
<main class="content page-columns page-full column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Linear models and misspecification</h1>
<p class="subtitle lead">Statistics III - CdL SSE</p>
</div>


<div class="quarto-title-meta-author column-page-left">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><span class="orange">Tommaso Rigon</span> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <em>Universit√† degli Studi di Milano-Bicocca</em>
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-page-left">

      
  
    
  </div>
  


</header>


<section id="homepage" class="level2">
<h2 class="anchored" data-anchor-id="homepage"><a href="../index.html">Homepage</a></h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><img src="img/ABC.png" class="img-fluid"></p>
<p><em>‚ÄúEverything should be made as simple as possible, but not simpler‚Äù</em></p>
<p>Attributed to <span class="grey">Albert Einstein</span></p>
</div><div class="column" style="width:60%;">
<ul>
<li><p>This unit will cover the following <span class="orange">topics</span>:</p>
<ul>
<li><span class="blue">Recap</span>: linear models and the modeling process</li>
<li>Robustness of OLS estimates, sandwich estimators</li>
<li>Weighted least squares</li>
<li>Box-Cox transform, variance stabilizing transformations</li>
</ul></li>
<li><p>The main theme is: what should we do when the <span class="blue">assumptions</span> of linear models are <span class="orange">violated</span>?</p></li>
<li><p>We will push the linear model to its limit, using it even when is not supposed to work.</p></li>
<li><p>The symbol üìñ means that a few extra steps are discussed in the <span class="blue">handwritten notes</span>.</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The content of this Unit is covered in <span class="orange">Chapter 1</span> of <span class="citation" data-cites="Salvan2020">Salvan et al. (<a href="#ref-Salvan2020" role="doc-biblioref">2020</a>)</span>. Alternatively, see <span class="blue">Chapter 2</span> of <span class="citation" data-cites="Agresti2015">Agresti (<a href="#ref-Agresti2015" role="doc-biblioref">2015</a>)</span> or <span class="grey">Chapter 5</span> of <span class="citation" data-cites="Azzalini2008">Azzalini (<a href="#ref-Azzalini2008" role="doc-biblioref">2008</a>)</span>.</p>
</div>
</div>
</div>
</section>
<section id="the-modeling-process" class="level1">
<h1>The modeling process</h1>
<section id="car-data-diesel-or-gas" class="level2">
<h2 class="anchored" data-anchor-id="car-data-diesel-or-gas">Car data (<span class="blue">diesel</span> or <span class="orange">gas</span>)</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="540"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>We consider data for <span class="math inline">n = 203</span> models of cars in circulation in 1985 in the USA.</li>
<li>We want to <span class="blue">predict</span> the distance per unit of fuel as a function of the vehicle features.</li>
<li>We consider the following <span class="orange">variables</span>:
<ul>
<li>The city distance per unit of fuel (km/L, <code>city.distance</code>)</li>
<li>The engine size (L, <code>engine.size</code>)</li>
<li>The number of cylinders (<code>n.cylinders</code>)</li>
<li>The curb weight (kg, <code>curb.weight</code>)</li>
<li>The fuel type (gasoline or diesel, <code>fuel</code>).</li>
</ul></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>We assume you are already familiar with linear models. The following is a brief recap rather than a full discussion.</p>
</div>
</div>
</div>
</section>
<section id="linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression">Linear regression</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="360"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li><p>Let us consider the variables <code>city.distance</code> (<span class="math inline">y</span>), <code>engine.size</code> (<span class="math inline">x</span>) and <code>fuel</code> (<span class="math inline">z</span>).</p></li>
<li><p>A <span class="blue">simple linear regression</span> <span class="math display">
Y_i = \beta_1 + \beta_2 x_i + \epsilon_i, \qquad i=1,\dots,n,
</span> could be easily fit by least squares‚Ä¶</p></li>
<li><p>‚Ä¶ but the plot suggests that the relationship between <code>city.distance</code> and <code>engine.size</code> is <span class="orange">not</span> well approximated by a <span class="orange">linear</span> function.</p></li>
<li><p>‚Ä¶ and also that <code>fuel</code> has a non-negligible effect on the response.</p></li>
</ul>
</div>
</div>
</section>
<section id="regression-models" class="level2">
<h2 class="anchored" data-anchor-id="regression-models">Regression models</h2>
<div class="incremental">
<ul class="incremental">
<li><p>A <span class="orange">general</span> and <span class="orange">more flexible formulation</span> for modeling the relationship between a vector of <span class="blue">fixed covariates</span> <span class="math inline">\boldsymbol{x}_i = (x_{i1},\dots,x_{ip})^T \in \mathbb{R}^p</span> and a random variable <span class="math inline">Y_i \in \mathbb{R}</span> is <span class="math display">
Y_i = f(\boldsymbol{x}_i; \beta) + \epsilon_i, \qquad i=1,\dots,n,
</span> where the ‚Äúerrors‚Äù <span class="math inline">\epsilon_i</span> are iid random variables, having zero mean and variance <span class="math inline">\sigma^2</span>.</p></li>
<li><p>To estimate the unknown parameters <span class="math inline">\beta</span>, a possibility is to rely on the <span class="blue">least squares criterion</span>: we seek the <span class="orange">minimum</span> of the objective function <span class="math display">
D(\beta) = \sum_{i=1}^n\{y_i - f(\boldsymbol{x}_i; \beta)\}^2,
</span> using <span class="math inline">n</span> pairs of covariates <span class="math inline">\boldsymbol{x}_i = (x_{i1},\dots,x_{ip})^T</span> and the observed realizations <span class="math inline">y_i</span> of the random variables <span class="math inline">Y_i</span>, for <span class="math inline">i = 1,\dots,n</span>. The <span class="orange">optimal value</span> is denoted by <span class="math inline">\hat{\beta}</span>.</p></li>
<li><p>The <span class="blue">predicted values</span> are <span class="math inline">\hat{y}_i = \widehat{\mathbb{E}(Y_i)} = f(\boldsymbol{x}_i; \hat{\beta})</span>, for <span class="math inline">i=1,\dots,n.</span></p></li>
</ul>
</div>
</section>
<section id="linear-models" class="level2">
<h2 class="anchored" data-anchor-id="linear-models">Linear models</h2>
<ul>
<li><p>Let us consider again the variables <code>city.distance</code> (<span class="math inline">y</span>), <code>engine.size</code> (<span class="math inline">x</span>) and <code>fuel</code> (<span class="math inline">z</span>).</p></li>
<li><p>Which function <span class="math inline">f(x,z;\beta)</span> should we choose?</p></li>
</ul>
<ul>
<li>A first attempt is to consider a <span class="orange">polynomial term</span> combined with a <span class="blue">dummy variable</span> <span class="math display">
f(x, z; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \beta_4 x^3 + \beta_5 I(z = \texttt{gas}),
</span> which is a special instance of <span class="orange">linear model</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Linear model
</div>
</div>
<div class="callout-body-container callout-body">
<p>In a <span class="blue">linear model</span> the response variable <span class="math inline">Y_i</span> is related to the covariates through the function<span class="math display">
    \mathbb{E}(Y_i) =f(\boldsymbol{x}_i; \beta) = \beta_1 x_{i1} + \cdots + \beta_p x_{ip} =\boldsymbol{x}_i^T\beta,
    </span> where <span class="math inline">\boldsymbol{x}_i = (x_{i1},\dots,x_{ip})^T</span> is a vector of <span class="orange">covariates</span> and <span class="math inline">\beta = (\beta_1,\dots,\beta_p)^T</span> is the corresponding vector of <span class="orange">coefficients</span>.</p>
</div>
</div>
</section>
<section id="matrix-notation" class="level2">
<h2 class="anchored" data-anchor-id="matrix-notation">Matrix notation</h2>
<ul>
<li><p>The <span class="orange">response random variables</span> are collected in the random vector <span class="math inline">\boldsymbol{Y} = (Y_1,\dots,Y_n)^T</span>, whose <span class="blue">observed realization</span> is <span class="math inline">\boldsymbol{y} = (y_1,\dots,y_n)^T</span>.</p></li>
<li><p>The <span class="blue">design matrix</span> is a <span class="math inline">n \times p</span> matrix, comprising the covariate‚Äôs values, defined by <span class="math display">
\boldsymbol{X} =
\begin{bmatrix}
x_{11} &amp; \cdots &amp; x_{1p}\\
\vdots &amp; \ddots &amp; \vdots \\
x_{n1} &amp; \cdots &amp; x_{np}
\end{bmatrix}.
</span></p></li>
</ul>
<ul>
<li>The <span class="math inline">j</span>th variable (column) is denoted with <span class="math inline">\tilde{\boldsymbol{x}}_j</span>, whereas the <span class="math inline">i</span>th observation (row) is <span class="math inline">\boldsymbol{x}_i</span>: <span class="math display">
\boldsymbol{X} = (\tilde{\boldsymbol{x}}_1,\dots,\tilde{\boldsymbol{x}}_p) = (\boldsymbol{x}_1, \dots,\boldsymbol{x}_n)^T.
</span></li>
</ul>
<ul>
<li>Then, a <span class="blue">linear model</span> can be written using the <span class="orange">compact notation</span>: <span class="math display">
\boldsymbol{Y} = \boldsymbol{X}\beta + \boldsymbol{\epsilon},
</span> where <span class="math inline">\boldsymbol{\epsilon} = (\epsilon_1,\dots,\epsilon_n)^T</span> is a vector of iid error terms with zero mean and variance <span class="math inline">\sigma^2</span>.</li>
</ul>
</section>
<section id="linear-regression-estimation-i" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-estimation-i">Linear regression: estimation I</h2>
<ul>
<li>The optimal set of coefficients <span class="math inline">\hat{\beta}</span> is the minimizer of the <span class="orange">least squared criterion</span> <span class="math display">
D(\beta) = (\boldsymbol{y} - \boldsymbol{X}\beta)^T(\boldsymbol{y} - \boldsymbol{X}\beta) = ||\boldsymbol{y} - \boldsymbol{X}\beta||^2,
</span> also known as <span class="orange">residual sum of squares (RSS)</span>, where <span class="math display">
||\boldsymbol{y}|| = \sqrt{y_1^2 + \cdots + y_n^2},</span> denotes the <span class="blue">Euclidean norm</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Least square estimate (OLS)
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the design matrix has <span class="blue">full rank</span>, that is, if <span class="math inline">\text{rk}(\boldsymbol{X}^T\boldsymbol{X}) = p</span>, then the <span class="orange">least square estimate</span> has an explicit solution: <span class="math display">
    \hat{\beta} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T \boldsymbol{y}.
    </span></p>
</div>
</div>
</section>
<section id="linear-regression-estimation-ii" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-estimation-ii">Linear regression: estimation II</h2>
<ul>
<li>In matrix notation, the predicted values can be obtained as <span class="math display">
\hat{\boldsymbol{y}} = \boldsymbol{X}\hat{\beta} = \boldsymbol{H}\boldsymbol{y}, \qquad \boldsymbol{H} = \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T,
</span> where <span class="math inline">\boldsymbol{H}</span> is a <span class="math inline">n \times n</span> <span class="orange">projection matrix</span> matrix sometimes called <span class="blue">hat matrix</span>. The matrix is idempotent, meaning that <span class="math inline">\boldsymbol{H} = \boldsymbol{H}^T</span> and <span class="math inline">\boldsymbol{H}^2 = \boldsymbol{H}</span>.</li>
</ul>
<ul>
<li><p>The quantity <span class="math inline">D(\hat{\beta})</span> is the so-called <span class="blue">deviance</span>, which is equal to <span class="math display">
D(\hat{\beta}) = ||\boldsymbol{y} - \hat{\boldsymbol{y}}||^2 = \boldsymbol{y}^T(I_n - \boldsymbol{H})\boldsymbol{y}.
</span></p></li>
<li><p>Moreover, a typical estimate for the <span class="orange">residual variance</span> <span class="math inline">\sigma^2</span> is obtained as follows: <span class="math display">
s^2 = \frac{D(\hat{\beta})}{n - p} = \frac{1}{n-p}\sum_{i=1}^n(y_i - \boldsymbol{x}_i^T\hat{\beta})^2.
</span></p></li>
<li><p>To evaluate the goodness of fit, we can calculate the <span class="orange">coefficient of determination</span>: <span class="math display">
R^2 = 1 - \frac{\textsf{(``Residual deviance'')}}{\textsf{(``Total deviance'')}} = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n(y_i - \bar{y})^2}.
</span></p></li>
</ul>
</section>
<section id="linear-regression-inference" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-inference">Linear regression: inference</h2>
<ul>
<li><p>Recall that the errors <span class="math inline">\boldsymbol{\epsilon}</span> have zero mean <span class="math inline">\mathbb{E}(\boldsymbol{\epsilon}) = 0</span> and are <span class="orange">uncorrelated</span> <span class="math inline">\text{var}(\boldsymbol{\epsilon}) = \sigma^2I_n</span>.</p></li>
<li><p>Then, the estimator <span class="math inline">\hat{\beta}</span> is <span class="orange">unbiased</span> <span class="math inline">\mathbb{E}(\hat{\beta}) = \beta</span> and its <span class="blue">variance</span> is <span class="math inline">\text{var}(\hat{\beta}) = \sigma^2 (\boldsymbol{X}^T\boldsymbol{X})^{-1}</span>. Since <span class="math inline">\sigma^2</span> is also unknown, we can estimate the variances of <span class="math inline">\hat{\beta}</span> as follows: <span class="math display">
  \widehat{\text{var}}(\hat{\beta}) = s^2 (\boldsymbol{X}^T\boldsymbol{X})^{-1}.
  </span></p></li>
<li><p>The <span class="orange">standard errors</span> of the components of <span class="math inline">\hat{\beta}</span> correspond to the square root of the diagonal of the above covariance matrix.</p></li>
</ul>
<ul>
<li><p>Let us additionally assume that the errors follow a Gaussian distribution: <span class="math inline">\epsilon_i \overset{\text{iid}}{\sim} \text{N}(0, \sigma^2)</span>.</p></li>
<li><p>This implies that the <span class="orange">distribution</span> of the <span class="orange">estimator</span> <span class="math inline">\hat{\beta}</span> is <span class="math display">
\hat{\beta} \sim \text{N}_p(\beta, \sigma^2 (\boldsymbol{X}^T\boldsymbol{X})^{-1}).
</span></p></li>
<li><p>Confidence interval and Wald‚Äôs tests can be obtained through classical inferential theory.</p></li>
</ul>
</section>
<section id="linear-regression-diagnostic" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-diagnostic">Linear regression: diagnostic</h2>
<ul>
<li><p>The diagonal elements <span class="math inline">h_i \in [0, 1]</span> of the matrix <span class="math inline">\boldsymbol{H}</span> are called <span class="blue">leverages</span> and it holds <span class="math display">
\text{var}(\hat{Y}_i) = \sigma^2 h_i, \qquad \text{var}(Y_i - \hat{Y}_i) = \sigma^2 (1 - h_i), \qquad \text{cor}(Y_i, \hat{Y}_i) = \sqrt{h_i}.
</span> The leverage <span class="math inline">h_i</span> determines the <span class="orange">precision</span> with which <span class="math inline">\hat{Y}_i</span> predicts <span class="math inline">Y_i</span>. For large <span class="math inline">h_i</span> close to <span class="math inline">1</span>, <span class="math inline">\text{cor}(Y_i, \hat{Y}_i) \approx 1</span>, therefore changes of a single point <span class="math inline">Y_i</span> leads to significant changes in <span class="math inline">\hat{Y}_i</span>.</p></li>
<li><p>Leverages also appear in the definition of <span class="blue">standardized residuals</span>: <span class="math display">
\tilde{r}_i = \frac{r_i}{\sqrt{s^2(1 - h_i)}} = \frac{y_i - \boldsymbol{x}_i^T\hat{\beta}}{\sqrt{s^2(1 - h_i)}},
</span> where <span class="math inline">r_i = y_i - \boldsymbol{x}_i^T\hat{\beta}</span> are the (raw) <span class="orange">residuals</span>.</p></li>
</ul>
<ul>
<li>An observation is <span class="blue">influent</span> if it has high leverage and high squared residual. <span class="orange">Cook‚Äôs distance</span> <span class="math inline">c_i</span> is based on the change in <span class="math inline">\hat{\beta}</span> when the observation is removed: <span class="math display">
p \cdot c_i = (\hat{\beta} - \hat{\beta}_{-i})^T\widehat{\text{var}}(\hat{\beta})^{-1}(\hat{\beta} - \hat{\beta}_{-i}) = \tilde{r}_i^2 \frac{h_i}{p(1 - h_i)}.
</span> Cook‚Äôs distance is considered relatively large when <span class="math inline">c_i \ge 1</span>.</li>
</ul>
</section>
<section id="leverages-outliers-and-influence-points" class="level2">
<h2 class="anchored" data-anchor-id="leverages-outliers-and-influence-points">Leverages, outliers and influence points</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="990"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><span class="orange">Left plot</span>: leverage, not outlier. <span class="blue">Central plot</span>: outlier, not leverage. <span class="grey">Right plot</span>: influence point = leverage + outlier.</li>
</ul>
</section>
<section id="a-first-model-estimated-coefficients" class="level2">
<h2 class="anchored" data-anchor-id="a-first-model-estimated-coefficients">A first model: estimated coefficients</h2>
<ul>
<li><p>Our first attempt for predicting <code>city.distance</code> (<span class="math inline">y</span>) via <code>engine.size</code> (<span class="math inline">x</span>) and <code>fuel</code> (<span class="math inline">z</span>) is: <span class="math display">
  f(x, z; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \beta_4 x^3 + \beta_5 I(z = \texttt{gas}).
  </span></p></li>
<li><p>We obtain the following <span class="orange">summary</span> for the regression coefficients <span class="math inline">\hat{\beta}</span>.</p></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>(Intercept)</code></td>
<td style="text-align: right;">28.045</td>
<td style="text-align: right;">3.076</td>
<td style="text-align: right;">9.119</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>engine.size</code></td>
<td style="text-align: right;">-10.980</td>
<td style="text-align: right;">3.531</td>
<td style="text-align: right;">-3.109</td>
<td style="text-align: right;">0.002</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>engine.size^2</code></td>
<td style="text-align: right;">2.098</td>
<td style="text-align: right;">1.271</td>
<td style="text-align: right;">1.651</td>
<td style="text-align: right;">0.100</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>engine.size^3</code></td>
<td style="text-align: right;">-0.131</td>
<td style="text-align: right;">0.139</td>
<td style="text-align: right;">-0.939</td>
<td style="text-align: right;">0.349</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>fuel_gas</code></td>
<td style="text-align: right;">-3.214</td>
<td style="text-align: right;">0.427</td>
<td style="text-align: right;">-7.523</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
<ul>
<li>Moreover, the coefficient <span class="math inline">R^2</span> and the residual standard deviation <span class="math inline">s</span> are:</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">r.squared</th>
<th style="text-align: right;">sigma</th>
<th style="text-align: right;">deviance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.5973454</td>
<td style="text-align: right;">1.790362</td>
<td style="text-align: right;">634.6687</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="a-first-model-fitted-values" class="level2">
<h2 class="anchored" data-anchor-id="a-first-model-fitted-values">A first model: fitted values</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="702"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="a-first-model-graphical-diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="a-first-model-graphical-diagnostics">A first model: graphical diagnostics</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="702"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="comments-and-criticisms" class="level2">
<h2 class="anchored" data-anchor-id="comments-and-criticisms">Comments and criticisms</h2>
<ul>
<li><p>Is this a good model?</p></li>
<li><p>The overall fit <span class="blue">seems satisfactory</span> at first glance, especially if we aim at predicting the urban distance of cars when average engine size (i.e., between <span class="math inline">1.5L</span> and <span class="math inline">3L</span>).</p></li>
</ul>
<ul>
<li>However, the plot of the <span class="orange">residuals</span> <span class="math inline">r_i = y_i - \hat{y}_i</span> suggests that the homoschedasticity assumption, i.e.&nbsp;<span class="math inline">\text{var}(\epsilon_i) = \sigma^2</span>, might be violated.</li>
</ul>
<ul>
<li><p>Also, this model is unsuitable for <span class="orange">extrapolation</span>. Indeed:</p>
<ul>
<li>It has no grounding in physics or engineering, leading to difficulties when interpreting the trend and to paradoxical situations.</li>
<li>For example, the curve of the set of gasoline cars shows a local minimum around <span class="math inline">4.6 L</span> and then rises again!</li>
</ul></li>
<li><p>It is plausible that we can find a better one, so what‚Äôs next?</p></li>
</ul>
</section>
<section id="linear-models-and-non-linear-patterns" class="level2">
<h2 class="anchored" data-anchor-id="linear-models-and-non-linear-patterns">Linear models and non-linear patterns</h2>
<ul>
<li>A significant advantage of linear models is that they can describe non-linear relationships via <span class="blue">variable transformations</span> such as polynomials, logarithms, etc.</li>
</ul>
<ul>
<li>This gives the statistician a lot of modeling flexibility. For instance, we could let: <span class="math display">
\log{Y_i} = \beta_1 + \beta_2 \log{x_i} + \beta_3 I(z_i = \texttt{gas}) + \epsilon_i, \qquad i=1,\dots,n.
</span></li>
</ul>
<ul>
<li>This specification is <span class="orange">linear in the parameters</span>, it fixes the domain issues, and it imposes a monotone relationship between engine size and consumption.</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>(Intercept)</code></td>
<td style="text-align: right;">3.060</td>
<td style="text-align: right;">0.047</td>
<td style="text-align: right;">64.865</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>log(engine.size)</code></td>
<td style="text-align: right;">-0.682</td>
<td style="text-align: right;">0.040</td>
<td style="text-align: right;">-17.129</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>fuel_gas</code></td>
<td style="text-align: right;">-0.278</td>
<td style="text-align: right;">0.038</td>
<td style="text-align: right;">-7.344</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</section>
<section id="second-model-fitted-values" class="level2">
<h2 class="anchored" data-anchor-id="second-model-fitted-values">Second model: fitted values</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="702"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="second-model-graphical-diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="second-model-graphical-diagnostics">Second model: graphical diagnostics</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="702"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="comments-and-criticisms-1" class="level2">
<h2 class="anchored" data-anchor-id="comments-and-criticisms-1">Comments and criticisms</h2>
<ul>
<li>The <span class="blue">goodness of fit</span> indices are the following:</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">r.squared.original</th>
<th style="text-align: right;">r.squared</th>
<th style="text-align: right;">sigma</th>
<th style="text-align: right;">deviance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.5847555</td>
<td style="text-align: right;">0.6196093</td>
<td style="text-align: right;">0.1600278</td>
<td style="text-align: right;">5.121777</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li>Do not mix <span class="blue">apple</span> and <span class="orange">oranges</span>! Compare <span class="math inline">R^2</span>s only if they refer to the same scale!</li>
</ul>
<ul>
<li><p>This second model is <span class="blue">more parsimonious</span>, and yet it reaches satisfactory predictive performance.</p></li>
<li><p>It is also more coherent with the nature of the data: the predictions cannot be negative, and the relationship between engine size and the consumption is monotone.</p></li>
<li><p>Yet, there is still some heteroscedasticity in the residuals ‚Äî is this is due to a missing covariate that has not been included in the model?</p></li>
</ul>
</section>
<section id="a-third-model-additional-variables" class="level2">
<h2 class="anchored" data-anchor-id="a-third-model-additional-variables">A third model: additional variables</h2>
<ul>
<li><p>Let us consider <span class="blue">two additional variables</span>: <code>curb.weight</code> (<span class="math inline">w</span>) and <code>n.cylinders</code> (<span class="math inline">v</span>).</p></li>
<li><p>A richer model, therefore, could be: <span class="math display">
\log{Y_i} = \beta_1 + \beta_2 \log{x_i} +  \beta_3 \log{w_i} + \beta_4 I(z_i = \texttt{gas}) + \beta_5 I(v_i = 2) + \epsilon_i,
  </span> for <span class="math inline">i=1,\dots,n</span>. The estimates are:</p></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>(Intercept)</code></td>
<td style="text-align: right;">9.423</td>
<td style="text-align: right;">0.482</td>
<td style="text-align: right;">19.549</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>log(engine.size)</code></td>
<td style="text-align: right;">-0.180</td>
<td style="text-align: right;">0.051</td>
<td style="text-align: right;">-3.504</td>
<td style="text-align: right;">0.001</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>log(curb.weight)</code></td>
<td style="text-align: right;">-0.943</td>
<td style="text-align: right;">0.072</td>
<td style="text-align: right;">-13.066</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>fuel_gas</code></td>
<td style="text-align: right;">-0.353</td>
<td style="text-align: right;">0.022</td>
<td style="text-align: right;">-15.934</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>cylinders2_TRUE</code></td>
<td style="text-align: right;">-0.481</td>
<td style="text-align: right;">0.052</td>
<td style="text-align: right;">-9.301</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
</section>
<section id="a-third-model-graphical-diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="a-third-model-graphical-diagnostics">A third model: graphical diagnostics</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="702"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="comments-and-criticisms-2" class="level2">
<h2 class="anchored" data-anchor-id="comments-and-criticisms-2">Comments and criticisms</h2>
<ul>
<li>The goodness of fit greatly <span class="blue">improved</span>:</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">r.squared.original</th>
<th style="text-align: right;">r.squared</th>
<th style="text-align: right;">sigma</th>
<th style="text-align: right;">deviance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.869048</td>
<td style="text-align: right;">0.8819199</td>
<td style="text-align: right;">0.0896089</td>
<td style="text-align: right;">1.589891</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li><p>In this third model, we handled the <span class="orange">outliers</span> appearing in the residual plots, which it turns out are identified by the group of cars having 2 cylinders.</p></li>
<li><p>The diagnostic plots are also very much improved, although still not perfect.</p></li>
<li><p>The estimates are coherent with our expectations, based on common knowledge. Have a look at the book (<span class="citation" data-cites="Azzalini2012">Azzalini and Scarpa (<a href="#ref-Azzalini2012" role="doc-biblioref">2012</a>)</span>) for a detailed explanation of <span class="math inline">\beta_4</span>!</p></li>
<li><p>The car dataset is available from the textbook (A&amp;S) website:</p>
<ul>
<li>Dataset <a href="http://azzalini.stat.unipd.it/Book-DM/auto.dat" class="uri">http://azzalini.stat.unipd.it/Book-DM/auto.dat</a></li>
<li>Variable description <a href="http://azzalini.stat.unipd.it/Book-DM/auto.names" class="uri">http://azzalini.stat.unipd.it/Book-DM/auto.names</a></li>
</ul></li>
</ul>
</section>
</section>
<section id="misspecification-and-remedies" class="level1 page-columns page-full">
<h1>Misspecification and remedies</h1>
<section id="assumptions-and-misspecification" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="assumptions-and-misspecification">Assumptions and misspecification</h2>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Classical assumptions of linear models
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>(A.1)</strong> <span class="blue">Linear structure</span>, namely <span class="math inline">\boldsymbol{Y} = \boldsymbol{X}\beta + \boldsymbol{\epsilon}</span> with <span class="math inline">\mathbb{E}(\boldsymbol{\epsilon}) = 0</span>, implying <span class="math inline">\mathbb{E}(\boldsymbol{Y}) = \boldsymbol{X}\beta</span>. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p></li>
<li><p><strong>(A.2)</strong> <span class="orange">Homoschedasticity</span> and <span class="orange">uncorrelation</span> of the errors, namely <span class="math inline">\text{var}(\boldsymbol{\epsilon}) = \sigma^2 I_n</span>.</p></li>
<li><p><strong>(A.3)</strong> <span class="grey">Gaussianity</span>, namely <span class="math inline">\boldsymbol{\epsilon} \sim \text{N}_n(0, \sigma^2 I_n)</span>. In other words, the errors <span class="math inline">\epsilon_i \overset{\text{iid}}{\sim}N(0, \sigma^2)</span> are iid Gaussian random variables with zero mean and variance <span class="math inline">\sigma^2</span>.</p></li>
</ul>
<p>It is also commonly asked that <span class="math inline">\text{rk}(\boldsymbol{X}) = p</span>, otherwise the model is not identifiable.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;If the intercept is included in <span class="math inline">\boldsymbol{X}</span>, the errors automatically satisfy the property <span class="math inline">\mathbb{E}(\boldsymbol{\epsilon}) = 0</span>.</p></div></div><ul>
<li>If one of the above assumptions is violated, it is not necessarily a huge problem, because
<ul>
<li>the OLS estimator <span class="math inline">\hat{\beta}</span> is fairly <span class="orange">robust</span> to misspecification;</li>
<li>simple <span class="blue">fixes</span> (variable transformations, standard error corrections) are available.</li>
</ul></li>
</ul>
<!-- - Here we review the [implications]{.blue} of each [assumption]{.orange}, and a few  [solutions]{.blue}. -->
<!-- - Even if the data are [heroschedastic]{.orange} the OLS estimator is still a very reasonable choice, but we need to "correct" the standard errors to account for that.  -->
</section>
<section id="robust-estimation-and-assumptions" class="level2">
<h2 class="anchored" data-anchor-id="robust-estimation-and-assumptions">Robust estimation and assumptions</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/plane.jpg" class="nostretch img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<ul>
<li><p>A plane can still fly with one of its <span class="orange">engines on fire</span>, but this is hardly an appealing situation.</p></li>
<li><p>Similarly, robust estimators may work under <span class="orange">model misspecification</span>, but this does not mean we should neglect <span class="blue">checking</span> whether the original <span class="blue">assumptions</span> hold.</p></li>
</ul>
</section>
<section id="non-normality-of-the-errors-i" class="level2">
<h2 class="anchored" data-anchor-id="non-normality-of-the-errors-i">Non-normality of the errors I üìñ</h2>
<ul>
<li><p>Let us consider the case in which assumptions <strong>(A.1)</strong>-<strong>(A.2)</strong> are <span class="blue">valid</span> but <strong>(A.3)</strong> <span class="orange">is not</span>, that is <span class="math inline">\mathbb{E}(\boldsymbol{\epsilon}) = 0</span> and <span class="math inline">\text{var}(\boldsymbol{\epsilon}) = \sigma^2 I_n</span>, but <span class="math inline">\epsilon</span> does <span class="orange">not</span> follow <span class="orange">a Gaussian</span> distribution.</p></li>
<li><p>For example, <span class="math inline">\epsilon_i</span> may follow a Laplace distribution, a skew-Normal, a logistic distribution, a Student‚Äôs t distribution, etc.</p></li>
</ul>
<ul>
<li>The OLS estimate <span class="math inline">\hat{\beta}</span> is <span class="orange">not</span> anymore the <span class="orange">maximum likelihood</span> estimator, but it <span class="blue">preserves</span> most of its <span class="blue">properties</span> and a <span class="blue">geometric interpretation</span>.</li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Under <strong>(A.1)</strong>-<strong>(A.2)</strong>, even without requiring normality of the errors <strong>(A.3)</strong>, we obtain the usual formulas: <span class="math display">
\mathbb{E}(\hat{\beta}) = \beta, \qquad \text{var}(\hat{\beta}) = \sigma^2(\boldsymbol{X}^T\boldsymbol{X})^{-1}.
</span> Moreover, because of <span class="blue">Gauss-Markov</span> theorem, the OLS estimator <span class="math inline">\hat{\beta}</span> is the most <span class="orange">efficient</span> within the class of linear and unbiased estimators (<span class="blue">BLUE</span>) for any distribution of the errors <span class="math inline">\boldsymbol{\epsilon}</span>.</p>
</div>
</div>
</div>
<ul>
<li>In fact, note that the <span class="orange">proof</span> of the Gauss-Markov theorem requires <strong>(A.1)</strong>-<strong>(A.2)</strong> but <span class="orange">not</span> <strong>(A.3)</strong>.</li>
</ul>
</section>
<section id="non-normality-of-the-errors-ii" class="level2">
<h2 class="anchored" data-anchor-id="non-normality-of-the-errors-ii">Non-normality of the errors II</h2>
<ul>
<li><p>When the errors are non Gaussian the <span class="blue">exact inferential results</span> are not valid. In particular <span class="math inline">\hat{\beta}</span> does not follow anymore a Gaussian distribution.</p></li>
<li><p>However, a <span class="orange">central limit theorem</span> can be invoked under very mild conditions on the design matrix <span class="math inline">\boldsymbol{X}</span>.</p></li>
<li><p>Thus, when the sample size <span class="math inline">n</span> is large enough, then the following <span class="orange">approximation</span> holds <span class="math display">
\hat{\beta} \:\dot{\sim}\: \text{N}_p(\beta, \sigma^2(\boldsymbol{X}^T\boldsymbol{X})^{-1}),
</span> from which <span class="blue">confidence intervals</span> and <span class="blue">test statistics</span> can be obtained as usual. The approximation is <span class="orange">excellent</span> if the errors are <span class="blue">symmetric</span> around <span class="math inline">0</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><span class="blue">Non-normality</span> of the errors is <span class="orange">not a major concern</span>: the OLS estimator preserves most of its properties, including approximate normality for sufficiently large <span class="math inline">n</span>.</p>
<p>There is often an <span class="orange">over-emphasis</span> on testing whether the residuals are Gaussian. However, even if normality is rejected, the practical implications are minimal.</p>
</div>
</div>
</div>
</section>
<section id="heteroschedasticity-of-the-errors-i" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="heteroschedasticity-of-the-errors-i">Heteroschedasticity of the errors I üìñ</h2>
<ul>
<li><p>Suppose now that the linearity assumption <strong>(A.1)</strong> is valid but <span class="orange">homoschedasticity</span> of the errors <strong>(A.2)</strong> is <span class="orange">not</span>. Instead, we consider <span class="blue">heteroschedastic errors</span>: <span class="math display">
\text{var}(\boldsymbol{\epsilon}) = \boldsymbol{\Sigma},\quad \text{or equivalenty} \quad \text{var}(Y_i) = \sigma^2_i, \quad i=1,\dots,n
</span> where <span class="math inline">\boldsymbol{\Sigma} = \text{diag}(\sigma^2_1,\dots,\sigma_n^2)</span> is a diagonal matrix with positive entries.</p></li>
<li><p>The OLS estimator is still <span class="orange">unbiased</span>, with a <span class="blue">modified covariance</span> structure<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> <span class="math display">
\mathbb{E}(\hat{\beta}) = \beta, \qquad \text{var}(\hat{\beta}) = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{\Sigma}\boldsymbol{X} (\boldsymbol{X}^T\boldsymbol{X})^{-1}.
</span> If in addition we assume Gaussianity of the errors, that is <span class="math inline">\boldsymbol{\epsilon} \sim \text{N}_n(0,\boldsymbol{\Sigma})</span>, then <span class="math display">
\hat{\beta} \sim \text{N}_p(\beta, (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{\Sigma}\boldsymbol{X} (\boldsymbol{X}^T\boldsymbol{X})^{-1}).
</span> Under suitable but mild conditions on <span class="math inline">\boldsymbol{X}</span> and <span class="math inline">\boldsymbol{\Sigma}</span>, the estimator is also <span class="orange">consistent</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;These results are valid even when the matrix <span class="math inline">\boldsymbol{\Sigma}</span> is non-diagonal. This is useful to model correlated responses.</p></div></div></section>
<section id="heteroschedasticity-of-the-errors-ii" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="heteroschedasticity-of-the-errors-ii">Heteroschedasticity of the errors II</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The OLS estimator in presence of heteroschedasticity still gives a <span class="blue">good point estimate</span>. However, the OLS estimator is <span class="orange">not efficient</span> and the classical <span class="orange">standard errors</span> are <span class="orange">wrong</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>A potential approach is to <span class="orange">accept the inefficiency</span> of the OLS estimator in this scenario and <span class="blue">correct</span> the standard errors.</p></li>
<li><p>The elements of <span class="math inline">\boldsymbol{\Sigma}</span> are <span class="orange">unknown</span>, but we can estimate them from the data. Note that <span class="math display">
\text{var}(r_i) = \text{var}(y_i - \boldsymbol{x}_i^T\hat{\beta}) = \sigma^2_i(1 - h_i),
</span> suggesting the <span class="blue">estimate</span> <span class="math inline">\hat{\sigma}^2_i = r_i^2 / (1 - h_i)</span>.</p></li>
<li><p>This leads to the so-called <span class="orange">sandwich estimator</span> of the covariance matrix: <span class="math display">
\widehat{\text{var}}(\hat{\beta}) = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\hat{\boldsymbol{\Sigma}}\boldsymbol{X} (\boldsymbol{X}^T\boldsymbol{X})^{-1},
</span> where <span class="math inline">\hat{\boldsymbol{\Sigma}} = \text{diag}(\hat{w}_1,\dots,\hat{w}_n)</span> and <span class="math inline">\hat{w}_i = r_i^2 / (1 - h_i)</span>.</p></li>
<li><p>These are known as <span class="blue">White‚Äôs</span> heteroscedasticity-consistent <span class="blue">standard errors</span>. <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;White originally proposed the simpler version <span class="math inline">\hat{\sigma}^2_i = r_i^2</span>. Another variant is <span class="math inline">\hat{\sigma}^2_i = r_i^2 / (1 - h_i)^2</span>.</p></div></div></section>
<section id="weighted-least-squares-i" class="level2">
<h2 class="anchored" data-anchor-id="weighted-least-squares-i">Weighted least squares I üìñ</h2>
<ul>
<li><p>Let us consider again the case of <span class="blue">heteroschedastic errors</span>: <span class="math display">
\text{var}(\boldsymbol{\epsilon}) = \sigma^2\boldsymbol{\Omega}^{-1},\quad \text{or equivalenty} \quad \text{var}(Y_i) = \sigma^2_i = \frac{\sigma^2}{\omega_i}, \quad i=1,\dots,n
</span> where <span class="math inline">\boldsymbol{\Omega} = \text{diag}(\omega_1,\dots,\omega_n)</span> are positive <span class="orange">weights</span>. However, here we assume that the weights <span class="math inline">\omega_1,\dots,\omega_n</span> are <span class="blue">known</span>, a common situation in survey design.</p></li>
<li><p>Let us define the <span class="orange">standardized</span> quantities: <span class="math display">
\boldsymbol{Y}^* = \boldsymbol{\Omega}^{1/2}\boldsymbol{Y}, \qquad \boldsymbol{X}^* = \boldsymbol{\Omega}^{1/2}\boldsymbol{X}.
</span> This is equivalent to say that <span class="math inline">Y_i^* = \sqrt{\omega_i} Y_i</span> and <span class="math inline">x_{ij}^* = \sqrt{\omega_i} x_{ij}</span>. Then, it is easy to show that <span class="math display">
\mathbb{E}(\boldsymbol{Y}^*) = \boldsymbol{X}^*\beta, \qquad \text{var}(\boldsymbol{Y}^*) = \sigma^2\boldsymbol{\Omega}^{1/2}\boldsymbol{\Omega}^{-1}\boldsymbol{\Omega}^{1/2}= \sigma^2 I_n,
</span> namely the <span class="orange">assumptions</span> <strong>(A.1)</strong> and <strong>(A.2)</strong> are valid in the <span class="blue">transformed scale</span>.</p></li>
<li><p>In other words, <span class="orange">after</span> a suitable <span class="orange">transformation</span>, we reconducted the problem to a <span class="blue">standard linear model</span>.</p></li>
</ul>
</section>
<section id="weighted-least-squares-ii" class="level2">
<h2 class="anchored" data-anchor-id="weighted-least-squares-ii">Weighted least squares II üìñ</h2>
<ul>
<li><p>Thus an estimator for <span class="math inline">\beta</span>, based on the transformed data, is obtained minimizing the deviance <span class="math display">
\begin{aligned}
D_\text{wls}(\beta) &amp;= (\boldsymbol{y}^* - \boldsymbol{X}^*\beta)^T(\boldsymbol{y}^* - \boldsymbol{X}^*\beta) = (\boldsymbol{y} - \boldsymbol{X}\beta)^T\boldsymbol{\Omega}(\boldsymbol{y} - \boldsymbol{X}\beta) \\
&amp;=\sum_{i=1}^n \omega_i (y_i - \boldsymbol{x}_i^T\beta)^2.
\end{aligned}
</span> which is a <span class="orange">weighted</span> version of the original <span class="blue">quadratic loss</span>, with <span class="orange">high weight = low variance</span>.</p></li>
<li><p>The resulting OLS estimate minimizing <span class="math inline">D_\text{wls}(\beta)</span> in the transformed and original scales is <span class="math display">
\hat{\beta}_\text{wls} = [(\boldsymbol{X}^*)^T\boldsymbol{X}^*]^{-1}(\boldsymbol{X}^*)^T\boldsymbol{y}^* = (\boldsymbol{X}^T\boldsymbol{\Omega}\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{\Omega}\boldsymbol{y}
</span> and it is referred to as <span class="blue">weighted least squares</span> estimator of <span class="math inline">\beta</span>.</p></li>
<li><p>Such an estimator is <span class="orange">unbiased</span> and <span class="orange">efficient</span> (<span class="blue">BLUE</span>), with <span class="math display">
\mathbb{E}(\hat{\beta}_\text{wls}) = \beta, \qquad \text{var}(\hat{\beta}_\text{wls}) = \sigma^2 (\boldsymbol{X}^T\boldsymbol{\Omega}\boldsymbol{X})^{-1}.
</span> Moreover, if <span class="math inline">\boldsymbol{\epsilon} \sim \text{N}_n(0, \sigma^2\boldsymbol{\Omega}^{-1})</span> it also coincides with the <span class="orange">maximum likelihood</span> estimator.</p></li>
</ul>
</section>
<section id="variable-transformations" class="level2">
<h2 class="anchored" data-anchor-id="variable-transformations">Variable transformations</h2>
<ul>
<li>Another remedy for <span class="orange">misspecification</span> was already applied in the analysis of the car dataset, namely through <span class="blue">variable transformation</span>.</li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>While the model may have been incorrectly specified for the original data, it could become <span class="blue">appropriate</span> once the <span class="orange">transformations</span> are considered, namely <span class="math display">
g(Y_i) = h_1(\boldsymbol{x}_i)\beta_1 + \cdots + h_p(\boldsymbol{x}_i)\beta_p + \epsilon_i, \qquad i=1,\dots,n,
</span> where <span class="math inline">g(\cdot)</span> and <span class="math inline">h_j(\cdot)</span> for <span class="math inline">j=1,\dots,p</span> are <span class="orange">non-linear</span> and <span class="blue">known</span> functions.</p>
</div>
</div>
</div>
<ul>
<li><p>This idea is conceptually <span class="blue">simple</span> and <span class="blue">powerful</span>. It also shows that linear models are capable of capturing non-linear relationships, as long as they remain <span class="orange">linear in the parameters</span>.</p></li>
<li><p>However, choosing <span class="math inline">g(\cdot)</span> and <span class="math inline">h_j(\cdot)</span> in practice is <span class="orange">not simple</span>. In our case study, we proceeded by trial and error and used <span class="blue">contextual information</span> to guide our final choice.</p></li>
</ul>
<ul>
<li>Regarding the functions <span class="math inline">h_j(\cdot)</span>, <span class="blue">polynomial</span> terms are a simple and common option. More advanced approaches based on <span class="orange">splines</span> will be discussed in <a href="https://tommasorigon.github.io/datamining/">Data Mining</a>.</li>
</ul>
<!-- - Here we consider two general approaches for selecting the function $g(\cdot)$: variance stabilizing transformations and the Box-Cox transform.   -->
</section>
<section id="box-cox-transform" class="level2">
<h2 class="anchored" data-anchor-id="box-cox-transform">Box-Cox transform</h2>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Box-Cox transform
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the data are <span class="math inline">y_i</span> are <span class="orange">positive</span>, we may consider a <span class="blue">parametric class</span> of transformations: <span class="math display">
g_\lambda(y) = \frac{y^\lambda - 1}{\lambda}, \qquad \lambda \neq 0.
</span> and <span class="math inline">g_\lambda(y) = \log{y}</span> when <span class="math inline">\lambda = 0</span>. This is the celebrated <span class="blue">Box-Cox transform</span>.</p>
<p>The case <span class="math inline">\lambda = 1</span> corresponds to no transformation, <span class="math inline">\lambda= 1/2</span> to the square root, <span class="math inline">\lambda = 0</span> to the logarithm, and <span class="math inline">\lambda= ‚àí1</span> to the reciprocal.</p>
</div>
</div>
<ul>
<li><p>We estimate <span class="math inline">\lambda</span> from the data using <span class="orange">maximum likelihood</span>, so that the data themselves can inform us about the best transformation. We assume <span class="math display">
g_\lambda(Y_i) = \boldsymbol{x}_i^T\beta + \epsilon_i, \qquad \epsilon_i \sim \text{N}(0, \sigma^2), \qquad i=1,\dots,n.
</span></p></li>
<li><p>The aim of the transformation is to produce a response for which the <span class="blue">variance</span> of <span class="math inline">\epsilon_i</span> is <span class="blue">constant</span> with an <span class="orange">approximately normal</span> distribution.</p></li>
</ul>
</section>
<section id="box-cox-transform-derivation-i" class="level2">
<h2 class="anchored" data-anchor-id="box-cox-transform-derivation-i">Box-Cox transform: derivation I üìñ</h2>
<ul>
<li><p>By assumption, the distribution of the <span class="blue">transformed data</span> <span class="math inline">\boldsymbol{Z}_\lambda  = (g_\lambda(Y_1), \dots,g_\lambda(Y_n))^T</span> is Gaussian, therefore their joint density is <span class="math display">
f_Z(\boldsymbol{z}_\lambda) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left\{-\frac{1}{2\sigma^2}(\boldsymbol{z}_\lambda - \boldsymbol{X}\beta)^T(\boldsymbol{z}_\lambda - \boldsymbol{X}\beta)\right\}.
</span></p></li>
<li><p>Using standard tools of probability theory, we can obtain the density of the <span class="orange">original data</span>: <span class="math display">
f_Y(\boldsymbol{y})= f_Z(g_\lambda(y_1),\dots,g_\lambda(y_n))\prod_{i=1}^n\left|\frac{\partial g_\lambda(y_i)}{\partial y_i}\right|, \quad \text{where}\quad \left|\frac{\partial g_\lambda(y_i)}{\partial y_i}\right| = y_i^{\lambda - 1}.</span> The additional term is the determinant of the <span class="orange">Jacobian</span> of the transformation.</p></li>
<li><p>The <span class="blue">log-likelihood</span> therefore is <span class="math display">
\ell(\beta, \sigma^2, \lambda) = -\frac{n}{2}\log{\sigma^2} - \frac{1}{2\sigma^2}(\boldsymbol{z}_\lambda - \boldsymbol{X}\beta)^T(\boldsymbol{z}_\lambda - \boldsymbol{X}\beta) + (\lambda - 1)\sum_{i=1}^n\log{y_i}.
</span></p></li>
</ul>
</section>
<section id="box-cox-transform-derivation-ii" class="level2">
<h2 class="anchored" data-anchor-id="box-cox-transform-derivation-ii">Box-Cox transform: derivation II üìñ</h2>
<ul>
<li><p>Note that, for any given value of <span class="math inline">\lambda</span>, the maximum likelihood estimates are <span class="math display">
\hat{\beta}_\lambda = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{z}_\lambda, \qquad \hat{\sigma}^2_\lambda = \frac{1}{n}(\boldsymbol{z}_\lambda - \boldsymbol{X}\hat{\beta}_\lambda)^T(\boldsymbol{z}_\lambda - \boldsymbol{X}\hat{\beta}_\lambda),
</span></p></li>
<li><p>We can <span class="orange">plug-in</span> the above estimates into the log-likelihood. This gives the <span class="blue">profile log-likelihood</span> for <span class="math inline">\lambda</span>, which admits a very simple expression: <span class="math display">
\ell_P(\lambda) = \ell(\hat{\beta}_\lambda, \hat{\sigma}^2_\lambda, \lambda) = -\frac{n}{2}\log{\hat{\sigma}^2_\lambda} + (\lambda -1)\sum_{i=1}^n\log{y_i},
</span> which must be <span class="orange">numerically maximized</span> over <span class="math inline">\lambda</span>, e.g.&nbsp;using <code>optim</code>.</p></li>
<li><p>The optimal value <span class="math inline">\hat{\lambda} = \arg\max\ell_P(\lambda)</span>, as well as a confidence interval for it, may offer guidance in choosing the right transformation.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Box and Cox suggested using this approach as an <span class="blue">exploratory tool</span>. For instance, an optimal value <span class="math inline">\hat{\lambda} = 0.4210283</span> is <span class="orange">hard to interpret</span> but it could suggest a square root transformation.</p>
</div>
</div>
</div>
</section>
<section id="box-cox-transform-for-the-auto-dataset" class="level2">
<h2 class="anchored" data-anchor-id="box-cox-transform-for-the-auto-dataset">Box-Cox transform for the auto dataset</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="630"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>The <span class="blue">Box-Cox transform</span> in the auto dataset suggests a <span class="orange">reciprocal</span> transformation: <span class="math display">
\frac{1}{Y_i} = \beta_1 + \beta_2 x_i +  \beta_3 w_i + \beta_4 I(z_i = \texttt{gas}) + \beta_5 I(v_i = 2) + \epsilon_i,
</span> which is a good alternative to our model based on logarithms of <span class="math inline">y_i, x_i</span>, and <span class="math inline">w_i</span> (<span class="orange">but</span>‚Ä¶).</li>
</ul>
</section>
<section id="a-fourth-model-graphical-diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="a-fourth-model-graphical-diagnostics">A fourth model: graphical diagnostics</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="702"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="variance-stabilizing-transformations-i" class="level2">
<h2 class="anchored" data-anchor-id="variance-stabilizing-transformations-i">Variance stabilizing transformations I üìñ</h2>
<ul>
<li><p>Let <span class="math inline">Y_i \sim \text{Poisson}(\mu_i)</span> with mean <span class="math inline">\mathbb{E}(Y_i) = \mu_i = f(\boldsymbol{x}_i;\beta) = \text{var}(Y_i)</span>. Note that <span class="math display">
Y_i \,\dot{\sim}\, \text{N}(\mu_i, \mu_i),
</span> is <span class="blue">asymptotically Gaussian</span> for large values of <span class="math inline">\mu_i</span>. However, data are <span class="orange">heteroschedastic</span>.</p></li>
<li><p>In modeling count data, we could transform the counts so that, at least <span class="orange">approximately</span>, the <span class="blue">variance</span> of <span class="math inline">g(Y_i)</span> is <span class="blue">constant</span> and ordinary least squares methods can be used.</p></li>
</ul>
<ul>
<li>As an application of the <span class="blue">delta method</span>, the following linearization holds <span class="math display">
g(Y_i) - g(\mu_i) \approx (Y_i - \mu_i)g'(\mu_i), \quad \text{ which implies }\quad \text{var}\{g(Y_i)\} \approx g'(\mu_i)^2\text{var}(Y_i).
</span> In the Poisson case <span class="math inline">\text{var}\{g(Y_i)\} \approx \mu_i \,g'(\mu_i)^2</span> and we would like this to be <span class="orange">constant</span>.</li>
</ul>
<ul>
<li>The choice <span class="math inline">g(y) = \sqrt{y}</span>, called <span class="orange">variance stabilizing</span> transformation, gives <span class="math display">
\text{var}(\sqrt{Y_i}) \approx \left(\frac{1}{2\sqrt{\mu_i}}\right)^2\mu_i = \frac{1}{4}.
</span></li>
</ul>
</section>
<section id="variance-stabilizing-transformations-ii" class="level2">
<h2 class="anchored" data-anchor-id="variance-stabilizing-transformations-ii">Variance stabilizing transformations II üìñ</h2>
<!-- - The variance stabilizing transformation is [broadly applicable]{.blue} to several distributions.  -->
<ul>
<li><p>Let <span class="math inline">Y_i \sim \text{Binomial}(\pi_i, m_i)</span>, with <span class="blue">success probability</span> <span class="math inline">\pi_i = f(\boldsymbol{x}_i; \beta)</span> and <span class="blue">trials</span> <span class="math inline">m_i</span>. For large values of <span class="math inline">m_i</span>, the <span class="orange">Gaussian approximation</span> holds <span class="math display">
Y_i \,\dot{\sim}\, \text{N}(m_i \pi_i, m_i\pi_i(1 - \pi_i)).
</span> However, the data are <span class="orange">heteroschedastic</span>, because <span class="math inline">\text{var}(Y_i) = m_i \pi_i(1- \pi_i)</span>.</p></li>
<li><p>Thus, a <span class="blue">variance stabilizing</span> transformation in this case is <span class="math display">
g_{m_i}(y) = \sqrt{m_i}\arcsin\left(\frac{2 y}{m_i} - 1\right),
</span> because in fact we have that <span class="math display">
\text{var}(g_{m_i}(Y_i)) \approx \left(\frac{\sqrt{m_i}}{\sqrt{1 - (2\pi_i-1)^2}} \frac{2}{m_i}\right)^2 m_i \pi_i(1- \pi_i) = 1.
</span></p></li>
</ul>
<ul>
<li>If the data are <span class="blue">gamma distributed</span>, the <span class="orange">variance stabilizing</span> transform is <span class="math inline">g(y) = \log{y}</span>.</li>
</ul>
</section>
<section id="limitations-of-variable-transformations-i" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-variable-transformations-i">Limitations of variable transformations I</h2>
<ul>
<li>Variable transformations are appealing for their simplicity and have a long history in statistics. However, they also have some <span class="orange">drawbacks</span>.</li>
</ul>
<ul>
<li>In the case of transformations applied only to the <span class="blue">explanatory variables</span>, the model is <span class="math display">
Y_i = h_1(\boldsymbol{x}_i)\beta_1 + \cdots + h_p(\boldsymbol{x}_i)\beta_p + \epsilon_i, \qquad i=1,\dots,n,
</span> Thus, the coefficient <span class="math inline">\beta_j</span> can <span class="orange">no longer</span> be <span class="orange">interpreted</span> as the change in the mean of <span class="math inline">Y_i</span> corresponding to a <span class="blue">one-unit increase</span> <span class="math inline">x_{ij} \rightarrow x_{ij}+1</span> of the <span class="math inline">j</span>th covariate.</li>
</ul>
<ul>
<li><p>In the case of transformations of the <span class="orange">response</span> variable we let <span class="math inline">\mathbb{E}(g(Y_i)) = \boldsymbol{x}_i^T\beta</span>. However:<br>
<span class="math display">
g(\mathbb{E}(Y_i)) \neq E(g(Y_i)) \quad \Longrightarrow \quad \mathbb{E}(Y_i) \neq g^{-1}(\boldsymbol{x}_i^T\beta).
</span> Thus <span class="math inline">\hat{y}_i = g^{-1}(\boldsymbol{x}_i^T\hat{\beta})</span> is a <span class="blue">reasonable prediction</span> for <span class="math inline">Y_i</span> and <span class="orange">not an estimate</span> for its <span class="orange">mean</span>.</p></li>
<li><p>When <span class="math inline">g(y) = \log{y}</span> this distinction can be made explicit, because we have <span class="math display">
g^{-1}(\mathbb{E}\{g(Y_i)\}) = g^{-1}(\boldsymbol{x}_i^T\beta) = \exp(\boldsymbol{x}_i^T\beta), \qquad \mathbb{E}(Y_i) = \exp(\boldsymbol{x}_i^T\beta + \sigma^2/2),
</span> the former being the <span class="orange">geometric mean</span> of <span class="math inline">Y_i</span>, whereas the latter is the usual <span class="blue">mean</span>.</p></li>
</ul>
</section>
<section id="limitations-of-variable-transformations-ii" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-variable-transformations-ii">Limitations of variable transformations II</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Suppose <span class="math inline">Y_i \sim \text{Binomial}(\pi, m_i)</span>. The variance stabilizing transformation is not fully satisfactory:</p>
<ul>
<li>It <span class="orange">complicates</span> the <span class="orange">interpretation</span>, because it models <span class="math inline">\mathbb{E}\{g(Y_i)\}</span> instead of <span class="math inline">\mathbb{E}(Y_i)</span>;</li>
<li>It is an <span class="blue">asymptotic approximation</span>, and is only valid for <span class="math inline">m_i \rightarrow \infty</span>.</li>
<li>The transform depends on <span class="math inline">m_i</span>, therefore we cannot make predictions for a generic covariate value <span class="math inline">\boldsymbol{x}_i</span> without knowing the associated <span class="math inline">m_i</span>.</li>
</ul>
<p>Besides, this transform is clearly not applicable when <span class="math inline">m_i = 1</span> and <span class="math inline">Y_i \in \{0, 1\}</span>, a very common problem called <span class="orange">binary regression</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>If we know that <span class="math inline">Y_i</span> follows, say, a Bernoulli or a Gamma distribution, then we should use the <span class="blue">appropriate likelihood</span> rather than a <span class="orange">Gaussian approximation</span>.</p></li>
<li><p><span class="blue">Generalized Linear Models</span> provide a <span class="orange">much more elegant solution</span> to the above problem.</p></li>
</ul>
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Agresti2015" class="csl-entry" role="listitem">
Agresti, A. (2015), <em><span class="nocase">Foundations of Linear and Generalized Linear Models</span></em>, Wiley.
</div>
<div id="ref-Azzalini2008" class="csl-entry" role="listitem">
Azzalini, A. (2008), <em>Inferenza statistica</em>, Springer Verlag.
</div>
<div id="ref-Azzalini2012" class="csl-entry" role="listitem">
Azzalini, A., and Scarpa, B. (2012), <em>Data analysis and data mining: An introduction</em>, Oxford University Press.
</div>
<div id="ref-Salvan2020" class="csl-entry" role="listitem">
Salvan, A., Sartori, N., and Pace, L. (2020), <em>Modelli lineari generalizzati</em>, Springer.
</div>
</div>
</section>
</section>


</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<script src="un_A_files/libs/quarto-html/zenscroll-min.js"></script>
</body></html>