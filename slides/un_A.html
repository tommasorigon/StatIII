<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tommaso Rigon">

<title>Point estimation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="un_A_files/libs/clipboard/clipboard.min.js"></script>
<script src="un_A_files/libs/quarto-html/quarto.js"></script>
<script src="un_A_files/libs/quarto-html/popper.min.js"></script>
<script src="un_A_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="un_A_files/libs/quarto-html/anchor.min.js"></script>
<link href="un_A_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="un_A_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="un_A_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="un_A_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="un_A_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-full">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#homepage" id="toc-homepage" class="nav-link active" data-scroll-target="#homepage">Homepage</a></li>
  <li><a href="#methods-of-finding-estimators" id="toc-methods-of-finding-estimators" class="nav-link" data-scroll-target="#methods-of-finding-estimators">Methods of finding estimators</a>
  <ul class="collapse">
  <li><a href="#estimator" id="toc-estimator" class="nav-link" data-scroll-target="#estimator">Estimator</a></li>
  <li><a href="#method-of-moments" id="toc-method-of-moments" class="nav-link" data-scroll-target="#method-of-moments">Method of moments</a></li>
  <li><a href="#asymptotic-evaluation-of-the-mm" id="toc-asymptotic-evaluation-of-the-mm" class="nav-link" data-scroll-target="#asymptotic-evaluation-of-the-mm">Asymptotic evaluation of the MM</a></li>
  <li><a href="#example-beta-distribution" id="toc-example-beta-distribution" class="nav-link" data-scroll-target="#example-beta-distribution">Example: beta distribution 📖</a></li>
  <li><a href="#example-beta-distribution-food-expenditure" id="toc-example-beta-distribution-food-expenditure" class="nav-link" data-scroll-target="#example-beta-distribution-food-expenditure">Example: beta distribution (food expenditure)</a></li>
  <li><a href="#example-beta-distribution-food-expenditure-1" id="toc-example-beta-distribution-food-expenditure-1" class="nav-link" data-scroll-target="#example-beta-distribution-food-expenditure-1">Example: beta distribution (food expenditure)</a></li>
  <li><a href="#example-binomial-with-unknown-trials-mm" id="toc-example-binomial-with-unknown-trials-mm" class="nav-link" data-scroll-target="#example-binomial-with-unknown-trials-mm">Example: binomial with unknown trials MM 📖</a></li>
  <li><a href="#maximum-likelihood-estimator" id="toc-maximum-likelihood-estimator" class="nav-link" data-scroll-target="#maximum-likelihood-estimator">Maximum likelihood estimator</a></li>
  <li><a href="#properties-and-remarks-about-the-mle" id="toc-properties-and-remarks-about-the-mle" class="nav-link" data-scroll-target="#properties-and-remarks-about-the-mle">Properties and remarks about the MLE 📖</a></li>
  <li><a href="#example-poisson-with-unknown-mean" id="toc-example-poisson-with-unknown-mean" class="nav-link" data-scroll-target="#example-poisson-with-unknown-mean">Example: Poisson with unknown mean 📖</a></li>
  <li><a href="#example-binomial-with-unknown-trials-mle" id="toc-example-binomial-with-unknown-trials-mle" class="nav-link" data-scroll-target="#example-binomial-with-unknown-trials-mle">Example: binomial with unknown trials MLE</a></li>
  <li><a href="#example-binomial-with-unknown-trials-mle-1" id="toc-example-binomial-with-unknown-trials-mle-1" class="nav-link" data-scroll-target="#example-binomial-with-unknown-trials-mle-1">Example: binomial with unknown trials MLE</a></li>
  <li><a href="#m-estimators" id="toc-m-estimators" class="nav-link" data-scroll-target="#m-estimators">M-estimators</a></li>
  <li><a href="#z-estimators" id="toc-z-estimators" class="nav-link" data-scroll-target="#z-estimators">Z-estimators</a></li>
  <li><a href="#huber-estimators-i" id="toc-huber-estimators-i" class="nav-link" data-scroll-target="#huber-estimators-i">Huber estimators I</a></li>
  <li><a href="#huber-estimators-ii" id="toc-huber-estimators-ii" class="nav-link" data-scroll-target="#huber-estimators-ii">Huber estimators II</a></li>
  <li><a href="#example-newcombs-speed-of-light" id="toc-example-newcombs-speed-of-light" class="nav-link" data-scroll-target="#example-newcombs-speed-of-light">Example: Newcomb’s speed of light</a></li>
  <li><a href="#bayesian-estimators" id="toc-bayesian-estimators" class="nav-link" data-scroll-target="#bayesian-estimators">Bayesian estimators</a></li>
  <li><a href="#example-binomial-bayes-estimator" id="toc-example-binomial-bayes-estimator" class="nav-link" data-scroll-target="#example-binomial-bayes-estimator">Example: binomial Bayes estimator</a></li>
  </ul></li>
  <li><a href="#methods-of-evaluating-estimators" id="toc-methods-of-evaluating-estimators" class="nav-link" data-scroll-target="#methods-of-evaluating-estimators">Methods of evaluating estimators</a>
  <ul class="collapse">
  <li><a href="#comparing-estimators" id="toc-comparing-estimators" class="nav-link" data-scroll-target="#comparing-estimators">Comparing estimators</a></li>
  <li><a href="#optimal-estimators" id="toc-optimal-estimators" class="nav-link" data-scroll-target="#optimal-estimators">Optimal estimators</a></li>
  <li><a href="#admissible-estimators" id="toc-admissible-estimators" class="nav-link" data-scroll-target="#admissible-estimators">Admissible estimators</a></li>
  <li><a href="#the-choice-of-the-loss-function" id="toc-the-choice-of-the-loss-function" class="nav-link" data-scroll-target="#the-choice-of-the-loss-function">The choice of the loss function</a></li>
  <li><a href="#other-loss-functions" id="toc-other-loss-functions" class="nav-link" data-scroll-target="#other-loss-functions">Other loss functions</a></li>
  <li><a href="#example-mse-of-binomial-estimators-i" id="toc-example-mse-of-binomial-estimators-i" class="nav-link" data-scroll-target="#example-mse-of-binomial-estimators-i">Example: MSE of binomial estimators I 📖</a></li>
  <li><a href="#example-mse-of-binomial-estimators-ii" id="toc-example-mse-of-binomial-estimators-ii" class="nav-link" data-scroll-target="#example-mse-of-binomial-estimators-ii">Example: MSE of binomial estimators II</a></li>
  <li><a href="#example-mse-of-binomial-estimators-iii" id="toc-example-mse-of-binomial-estimators-iii" class="nav-link" data-scroll-target="#example-mse-of-binomial-estimators-iii">Example: MSE of binomial estimators III</a></li>
  <li><a href="#james-stein-estimator-i" id="toc-james-stein-estimator-i" class="nav-link" data-scroll-target="#james-stein-estimator-i">James-Stein estimator I</a></li>
  <li><a href="#james-stein-estimator-ii" id="toc-james-stein-estimator-ii" class="nav-link" data-scroll-target="#james-stein-estimator-ii">James-Stein estimator II</a></li>
  <li><a href="#james-stein-estimator-iii" id="toc-james-stein-estimator-iii" class="nav-link" data-scroll-target="#james-stein-estimator-iii">James-Stein estimator III 📖</a></li>
  <li><a href="#james-stein-estimator-iv" id="toc-james-stein-estimator-iv" class="nav-link" data-scroll-target="#james-stein-estimator-iv">James-Stein estimator IV</a></li>
  <li><a href="#james-stein-estimator-v" id="toc-james-stein-estimator-v" class="nav-link" data-scroll-target="#james-stein-estimator-v">James-Stein estimator V</a></li>
  <li><a href="#efron-and-morris-1975" id="toc-efron-and-morris-1975" class="nav-link" data-scroll-target="#efron-and-morris-1975">Efron and Morris (1975)</a></li>
  <li><a href="#efron-and-morris-1975-1" id="toc-efron-and-morris-1975-1" class="nav-link" data-scroll-target="#efron-and-morris-1975-1">Efron and Morris (1975)</a></li>
  <li><a href="#james-stein-estimator-vi" id="toc-james-stein-estimator-vi" class="nav-link" data-scroll-target="#james-stein-estimator-vi">James-Stein estimator VI</a></li>
  <li><a href="#criticism-to-the-risk-function-approach" id="toc-criticism-to-the-risk-function-approach" class="nav-link" data-scroll-target="#criticism-to-the-risk-function-approach">Criticism to the risk function approach</a></li>
  <li><a href="#integrated-risk" id="toc-integrated-risk" class="nav-link" data-scroll-target="#integrated-risk">Integrated risk</a></li>
  <li><a href="#example-integrated-risk-of-binomial-estimators" id="toc-example-integrated-risk-of-binomial-estimators" class="nav-link" data-scroll-target="#example-integrated-risk-of-binomial-estimators">Example: integrated risk of binomial estimators 📖</a></li>
  <li><a href="#example-integrated-risk-of-binomial-estimators-1" id="toc-example-integrated-risk-of-binomial-estimators-1" class="nav-link" data-scroll-target="#example-integrated-risk-of-binomial-estimators-1">Example: integrated risk of binomial estimators</a></li>
  <li><a href="#bayesian-estimators-minimize-the-integrated-risk" id="toc-bayesian-estimators-minimize-the-integrated-risk" class="nav-link" data-scroll-target="#bayesian-estimators-minimize-the-integrated-risk">Bayesian estimators minimize the integrated risk 📖</a></li>
  <li><a href="#decision-theoretic-justification-of-the-posterior-mean" id="toc-decision-theoretic-justification-of-the-posterior-mean" class="nav-link" data-scroll-target="#decision-theoretic-justification-of-the-posterior-mean">Decision-theoretic justification of the posterior mean 📖</a></li>
  <li><a href="#example-integrated-risk-of-binomial-estimators-2" id="toc-example-integrated-risk-of-binomial-estimators-2" class="nav-link" data-scroll-target="#example-integrated-risk-of-binomial-estimators-2">Example: integrated risk of binomial estimators 📖</a></li>
  <li><a href="#example-integrated-risk-of-binomial-estimators-3" id="toc-example-integrated-risk-of-binomial-estimators-3" class="nav-link" data-scroll-target="#example-integrated-risk-of-binomial-estimators-3">Example: integrated risk of binomial estimators</a></li>
  <li><a href="#admissibility-of-bayesian-estimators" id="toc-admissibility-of-bayesian-estimators" class="nav-link" data-scroll-target="#admissibility-of-bayesian-estimators">Admissibility of Bayesian estimators 📖</a></li>
  <li><a href="#minimax-estimators" id="toc-minimax-estimators" class="nav-link" data-scroll-target="#minimax-estimators">Minimax estimators</a></li>
  <li><a href="#example-mse-of-binomial-estimators-minimax" id="toc-example-mse-of-binomial-estimators-minimax" class="nav-link" data-scroll-target="#example-mse-of-binomial-estimators-minimax">Example: MSE of binomial estimators (minimax)</a></li>
  <li><a href="#minimax-and-bayesian-estimators" id="toc-minimax-and-bayesian-estimators" class="nav-link" data-scroll-target="#minimax-and-bayesian-estimators">Minimax and Bayesian estimators 📖</a></li>
  </ul></li>
  <li><a href="#unbiasedness" id="toc-unbiasedness" class="nav-link" data-scroll-target="#unbiasedness">Unbiasedness</a>
  <ul class="collapse">
  <li><a href="#unbiased-estimators" id="toc-unbiased-estimators" class="nav-link" data-scroll-target="#unbiased-estimators">Unbiased estimators</a></li>
  <li><a href="#nonexistence-of-unbiased-estimators" id="toc-nonexistence-of-unbiased-estimators" class="nav-link" data-scroll-target="#nonexistence-of-unbiased-estimators">Nonexistence of unbiased estimators</a></li>
  <li><a href="#bayesian-estimators-and-unbiasedness" id="toc-bayesian-estimators-and-unbiasedness" class="nav-link" data-scroll-target="#bayesian-estimators-and-unbiasedness">Bayesian estimators and unbiasedness 📖</a></li>
  <li><a href="#example-poisson-unbiased-estimation" id="toc-example-poisson-unbiased-estimation" class="nav-link" data-scroll-target="#example-poisson-unbiased-estimation">Example: Poisson unbiased estimation</a></li>
  <li><a href="#umvu-estimators" id="toc-umvu-estimators" class="nav-link" data-scroll-target="#umvu-estimators">UMVU estimators</a></li>
  <li><a href="#cramér-rao-inequality" id="toc-cramér-rao-inequality" class="nav-link" data-scroll-target="#cramér-rao-inequality">Cramér-Rao inequality 📖</a></li>
  <li><a href="#cramér-rao-considerations" id="toc-cramér-rao-considerations" class="nav-link" data-scroll-target="#cramér-rao-considerations">Cramér-Rao: considerations</a></li>
  <li><a href="#bartlett-identities-i" id="toc-bartlett-identities-i" class="nav-link" data-scroll-target="#bartlett-identities-i">Bartlett identities I 📖</a></li>
  <li><a href="#bartlett-identities-ii" id="toc-bartlett-identities-ii" class="nav-link" data-scroll-target="#bartlett-identities-ii">Bartlett identities II 📖</a></li>
  <li><a href="#cramér-rao-iid-and-regular-case" id="toc-cramér-rao-iid-and-regular-case" class="nav-link" data-scroll-target="#cramér-rao-iid-and-regular-case">Cramér-Rao: iid and regular case</a></li>
  <li><a href="#example-poisson-unbiased-estimation-1" id="toc-example-poisson-unbiased-estimation-1" class="nav-link" data-scroll-target="#example-poisson-unbiased-estimation-1">Example: Poisson unbiased estimation 📖</a></li>
  <li><a href="#cramér-rao-multiparameter-case" id="toc-cramér-rao-multiparameter-case" class="nav-link" data-scroll-target="#cramér-rao-multiparameter-case">Cramér-Rao, multiparameter case</a></li>
  <li><a href="#rao-blackwell" id="toc-rao-blackwell" class="nav-link" data-scroll-target="#rao-blackwell">Rao-Blackwell 📖</a></li>
  <li><a href="#en-route-to-finding-unique-umvue-i" id="toc-en-route-to-finding-unique-umvue-i" class="nav-link" data-scroll-target="#en-route-to-finding-unique-umvue-i"><em>En route</em> to finding unique UMVUE I 📖</a></li>
  <li><a href="#en-route-to-finding-unique-umvue-ii" id="toc-en-route-to-finding-unique-umvue-ii" class="nav-link" data-scroll-target="#en-route-to-finding-unique-umvue-ii"><em>En route</em> to finding unique UMVUE II 📖</a></li>
  <li><a href="#completeness" id="toc-completeness" class="nav-link" data-scroll-target="#completeness">Completeness</a></li>
  <li><a href="#lehmann-scheffé-theorem" id="toc-lehmann-scheffé-theorem" class="nav-link" data-scroll-target="#lehmann-scheffé-theorem">Lehmann-Scheffé theorem</a></li>
  <li><a href="#example-binomial-best-unbiased-estimation" id="toc-example-binomial-best-unbiased-estimation" class="nav-link" data-scroll-target="#example-binomial-best-unbiased-estimation">Example: binomial best unbiased estimation 📖</a></li>
  <li><a href="#rao-blackwell-multiparameter-case" id="toc-rao-blackwell-multiparameter-case" class="nav-link" data-scroll-target="#rao-blackwell-multiparameter-case">Rao-Blackwell, multiparameter case</a></li>
  </ul></li>
  <li><a href="#alternative-notions-of-optimality" id="toc-alternative-notions-of-optimality" class="nav-link" data-scroll-target="#alternative-notions-of-optimality">Alternative notions of optimality</a>
  <ul class="collapse">
  <li><a href="#unbiased-estimating-equations-i" id="toc-unbiased-estimating-equations-i" class="nav-link" data-scroll-target="#unbiased-estimating-equations-i">Unbiased estimating equations I</a></li>
  <li><a href="#unbiased-estimating-equations-ii" id="toc-unbiased-estimating-equations-ii" class="nav-link" data-scroll-target="#unbiased-estimating-equations-ii">Unbiased estimating equations II</a></li>
  <li><a href="#asymptotic-behavior-of-unbiased-estimating-equations" id="toc-asymptotic-behavior-of-unbiased-estimating-equations" class="nav-link" data-scroll-target="#asymptotic-behavior-of-unbiased-estimating-equations">Asymptotic behavior of unbiased estimating equations</a></li>
  <li><a href="#godambe-information" id="toc-godambe-information" class="nav-link" data-scroll-target="#godambe-information">Godambe information</a></li>
  <li><a href="#godambe-efficiency" id="toc-godambe-efficiency" class="nav-link" data-scroll-target="#godambe-efficiency">Godambe efficiency</a></li>
  <li><a href="#godambe-information-of-the-score-function" id="toc-godambe-information-of-the-score-function" class="nav-link" data-scroll-target="#godambe-information-of-the-score-function">Godambe information of the score function</a></li>
  <li><a href="#godambe-efficiency-1" id="toc-godambe-efficiency-1" class="nav-link" data-scroll-target="#godambe-efficiency-1">Godambe efficiency</a></li>
  <li><a href="#linear-estimating-equations-i" id="toc-linear-estimating-equations-i" class="nav-link" data-scroll-target="#linear-estimating-equations-i">Linear estimating equations I</a></li>
  <li><a href="#linear-estimating-equations-ii" id="toc-linear-estimating-equations-ii" class="nav-link" data-scroll-target="#linear-estimating-equations-ii">Linear estimating equations II</a></li>
  <li><a href="#blue-estimators-i" id="toc-blue-estimators-i" class="nav-link" data-scroll-target="#blue-estimators-i">BLUE estimators I</a></li>
  <li><a href="#blue-estimators-ii" id="toc-blue-estimators-ii" class="nav-link" data-scroll-target="#blue-estimators-ii">BLUE estimators II</a></li>
  <li><a href="#blue-estimators-and-unbiased-estimating-equations-i" id="toc-blue-estimators-and-unbiased-estimating-equations-i" class="nav-link" data-scroll-target="#blue-estimators-and-unbiased-estimating-equations-i">BLUE estimators and unbiased estimating equations I</a></li>
  <li><a href="#blue-estimators-and-unbiased-estimating-equations-ii" id="toc-blue-estimators-and-unbiased-estimating-equations-ii" class="nav-link" data-scroll-target="#blue-estimators-and-unbiased-estimating-equations-ii">BLUE estimators and unbiased estimating equations II</a></li>
  </ul></li>
  <li><a href="#asymptotic-evaluations" id="toc-asymptotic-evaluations" class="nav-link" data-scroll-target="#asymptotic-evaluations">Asymptotic evaluations</a>
  <ul class="collapse">
  <li><a href="#asymptotic-evaluations-preliminaries" id="toc-asymptotic-evaluations-preliminaries" class="nav-link" data-scroll-target="#asymptotic-evaluations-preliminaries">Asymptotic evaluations: preliminaries</a></li>
  <li><a href="#asymptotic-evaluations-preliminaries-1" id="toc-asymptotic-evaluations-preliminaries-1" class="nav-link" data-scroll-target="#asymptotic-evaluations-preliminaries-1">Asymptotic evaluations: preliminaries</a></li>
  <li><a href="#example-poisson-with-unknown-mean-1" id="toc-example-poisson-with-unknown-mean-1" class="nav-link" data-scroll-target="#example-poisson-with-unknown-mean-1">Example: Poisson with unknown mean</a></li>
  <li><a href="#the-classical-regularity-conditions" id="toc-the-classical-regularity-conditions" class="nav-link" data-scroll-target="#the-classical-regularity-conditions">The classical “regularity conditions”</a></li>
  <li><a href="#wald-inequality" id="toc-wald-inequality" class="nav-link" data-scroll-target="#wald-inequality">Wald inequality 📖</a></li>
  <li><a href="#consistency-for-the-mle" id="toc-consistency-for-the-mle" class="nav-link" data-scroll-target="#consistency-for-the-mle">Consistency for the MLE</a></li>
  <li><a href="#what-could-go-wrong" id="toc-what-could-go-wrong" class="nav-link" data-scroll-target="#what-could-go-wrong">What could go wrong?</a></li>
  <li><a href="#what-could-go-wrong-1" id="toc-what-could-go-wrong-1" class="nav-link" data-scroll-target="#what-could-go-wrong-1">What could go wrong?</a></li>
  <li><a href="#what-else-could-go-wrong" id="toc-what-else-could-go-wrong" class="nav-link" data-scroll-target="#what-else-could-go-wrong">What else could go wrong?</a></li>
  <li><a href="#consistency-for-the-mle-1" id="toc-consistency-for-the-mle-1" class="nav-link" data-scroll-target="#consistency-for-the-mle-1">Consistency for the MLE 📖</a></li>
  <li><a href="#asymptotic-normality-of-the-mle" id="toc-asymptotic-normality-of-the-mle" class="nav-link" data-scroll-target="#asymptotic-normality-of-the-mle">Asymptotic normality of the MLE 📖</a></li>
  <li><a href="#observed-vs-fisher-information" id="toc-observed-vs-fisher-information" class="nav-link" data-scroll-target="#observed-vs-fisher-information">Observed vs Fisher information</a></li>
  <li><a href="#consistency-for-m-estimators-i" id="toc-consistency-for-m-estimators-i" class="nav-link" data-scroll-target="#consistency-for-m-estimators-i">Consistency for M-estimators I</a></li>
  <li><a href="#consistency-for-m-estimators-ii" id="toc-consistency-for-m-estimators-ii" class="nav-link" data-scroll-target="#consistency-for-m-estimators-ii">Consistency for M-estimators II 📖</a></li>
  <li><a href="#consistency-for-z-estimators" id="toc-consistency-for-z-estimators" class="nav-link" data-scroll-target="#consistency-for-z-estimators">Consistency for Z-estimators 📖</a></li>
  <li><a href="#asymptotic-normality-of-m-estimators-i" id="toc-asymptotic-normality-of-m-estimators-i" class="nav-link" data-scroll-target="#asymptotic-normality-of-m-estimators-i">Asymptotic normality of M-estimators I</a></li>
  <li><a href="#asymptotic-normality-of-m-estimators-ii" id="toc-asymptotic-normality-of-m-estimators-ii" class="nav-link" data-scroll-target="#asymptotic-normality-of-m-estimators-ii">Asymptotic normality of M-estimators II</a></li>
  <li><a href="#first-order-bias-correction" id="toc-first-order-bias-correction" class="nav-link" data-scroll-target="#first-order-bias-correction">First-order bias-correction</a></li>
  <li><a href="#bias-reduction-using-firths-correction-i" id="toc-bias-reduction-using-firths-correction-i" class="nav-link" data-scroll-target="#bias-reduction-using-firths-correction-i">Bias reduction using Firth’s correction I</a></li>
  <li><a href="#bias-reduction-using-firths-correction-ii" id="toc-bias-reduction-using-firths-correction-ii" class="nav-link" data-scroll-target="#bias-reduction-using-firths-correction-ii">Bias reduction using Firth’s correction II</a></li>
  <li><a href="#bias-reduction-using-firths-correction-iii" id="toc-bias-reduction-using-firths-correction-iii" class="nav-link" data-scroll-target="#bias-reduction-using-firths-correction-iii">Bias reduction using Firth’s correction III 📖</a></li>
  </ul></li>
  <li><a href="#robustness" id="toc-robustness" class="nav-link" data-scroll-target="#robustness">Robustness</a>
  <ul class="collapse">
  <li><a href="#robustness-preliminaries" id="toc-robustness-preliminaries" class="nav-link" data-scroll-target="#robustness-preliminaries">Robustness: preliminaries</a></li>
  <li><a href="#example-huber-estimators-i" id="toc-example-huber-estimators-i" class="nav-link" data-scroll-target="#example-huber-estimators-i">Example: Huber estimators I 📖</a></li>
  <li><a href="#example-huber-estimators-ii" id="toc-example-huber-estimators-ii" class="nav-link" data-scroll-target="#example-huber-estimators-ii">Example: Huber estimators II 📖</a></li>
  <li><a href="#example-omitted-variable-in-linear-regression" id="toc-example-omitted-variable-in-linear-regression" class="nav-link" data-scroll-target="#example-omitted-variable-in-linear-regression">Example: omitted variable in linear regression 📖</a></li>
  <li><a href="#maximum-likelihood-under-a-misspecified-model" id="toc-maximum-likelihood-under-a-misspecified-model" class="nav-link" data-scroll-target="#maximum-likelihood-under-a-misspecified-model">Maximum likelihood under a misspecified model 📖</a></li>
  <li><a href="#example-misspecified-exponential-model-i" id="toc-example-misspecified-exponential-model-i" class="nav-link" data-scroll-target="#example-misspecified-exponential-model-i">Example: misspecified exponential model I</a></li>
  <li><a href="#robustness-and-unbiased-estimating-equations" id="toc-robustness-and-unbiased-estimating-equations" class="nav-link" data-scroll-target="#robustness-and-unbiased-estimating-equations">Robustness and unbiased estimating equations</a></li>
  <li><a href="#misspecified-likelihoods-are-m-estimators" id="toc-misspecified-likelihoods-are-m-estimators" class="nav-link" data-scroll-target="#misspecified-likelihoods-are-m-estimators">Misspecified likelihoods are M-estimators</a></li>
  <li><a href="#sandwich-estimators" id="toc-sandwich-estimators" class="nav-link" data-scroll-target="#sandwich-estimators">Sandwich estimators</a></li>
  <li><a href="#example-misspecified-exponential-model-ii" id="toc-example-misspecified-exponential-model-ii" class="nav-link" data-scroll-target="#example-misspecified-exponential-model-ii">Example: misspecified exponential model II 📖</a></li>
  <li><a href="#example-correlated-observations" id="toc-example-correlated-observations" class="nav-link" data-scroll-target="#example-correlated-observations">Example: correlated observations</a></li>
  <li><a href="#example-the-probability-of-observing-a-zero" id="toc-example-the-probability-of-observing-a-zero" class="nav-link" data-scroll-target="#example-the-probability-of-observing-a-zero">Example: the probability of observing a zero 📖</a></li>
  <li><a href="#example-linear-models-with-misspecified-variance-i" id="toc-example-linear-models-with-misspecified-variance-i" class="nav-link" data-scroll-target="#example-linear-models-with-misspecified-variance-i">Example: linear models with misspecified variance I 📖</a></li>
  <li><a href="#example-linear-models-with-misspecified-variance-ii" id="toc-example-linear-models-with-misspecified-variance-ii" class="nav-link" data-scroll-target="#example-linear-models-with-misspecified-variance-ii">Example: linear models with misspecified variance II 📖</a></li>
  <li><a href="#example-linear-models-with-misspecified-variance-iii" id="toc-example-linear-models-with-misspecified-variance-iii" class="nav-link" data-scroll-target="#example-linear-models-with-misspecified-variance-iii">Example: linear models with misspecified variance III</a></li>
  <li><a href="#example-linear-models-with-misspecified-variance-iv" id="toc-example-linear-models-with-misspecified-variance-iv" class="nav-link" data-scroll-target="#example-linear-models-with-misspecified-variance-iv">Example: linear models with misspecified variance IV</a></li>
  </ul></li>
  <li><a href="#references-and-study-material" id="toc-references-and-study-material" class="nav-link" data-scroll-target="#references-and-study-material">References and study material</a>
  <ul class="collapse">
  <li><a href="#main-references" id="toc-main-references" class="nav-link" data-scroll-target="#main-references">Main references</a></li>
  <li><a href="#additional-references" id="toc-additional-references" class="nav-link" data-scroll-target="#additional-references">Additional references</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="un_A_slides.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></nav>
</div>
<main class="content page-columns page-full column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Point estimation</h1>
<p class="subtitle lead">Statistical Inference - PhD EcoStatData</p>
</div>


<div class="quarto-title-meta-author column-page-left">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><span class="orange">Tommaso Rigon</span> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <em>Università degli Studi di Milano-Bicocca</em>
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-page-left">

      
  
    
  </div>
  


</header>


<section id="homepage" class="level2">
<h2 class="anchored" data-anchor-id="homepage"><a href="../index.html">Homepage</a></h2>
<div class="columns">
<div class="column" style="width:20%;">
<p><img src="img/target.png" class="img-fluid"></p>
<!-- *"Pluralitas non est ponenda sine necessitate."* -->
<!-- [William of Ockham]{.grey} -->
</div><div class="column" style="width:80%;">
<ul>
<li><p>This unit will cover the following <span class="orange">topics</span>:</p>
<ul>
<li>Methods of finding estimators</li>
<li>Methods of evaluating estimators</li>
<li>Unbiasedness</li>
<li>Asymptotic evaluations</li>
<li>Robustness and model misspecification</li>
</ul></li>
</ul>
</div><ul>
<li><p>The rationale behind <span class="blue">point estimation</span> is quite simple:</p></li>
<li><p>When sampling is from a <span class="orange">population</span> described by a pdf or a pmf <span class="math inline">f(\cdot ; \theta)</span>, knowledge of <span class="math inline">\theta</span> yields knowledge of the entire population.</p></li>
<li><p>Hence, it is natural to seek a method of finding a good <span class="blue">estimator</span> of the unknown point <span class="math inline">\theta</span>.</p></li>
</ul>
</div>
</section>
<section id="methods-of-finding-estimators" class="level1 page-columns page-full">
<h1>Methods of finding estimators</h1>
<section id="estimator" class="level2">
<h2 class="anchored" data-anchor-id="estimator">Estimator</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>A <span class="blue">point estimator</span> <span class="math inline">\hat{\theta}</span> is any function of the random sample <span class="math inline">Y_1,\dots,Y_n</span>, namely <span class="math display">
\hat{\theta}(\bm{Y}) = \hat{\theta}(Y_1,\dots,Y_n).
</span> That is, any <span class="orange">statistic</span> is a point estimator.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li><p>To streamline the presentation, we consider estimators that target the <span class="orange">unknown parameter</span> <span class="math inline">\theta</span> rather than an arbitrary (non-one-to-one) <span class="blue">transformation</span> <span class="math inline">g(\theta)</span>.</p></li>
<li><p>Most theoretical results <span class="orange">extend naturally</span> to the general case <span class="math inline">g(\theta)</span>.</p></li>
</ul>
<!-- - In principle, the range of the estimator coincides with that of the parameter, i.e. $\hat{\theta} : \mathcal{Y} \rightarrow \Theta$, but there are exceptions. -->
</div>
</div>
</div>
<ul>
<li><p>An <span class="blue">estimator</span> <span class="math inline">\hat{\theta}(Y_1,\dots,Y_n)</span> is a function of the sample <span class="math inline">Y_1,\dots,Y_n</span> and is a <span class="blue">random variable</span>.</p></li>
<li><p>An <span class="orange">estimate</span> <span class="math inline">\hat{\theta}(y_1,\dots,y_n)</span> is a function of the realized values <span class="math inline">y_1,\dots,y_n</span> and is a <span class="orange">number</span>.</p></li>
<li><p>We will write <span class="math inline">\hat{\theta}</span> to denote both estimators and estimates whenever its meaning is clear from the context.</p></li>
</ul>
</section>
<section id="method-of-moments" class="level2">
<h2 class="anchored" data-anchor-id="method-of-moments">Method of moments</h2>
<ul>
<li><p>The <span class="blue">method of moments</span> is, perhaps, the oldest method of finding point estimators, dating back at least to Karl Pearson in the late 1800s.</p></li>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be an iid sample from <span class="math inline">f(\cdot; \theta)</span>, <span class="math inline">\theta = (\theta_1,\dots,\theta_p)</span>, and <span class="math inline">\Theta \subseteq \mathbb{R}^p</span>. Moreover, define <span class="math display">
  m_r = \frac{1}{n}\sum_{i=1}^n Y_i^r, \qquad \mu_r(\theta) = \mu_r(\theta_1,\dots,\theta_p) = \mathbb{E}_\theta(Y^r), \qquad r=1,\dots,p.
  </span> corresponding to the <span class="blue">population moment</span> <span class="math inline">\mu_r(\theta_1,\dots,\theta_p)</span> and the <span class="orange">sample moment</span> <span class="math inline">m_r</span>.</p></li>
<li><p>The method of moments estimator <span class="math inline">\hat{\theta}</span> is obtained by solving the following system of equations for <span class="math inline">(\theta_1,\dots,\theta_p)</span>: <span class="math display">
  \begin{aligned}
  \mu_1(\theta_1,\dots,\theta_p) &amp;= m_1, \\
  \mu_2(\theta_1,\dots,\theta_p) &amp;= m_2, \\
  &amp;\vdots \\
  \mu_p(\theta_1,\dots,\theta_p) &amp;= m_p. \\
  \end{aligned}
  </span></p></li>
<li><p>In general, it is <span class="orange">not guaranteed</span> that a <span class="blue">solution exists</span> nor its <span class="blue">uniqueness</span>.</p></li>
</ul>
</section>
<section id="asymptotic-evaluation-of-the-mm" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="asymptotic-evaluation-of-the-mm">Asymptotic evaluation of the MM</h2>
<ul>
<li><p>Moments estimators are not necessarily the best estimators, but <span class="blue">under reasonable conditions</span> they are <span class="orange">consistent</span>, they have converge rate <span class="math inline">\sqrt{n}</span>, and they are <span class="orange">asymptotically normal</span>.</p></li>
<li><p>Suppose <span class="math inline">(Y,Y^2,\dots,Y^p)</span> has <span class="blue">covariance</span> <span class="math inline">\Sigma</span>, then the multivariate <span class="orange">central limit theorem</span> implies that as <span class="math inline">n\rightarrow \infty</span> <span class="math display">
\sqrt{n}\{m - \mu(\theta)\} \overset{\text{d}}{\longrightarrow} Z, \qquad Z\sim N_p(0, \Sigma),
</span> where <span class="math inline">m = (m_1,\dots,m_p)</span> and <span class="math inline">\mu(\theta) = (\mu_1(\theta),\dots,\mu_p(\theta))</span>.</p></li>
<li><p>Suppose also that <span class="math inline">\mu(\theta)</span> is a <span class="blue">one-to-one</span> mapping and let <span class="math inline">g(\mu)</span> be the inverse of <span class="math inline">\mu(\theta)</span>, that is <span class="math inline">g = \mu^{-1}</span>. We assume that <span class="math inline">g</span> has <span class="blue">differentiable</span> components <span class="math inline">g_r(\cdot)</span> for <span class="math inline">r = 1,\dots,p</span>.</p></li>
<li><p>The moments estimator can be written as <span class="math inline">\hat{\theta} = g(m)</span> and <span class="math inline">\theta = g(\mu(\theta))</span>. Then, as a consequence of the <span class="orange">delta method</span>, the following general result holds: <span class="math display">
\sqrt{n}(\hat{\theta} - \theta) \overset{\text{d}}{\longrightarrow} Z, \qquad Z\sim N_p\left(0, D \Sigma D^T\right),
</span> where <span class="math inline">D = [d_{rr'}]</span> is a <span class="math inline">p \times p</span> matrix whose entries are the derivatives <span class="math inline">d_{rr'} = \partial g_r(\mu)/\partial \mu_{r'}</span>.</p></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="margin-aside">
<p>Refer to <span class="citation" data-cites="vandervaart1998">van der Vaart (<a href="#ref-vandervaart1998" role="doc-biblioref">1998</a>)</span>, Theorem 4.1, pag. 35-36.</p>
</div></div></section>
<section id="example-beta-distribution" class="level2">
<h2 class="anchored" data-anchor-id="example-beta-distribution">Example: beta distribution 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be an iid random sample from a beta distribution of parameters <span class="math inline">\alpha,\beta &gt; 0</span> with density <span class="math display">
f(y; \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} y^{\alpha-1}(1 - y)^{\beta-1}, \qquad 0 &lt; y &lt; 1.
</span></p></li>
<li><p>The <span class="blue">moment estimator</span> for <span class="math inline">(\alpha, \beta)</span> is the (<span class="orange">explicitly available</span>) solution of the system of equations <span class="math display">
m_1 = \frac{\alpha}{\alpha + \beta}, \qquad m_2 = \frac{\alpha (\alpha+1)}{(\alpha + \beta) (\alpha + \beta + 1)}.
</span></p></li>
<li><p>After some algebra we obtain the following relationship, which is a <span class="orange">smooth</span> and <span class="orange">regular</span> function of <span class="math inline">(m_1,m_2)</span>: <span class="math display">
\hat{\alpha} = m_1 \frac{m_1 - m_2}{m_2 - m_1^2}, \qquad \hat{\beta} = (1 - m_1) \frac{m_1 - m_2}{m_2 - m_1^2}.
</span> where <span class="math inline">\hat{\sigma}^2 = m_2 - m_1^2</span> is the <span class="blue">sample variance</span>. <span class="orange">Remark</span>: is it possible that <span class="math inline">m_1 &lt; m_2</span>?</p></li>
</ul>
</section>
<section id="example-beta-distribution-food-expenditure" class="level2">
<h2 class="anchored" data-anchor-id="example-beta-distribution-food-expenditure">Example: beta distribution (food expenditure)</h2>
<ul>
<li>We consider data on <span class="orange">proportion of income</span> spent on <span class="blue">food</span> for a random sample of <span class="math inline">38</span> households in a large US city.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><span class="blue">Low</span> income level (<span class="math inline">n</span> = 17)</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0.07431 0.13548 0.08825 0.13728 0.09629 0.09160 0.13882 0.09670 0.10866
[10] 0.11629 0.18067 0.14539 0.15869 0.14910 0.09550 0.23066 0.14751</code></pre>
</div>
</div>
<p>Here <span class="math inline">m_1</span> = 0.129 and <span class="math inline">m_2</span> = 0.018, giving the <span class="orange">MM estimates</span>: <span class="math inline">\hat{\alpha}</span> = 9.7 and <span class="math inline">\hat{\beta}</span> = 65.7.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-warning no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><span class="orange">High</span> income level (<span class="math inline">n</span> = 21)</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0.15998 0.16652 0.21741 0.10481 0.23256 0.17976 0.14161 0.14184 0.19604
[10] 0.21141 0.17446 0.14005 0.18831 0.07641 0.21604 0.28980 0.10882 0.18561
[19] 0.19192 0.25918 0.28833</code></pre>
</div>
</div>
<p>Here <span class="math inline">m_1</span> = 0.184 and <span class="math inline">m_2</span> = 0.037, giving the <span class="orange">MM estimates</span>: <span class="math inline">\hat{\alpha}</span> = 9 and <span class="math inline">\hat{\beta}</span> = 39.9.</p>
</div>
</div>
</div>
</section>
<section id="example-beta-distribution-food-expenditure-1" class="level2">
<h2 class="anchored" data-anchor-id="example-beta-distribution-food-expenditure-1">Example: beta distribution (food expenditure)</h2>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="1350"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><span class="orange">Estimated densities</span> <span class="math inline">f(y; \hat{\alpha}, \hat{\beta})</span> for each income level, showing a reasonable fit in both cases.</li>
</ul>
</section>
<section id="example-binomial-with-unknown-trials-mm" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="example-binomial-with-unknown-trials-mm">Example: binomial with unknown trials MM 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be iid <span class="math inline">\textup{Bin}(N, p)</span> and we assume that <span class="orange">both</span> <span class="math inline">N</span> and <span class="math inline">p</span> are unknown.</p></li>
<li><p>This is a somewhat unusual application of the binomial model, which can be used e.g.&nbsp;to estimate crime rates for crimes that are known to have many unreported occurrences.</p></li>
<li><p>Equating the first two moments yields the system of equations <span class="math display">
m_1 = N p, \qquad m_2 = N p(1-p) + N^2p^2.
</span></p></li>
<li><p>After some algebra we obtain the <span class="blue">moment estimator</span> for <span class="math inline">(N, p)</span>, which is <span class="orange">smooth</span> and <span class="orange">regular</span> function of <span class="math inline">(m_1,m_2)</span>: <span class="math display">
\hat{p} = \frac{m_1}{\hat{N}}, \qquad \hat{N} = \frac{m_1^2}{m_1 - \hat{\sigma}^2},
</span> where <span class="math inline">\hat{\sigma}^2 = m_2 - m_1^2</span> is the sample variance.</p></li>
<li><p><span class="orange">Remark</span>: what if <span class="math inline">m_1 &lt; \hat{\sigma}^2</span>?</p></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="margin-aside">
<p>This problem is described in Example 7.2.2 <span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>, pag. 313.</p>
</div></div></section>
<section id="maximum-likelihood-estimator" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-estimator">Maximum likelihood estimator</h2>
<ul>
<li><p>The method of <span class="orange">maximum likelihood</span> is, by far, the most popular technique for deriving estimators, developed by <span class="blue">Ronald A. Fisher</span> in Fisher (1922; 1925).</p></li>
<li><p>Recall that <span class="math inline">L(\theta) = L(\theta; \bm{y})</span> is the likelihood function and <span class="math inline">\ell(\theta) = \log{L(\theta)}</span> is the log-likelihood.</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Given a likelihood function <span class="math inline">L(\theta)</span> of <span class="math inline">\theta \in \Theta</span>, a <span class="orange">maximum likelihood estimate</span> of <span class="math inline">\theta</span> is an element <span class="math inline">\hat{\theta} \in \Theta</span> which attains the maximum value of <span class="math inline">L(\theta)</span> in <span class="math inline">\Theta</span>, i.e.&nbsp;such that <span class="math inline">L(\hat{\theta}) \ge L(\theta)</span> or equivalently <span class="math display">
L(\hat{\theta}) = \max_{\theta \in \Theta} L(\theta).
</span></p>
<p>The <span class="blue">maximum likelihood estimator</span> (MLE) of the parameter <span class="math inline">\theta</span> based on a sample <span class="math inline">\bm{Y}</span> is <span class="math inline">\hat{\theta}(\bm{Y})</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>Intuitively, the MLE is a reasonable choice: it is the parameter point for which the observed sample is <span class="blue">most likely</span>.</p></li>
<li><p>Clearly, the MLE is also the maximizer of the log-likelihood: <span class="math inline">\ell(\hat{\theta}) = \max_{\theta \in \Theta} \ell(\theta)</span>.</p></li>
</ul>
</section>
<section id="properties-and-remarks-about-the-mle" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="properties-and-remarks-about-the-mle">Properties and remarks about the MLE 📖</h2>
<ul>
<li><span class="blue">Remark I</span>: the MLE may <span class="orange">not exists</span> and is <span class="orange">not</span> necessarily <span class="orange">unique</span>. On the other hand, if <span class="math inline">\Theta \subseteq \mathbb{R}^p</span> and <span class="math inline">l(\theta)</span> is differentiable, then it can be found as the solution of the <span class="blue">score equations</span>: <span class="math display">
\ell^*(\theta) = \frac{\partial}{\partial \theta}\ell(\theta) = 0.
</span></li>
</ul>
<!-- - If $s(y)$ is a [sufficient statistic]{.blue}, then the MLE is a function of it. -->
<ul>
<li><p><span class="blue">Remark II</span>: often <span class="math inline">\hat{\theta}</span> cannot be written explicitly as a function of the sample values, i.e.&nbsp;in general the MLE has <span class="orange">no closed-form expression</span> but it must be found using <span class="orange">numerical procedures</span>.</p></li>
<li><p><span class="blue">Remark III</span>: the likelihood function has to be maximized in the set space <span class="math inline">\Theta</span> specified by the statistical model, not over the set of the mathematically admissible values of <span class="math inline">\theta</span>.</p></li>
</ul>
<!-- - [Remark IV]{.blue}: it is not necessary for $\Theta$ to be a numeric set, i.e. we need [not]{.orange} be dealing with a [parametric]{.orange} model, although we shall restrict ourself to this case. -->
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem <span class="citation" data-cites="Casella2002">(Invariance, <a href="#ref-Casella2002" role="doc-biblioref">Casella and Berger 2002</a>, Theorem 7.2.10)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\psi(\cdot)</span> be <span class="orange">one-to-one</span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, that is, a <span class="blue">reparametrization</span>, from the set <span class="math inline">\Theta</span> onto the set <span class="math inline">\Psi</span>. Then the MLE of <span class="math inline">\psi = \psi(\theta)</span> is <span class="math inline">\hat{\psi} = \psi(\hat{\theta})</span> where <span class="math inline">\hat{\theta}</span> denotes the MLE of <span class="math inline">\theta</span>.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;It generalizes to any <span class="math inline">g(\cdot)</span>. If <span class="math inline">\hat{\theta}</span> is the MLE, then <span class="math inline">g(\hat{\theta})</span> is the “MLE” of an “induced likelihood”.</p></div></div></section>
<section id="example-poisson-with-unknown-mean" class="level2">
<h2 class="anchored" data-anchor-id="example-poisson-with-unknown-mean">Example: Poisson with unknown mean 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be a iid random sample from a Poisson distribution of mean parameter <span class="math inline">\lambda &gt; 0</span>, with <span class="orange">likelihood function</span> <span class="math display">
L(\lambda) = \prod_{i=1}^n \frac{e^{-\lambda} \lambda^{y_i}}{y_i!}.
</span></p></li>
<li><p>Therefore the <span class="blue">log-likelihood</span>, up to an additive constant <span class="math inline">c</span> not depending on <span class="math inline">\lambda</span>, is <span class="math display">
\ell(\lambda) = \sum_{i=1}^ny_i\log{\lambda} - n\lambda + c.
</span></p></li>
<li><p>The <span class="orange">maximum likelihood estimator</span> <span class="math inline">\hat{\lambda}</span> is found by maximizing <span class="math inline">\ell(\lambda)</span>. In this regular problem, this can be done by studying the first derivative: <span class="math display">
\ell^*(\lambda) = \frac{1}{\lambda}\sum_{i=1}^ny_i - n.
</span></p></li>
<li><p>We solve <span class="math inline">\ell^*(\lambda) = 0</span>, obtaining <span class="math inline">\hat{\lambda} = \bar{y}</span>. This is indeed a maximizer of <span class="math inline">\ell(\lambda)</span> (<span class="orange">why</span>?).</p></li>
</ul>
</section>
<section id="example-binomial-with-unknown-trials-mle" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="example-binomial-with-unknown-trials-mle">Example: binomial with unknown trials MLE</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be iid <span class="math inline">\textup{Bin}(N, p)</span>, and suppose <span class="math inline">N</span> is <span class="orange">unknown</span>, while <span class="math inline">p</span> is considered <span class="blue">known</span>. This constitutes a non-regular problem because <span class="math inline">N</span> is <span class="blue">discrete</span>.</p></li>
<li><p>The likelihood function is <span class="math display">
L(N) = \prod_{i=1}^n\binom{N}{y_i} p^{y_i}(1 - p)^{N - y_i},
</span> where the maximum <span class="orange">cannot</span> be obtained through <span class="orange">differentiation</span>, as <span class="math inline">N \in \mathbb{N}</span>.</p></li>
<li><p>Naturally, we require that <span class="math inline">\hat{N} \ge \max_i y_i</span>, since <span class="math inline">L(N) = 0</span> for any <span class="math inline">N &lt; \max_i y_i</span>. The ML is therefore an integer <span class="math inline">\hat{N} \ge \max_i y_i</span> such that <span class="math display">
L(\hat{N}) \ge L(\hat{N} - 1), \qquad L(\hat{N} + 1) &lt; L(\hat{N}).
</span></p></li>
<li><p>This value must be found <span class="orange">numerically</span>. However, it can be shown<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> that there exists exactly one such <span class="math inline">\hat{N}</span>, meaning the MLE is <span class="blue">unique</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;This problem is described in Example 7.2.9 of <span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>, pag. 318. See also Example 7.2.13: such estimate has a large variance in practice.</p></div></div></section>
<section id="example-binomial-with-unknown-trials-mle-1" class="level2">
<h2 class="anchored" data-anchor-id="example-binomial-with-unknown-trials-mle-1">Example: binomial with unknown trials MLE</h2>
<ul>
<li>Let us consider the following data, in which both <span class="math inline">N</span> and <span class="math inline">p</span> are unknown. These are <span class="blue">simulated data</span> and the <span class="grey">true values</span> were <span class="math inline">N = 75</span> and <span class="math inline">p = 0.32</span>.</li>
</ul>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] 16 18 22 25 27</code></pre>
</div>
</div>
<ul>
<li>The <span class="orange">method of moments</span> estimator gives <span class="math inline">\hat{N}_\text{MM}</span> = 102 (rounded to the closest integer) and <span class="math inline">\hat{p}_\text{MM}</span> = 0.21. The <span class="blue">maximum likelihood</span>, instead, gives <span class="math inline">\hat{N}_\text{ML}</span> = 99 and <span class="math inline">\hat{p}_\text{ML}</span> = 0.22.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1350"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>If we replace <span class="math inline">27</span> with a <span class="math inline">28</span>, we obtain drastically different estimates, namely <span class="math inline">\hat{N}_\text{MM} =  195</span> and <span class="math inline">\hat{N}_\text{ML} = 191</span>, demonstrating a <span class="orange">large</span> amount of <span class="orange">variability</span>.</li>
</ul>
</section>
<section id="m-estimators" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="m-estimators">M-estimators</h2>
<ul>
<li>M- and Z- estimators are broad class of estimators that encompass the maximum likelihood (iid observations) and other popular methods as special cases. <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;A detailed discussion is offered in <span class="citation" data-cites="vandervaart1998">van der Vaart (<a href="#ref-vandervaart1998" role="doc-biblioref">1998</a>)</span>, Chap. 5.</p></div></div><div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>An <span class="blue">M-estimator</span> is the <span class="orange">maximizer</span> over <span class="math inline">\Theta</span> of a function <span class="math inline">M(\theta) : \Theta \rightarrow \mathbb{R}</span> of the type: <span class="math display">
M(\theta) = \sum_{i=1}^n m(\theta; Y_i),
</span> where <span class="math inline">m(\theta; Y_i)</span> are known real-valued functions.</p>
</div>
</div>
</div>
<ul>
<li><span class="orange">Remark</span>: when <span class="math inline">m(\theta; y) = \log{f(Y_i; \theta)}</span> this coincides with the MLE of a model with iid observations.</li>
</ul>
<!-- -   The term $1/n$ is included here to facilitate the description of the -->
<!--     subsequent asymptotic theory, but it is obviously non influential. -->
</section>
<section id="z-estimators" class="level2">
<h2 class="anchored" data-anchor-id="z-estimators">Z-estimators</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>A <span class="blue">Z-estimator</span> is the <span class="orange">solution</span> over <span class="math inline">\Theta</span> of a system of equations function <span class="math inline">Q(\theta) = \bm{0}</span> of the type: <span class="math display">
Q(\theta) = Q(\theta; Y) = \sum_{i=1}^n q(\theta; Y_i) = \bm{0},
</span> where <span class="math inline">q(\theta) = q(\theta; y)</span> are known vector-valued maps. These are called <span class="blue">estimating equations</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>When <span class="math inline">\theta = (\theta_1,\dots,\theta_p)</span>, <span class="math inline">Q</span> and <span class="math inline">q</span> typically have <span class="math inline">p</span> coordinate functions, namely we consider: <span class="math display">
Q_r(\theta) = \sum_{i=1}^n q_r(\theta; Y_i)= 0, \qquad r = 1,\dots,p.
</span></p></li>
<li><p>In many examples <span class="math inline">q_r(y; \theta)</span> are the partial derivatives of a function <span class="math inline">m(\theta; y)</span>, that is <span class="math display">
Q(\theta) = \frac{\partial}{\partial \theta} M(\theta).</span> An example is the <span class="blue">score function</span> <span class="math inline">\ell^*(\theta)</span>. However, this is <span class="orange">not always the case</span>.</p></li>
</ul>
<!-- ## Plug-in estimators -->
</section>
<section id="huber-estimators-i" class="level2">
<h2 class="anchored" data-anchor-id="huber-estimators-i">Huber estimators I</h2>
<ul>
<li><p>The <span class="orange">location</span> of a r.v. <span class="math inline">Y</span> is a vague term that can be made precise by defining it as the expectation <span class="math inline">\mathbb{E}(Y)</span>, a quantile, or the center of symmetry, as in the following example.</p></li>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be a iid sample of real-valued random variables belonging to family of distributions <span class="math inline">\mathcal{F}</span> defined as <span class="math display">
\mathcal{F} = \{f(y - \theta) : \theta \in \Theta \subseteq \mathbb{R} \},
</span> for some unknown density <span class="math inline">f(y)</span> <span class="blue">symmetric</span> around <span class="math inline">0</span>. The parameter <span class="math inline">\theta</span> is the <span class="orange">location</span>.</p></li>
<li><p>Classical M- estimators for <span class="math inline">\theta</span> are the <span class="blue">mean</span> and the <span class="orange">median</span>, maximizing: <span class="math display">
-\sum_{i=1}^n (Y_i - \theta)^2, \quad (\text{Mean}) \qquad \qquad - \sum_{i=1}^n |Y_i - \theta|, \quad (\text{Median})
</span> or alternatively (Z- estimator forms) solving the equations <span class="math display">
\sum_{i=1}^n (Y_i - \theta) = 0, \quad (\text{Mean}) \qquad \qquad \sum_{i=1}^n \text{sign}(Y_i - \theta) = 0, \quad (\text{Median}).
</span></p></li>
</ul>
<!-- ## Location estimators II -->
<!-- - Both estimating equations involve functions of the form $q(\theta; Y) = q(Y - \theta)$ that are [monotone]{.blue} and [odd]{.orange} around zero.  -->
<!-- - It is hence reasonable to study estimating equations of the type:$$ -->
<!-- Q(\theta) = \frac{1}{n}\sum_{i=1}^n q(Y_i - \theta)= 0. -->
<!-- $$ -->
<!-- - This class of estimators has an appealing [equivariance]{.orange} property. If the observations $Y_i$ are shifted by a fixed amount $\alpha$, then so is the estimate: -->
<!-- $$ -->
<!-- \hat{\theta} + \alpha \quad \text{ solves } \quad Q(\theta) = \frac{1}{n}\sum_{i=1}^n q(Y_i + \alpha - \theta)= 0, -->
<!-- $$ -->
<!-- if $\hat{\theta}$ solves the original equation. -->
</section>
<section id="huber-estimators-ii" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="huber-estimators-ii">Huber estimators II</h2>
<ul>
<li><p><span class="blue">Huber estimators</span> can be regarded as a compromise between the mean and the median, maximizing the following function: <span class="math display">
M(\theta) = -\sum_{i=1}^n m(Y_i - \theta), \qquad m(y) = \begin{cases}\frac{1}{2}y^2 \quad &amp;\text{ if } |y| \le k\\
k |y| - \frac{1}{2}k^2 \quad &amp;\text{ if } |y| \ge k
\end{cases}
</span> where <span class="math inline">k &gt; 0</span> is a <span class="orange">tuning</span> parameter. The function <span class="math inline">m(y)</span> is continuous and differentiable<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. The choice <span class="math inline">k \rightarrow 0</span> leads to the median, whereas for <span class="math inline">k \rightarrow \infty</span> we get the mean.</p></li>
<li><p>Equivalently, we can consider the solution of the following estimating equation: <span class="math display">
Q(\theta) = \sum_{i=1}^n q(Y_i - \theta)= 0, \qquad q(y) = \begin{cases} -k \quad &amp;\text{ if }\: y \le -k\\
y \quad &amp;\text{ if  }\: |y| \le k \\
k \quad &amp;\text{ if }\: y \ge k\end{cases}
</span></p></li>
<li><p>Unfortunately, there is no closed-form expression and the equation must be solved numerically.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;See Exercise 10.28 of <span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>.</p></div></div></section>
<section id="example-newcombs-speed-of-light" class="level2">
<h2 class="anchored" data-anchor-id="example-newcombs-speed-of-light">Example: Newcomb’s speed of light</h2>
<ul>
<li>Data <span class="math inline">y_1, \dots, y_{66}</span> represent Simon Newcomb’s measurements (1882) of the <span class="orange">speed of light</span>. The data are recordes as <span class="blue">deviations</span> from 24,800 nanoseconds.</li>
</ul>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code> [1]  28  26  33  24  34 -44  27  16  40  -2  29  22  24  21  25  30  23  29  31
[20]  19  24  20  36  32  36  28  25  21  28  29  37  25  28  26  30  32  36  26
[39]  30  22  36  23  27  27  28  27  31  27  26  33  26  32  32  24  39  28  24
[58]  25  32  25  29  27  28  29  16  23</code></pre>
</div>
</div>
<ul>
<li><p>There are <span class="orange">two outliers</span> (-44 and -2) which could influence the analysis.</p></li>
<li><p>We see that as <span class="math inline">k</span> increases, the <span class="blue">Huber estimate</span> varies between the median (27) and the mean (26.21), so we interpret increasing <span class="math inline">k</span> as decreasing <span class="orange">robustness</span> to outliers.</p></li>
<li><p>The suggested <span class="blue">default</span> for <span class="math inline">k</span> is roughly <span class="math inline">k \approx 4.5</span>, that is <span class="math inline">k = 1.5 \times \text{MAD}</span>.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 4%">
<col style="width: 9%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><span class="math inline">k</span></th>
<th style="text-align: right;">0</th>
<th style="text-align: right;">5</th>
<th style="text-align: right;">10</th>
<th style="text-align: right;">20</th>
<th style="text-align: right;">30</th>
<th style="text-align: right;">40</th>
<th style="text-align: right;">50</th>
<th style="text-align: right;">60</th>
<th style="text-align: right;">70</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Est.</td>
<td style="text-align: right;">27</td>
<td style="text-align: right;">27.37</td>
<td style="text-align: right;">27.417</td>
<td style="text-align: right;">27.125</td>
<td style="text-align: right;">26.831</td>
<td style="text-align: right;">26.677</td>
<td style="text-align: right;">26.523</td>
<td style="text-align: right;">26.369</td>
<td style="text-align: right;">26.215</td>
</tr>
</tbody>
</table>
<ul>
<li>Based on recent measurements, the <span class="orange">true value</span> of the speed of light, expressed in this scale, is 33. <span class="blue">Huber estimate</span> for reasonable values of <span class="math inline">k</span> is <span class="blue">closer</span> than both median and mean.</li>
</ul>
</section>
<section id="bayesian-estimators" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-estimators">Bayesian estimators</h2>
<ul>
<li><p>Bayesian estimators are obtained following a <span class="orange">different inferential paradigm</span> than the one considered here, but they also exhibit <span class="blue">appealing frequentist properties</span>.</p></li>
<li><p>Let <span class="math inline">L(\theta; \bm{y})</span> denote the likelihood function, and let <span class="math inline">\pi(\theta)</span> represent the <span class="orange">prior</span> distribution. Bayesian inference is based on the <span class="blue">posterior</span> distribution, defined as: <span class="math display">
\pi(\theta \mid \bm{y}) = \frac{\pi(\theta) L(\theta; \bm{y})}{\int_\Theta \pi(\theta) L(\theta; \bm{y}) \mathrm{d}\theta}.
</span></p></li>
<li><p>Under certain hypotheses, which will be clarified later, the <span class="blue">posterior mean</span> serves as an <span class="orange">optimal Bayesian estimator</span>: <span class="math display">
\hat{\theta}_\text{Bayes} = \mathbb{E}(\theta \mid \bm{Y}) = \frac{\int_\Theta \theta \: \pi(\theta) L(\theta; \bm{Y}) \mathrm{d}\theta}{\int_\Theta \pi(\theta) L(\theta; \bm{Y}) \mathrm{d}\theta}.
</span> However, this estimator is not always available in closed form.</p></li>
<li><p>Other “optimal” Bayesian estimators include, for instance, the posterior median.</p></li>
</ul>
</section>
<section id="example-binomial-bayes-estimator" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="example-binomial-bayes-estimator">Example: binomial Bayes estimator</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be iid Bernoulli random variables with probability <span class="math inline">p</span>, and let the prior <span class="math inline">p \sim \text{Beta}(a, b)</span>. Moreover, let <span class="math inline">n_1 = \sum_{i=1}^n y_i</span> be the <span class="blue">number of successes</span> out of <span class="math inline">n</span> trials.</p></li>
<li><p>Standard calculations in Bayesian statistics yield the (<span class="orange">conjugate</span>) posterior <span class="math inline">(p \mid Y_1,\dots,Y_n) \sim \text{Beta}(a + n_1, b + n - n_1)</span>. Hence, the <span class="blue">posterior mean</span> is <span class="math display">
\hat{p} = \mathbb{E}(p \mid Y_1,\dots,Y_n) = \frac{a + n_1}{a + b + n}.
</span></p></li>
<li><p>Note that we can rewrite <span class="math inline">\hat{p}</span> in the following way: <span class="math display">
\hat{p} = \left(\frac{n}{a + b + n}\right)\bar{y} + \left(\frac{a + b}{a + b + n}\right) \left(\frac{a}{a + b}\right),
</span> that is, as a <span class="orange">linear combination</span> of the prior mean and the sample mean, with weights determined by <span class="math inline">a, b</span>, and <span class="math inline">n</span>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;This is not a coincidence, and it essentially holds for general exponential families; see the elegant paper by <span class="citation" data-cites="Diaconis1979">Diaconis and Ylvisaker (<a href="#ref-Diaconis1979" role="doc-biblioref">1979</a>)</span>.</p></div></div></section>
</section>
<section id="methods-of-evaluating-estimators" class="level1 page-columns page-full">
<h1>Methods of evaluating estimators</h1>
<section id="comparing-estimators" class="level2">
<h2 class="anchored" data-anchor-id="comparing-estimators">Comparing estimators</h2>
<ul>
<li><p>We study the performance of estimators, aiming to determine when an estimator <span class="math inline">\hat{\theta}</span> can be considered <span class="orange">good</span> or <span class="blue">optimal</span>.</p></li>
<li><p>This requires <span class="orange">criteria</span> for evaluation, often provided by <span class="blue">decision theory</span>, which relies on a <span class="orange">loss function</span> <span class="math inline">\mathscr{L}</span>. The function <span class="math inline">\mathscr{L}(\theta, \hat{\theta})</span> quantifies the loss incurred when estimating <span class="math inline">\theta</span> by <span class="math inline">\hat{\theta}</span>.</p></li>
<li><p>Typically, we assume <span class="math display">
\mathscr{L}(\theta, \theta) = 0 \text{ (no loss for the correct estimate)}, \qquad \mathscr{L}(\theta, \hat{\theta}) \geq 0,
</span> for all <span class="math inline">\theta</span> and <span class="math inline">\hat{\theta}</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Since <span class="math inline">\hat{\theta}(\bm{Y})</span> is random, we need a way to summarize the loss. A common criterion is the (frequentist) <span class="orange">risk function</span>, namely the <span class="blue">average loss</span>, defined as the expectation <span class="math display">
R(\theta; \hat{\theta}) = \mathbb{E}_\theta\{\mathscr{L}(\theta, \hat{\theta}(\bm{Y}))\}.
</span></p>
</div>
</div>
</div>
</section>
<section id="optimal-estimators" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="optimal-estimators">Optimal estimators</h2>
<ul>
<li><p>An <span class="blue">oracle estimator</span> <span class="math inline">\hat{\theta}_\text{oracle}</span>, which never makes errors, satisfies <span class="math display">
R(\theta; \hat{\theta}_\text{oracle}) = 0,
</span> for all <span class="math inline">\theta \in \Theta</span>. Clearly, such an estimator <span class="orange">does not exist</span>.</p></li>
<li><p>An <span class="blue">optimal estimator</span> <span class="math inline">\hat{\theta}_\text{opt}</span> <span class="blue">uniformly</span> minimizes the risk, meaning <span class="math display">
R(\theta; \hat{\theta}_\text{opt}) \leq R(\theta; \tilde{\theta}),
</span> for all <span class="math inline">\theta \in \Theta</span> and estimators <span class="math inline">\tilde{\theta}</span>. Except for trivial cases, such an estimator <span class="orange">does not exist</span> unless one <span class="blue">restricts</span> the class of estimators considered <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p></li>
<li><p>In fact, the constant estimator <span class="math inline">\hat{\theta}_\text{const}(\bm{Y}) = \theta_1</span> has zero risk when <span class="math inline">\theta = \theta_1</span> but positive risk otherwise. Thus, <span class="math inline">\hat{\theta}_\text{opt}</span> would need <span class="math inline">R(\theta; \hat{\theta}_\text{opt}) = 0</span> for all <span class="math inline">\theta \in \Theta</span>, making it identical to the oracle.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;A common restriction is unbiasedness, namely the requirement <span class="math inline">\mathbb{E}_\theta(\hat{\theta}) = \theta</span>. An alternative restriction is equivariance.</p></div></div></section>
<section id="admissible-estimators" class="level2">
<h2 class="anchored" data-anchor-id="admissible-estimators">Admissible estimators</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>An estimator <span class="math inline">\hat{\theta}</span> is <span class="orange">admissible</span> if no other estimator <span class="math inline">\tilde{\theta}</span> exists such that<br>
<span class="math display">
R(\theta; \tilde{\theta}) \le R(\theta; \hat{\theta}), \qquad \text { for all } \theta \in \Theta,
</span> with strict inequality for at least one value of <span class="math inline">\theta</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>Broadly speaking, an <span class="blue">admissible</span> estimator <span class="math inline">\hat{\theta}</span> will perform better than an alternative <span class="math inline">\tilde{\theta}</span> for some values of <span class="math inline">\theta</span> and worse for others.</p></li>
<li><p>In principle, disregarding any practical implications, an <span class="orange">inadmissible</span> estimator should not be used, as it is <span class="orange">dominated</span> by a better alternative.</p></li>
<li><p>The admissibility criterion is interesting due to its <span class="blue">selective</span> nature, eliminating dominated estimators.</p></li>
<li><p>However, <span class="blue">admissibility</span> alone is <span class="orange">not enough</span>: for instance, the constant estimator <span class="math inline">\hat{\theta}_\text{const} = \theta_1</span> is admissible but clearly unsatisfactory.</p></li>
</ul>
</section>
<section id="the-choice-of-the-loss-function" class="level2">
<h2 class="anchored" data-anchor-id="the-choice-of-the-loss-function">The choice of the loss function</h2>
<ul>
<li><p>The choice of the loss function has its roots in <span class="blue">decision theory</span>. The loss function <span class="math inline">\mathscr{L}</span> is a nonnegative function that generally increases as the distance between <span class="math inline">\hat{\theta}</span> and <span class="math inline">\theta</span> increases.</p></li>
<li><p>Let <span class="math inline">\theta \in \mathbb{R}^p</span>. Two <span class="orange">commonly used</span> loss functions are <span class="math display">
\mathscr{L}(\theta, \hat{\theta}) = ||\hat{\theta} - \theta||_1, \qquad \text{(Absolute error loss)},
</span> and<br>
<span class="math display">
\mathscr{L}(\theta, \hat{\theta}) = ||\hat{\theta} - \theta||_2^2, \qquad \text{(Quadratic loss)}.
</span></p></li>
<li><p>The quadratic loss is the de facto standard in many contexts, leading to the <span class="orange">mean squared error</span> and the well-known <span class="blue">bias-variance</span> decomposition. Let <span class="math inline">\Theta \subseteq \mathbb{R}</span>, then<br>
<span class="math display">
R(\theta; \hat{\theta}) = \mathbb{E}_\theta\{(\hat{\theta} - \theta)^2\} = \text{bias}_\theta(\hat{\theta})^2 + \text{var}_\theta(\hat{\theta}), \qquad \text{(Mean squared error)}.
</span></p></li>
<li><p>Both these losses are <span class="blue">convex</span>, and the quadratic loss is <span class="orange">strictly convex</span>. Convexity will be crucial in the subsequent theoretical developments.</p></li>
</ul>
</section>
<section id="other-loss-functions" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="other-loss-functions">Other loss functions</h2>
<ul>
<li><p>In principle, the choice of the loss should be based on its properties rather than mathematical convenience. Here we present some less common <span class="orange">examples</span>.</p></li>
<li><p><span class="blue">Weighted quadratic loss</span>, a variant of the quadratic loss, accounting for weights. Let <span class="math inline">\theta \in \mathbb{R}^p</span> <span class="math display">
\mathscr{L}(\theta, \hat{\theta}) = (\theta - \hat{\theta})^T A (\theta - \hat{\theta}),
</span> where <span class="math inline">A \in \mathbb{R}^{p \times p}</span> is a positive definite matrix. This loss is <span class="orange">strictly convex</span>.</p></li>
<li><p><span class="blue">Stein Loss</span> <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. Let <span class="math inline">\theta &gt; 0</span>, such as the <span class="orange">population variance</span> of a model. Stein loss is defined as: <span class="math display">
\mathscr{L}(\theta,\hat{\theta}) = \frac{\hat{\theta}}{\theta} - 1 - \log\frac{\hat{\theta}}{\theta}.
</span> A criticism of quadratic loss for variance estimation is that underestimation has finite penalty, while overestimation as infinite penalty. Instead, Stein loss <span class="math inline">\mathscr{L}(\theta,\hat{\theta}) \rightarrow \infty</span> as <span class="math inline">\hat{\theta}\rightarrow 0</span> and <span class="math inline">\hat{\theta}\rightarrow \infty</span>.</p></li>
<li><p>Other examples are <span class="blue">Huber losses</span> <span class="citation" data-cites="Lehmann1998">(see <a href="#ref-Lehmann1998" role="doc-biblioref">Lehmann and Casella 1998,pp. 51–52</a>)</span> and <span class="blue">intrinsic losses</span> <span class="citation" data-cites="Robert1994">(see <a href="#ref-Robert1994" role="doc-biblioref">Robert 1994,p. 2.5.4</a>)</span>, such as the <span class="orange">entropy distance</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;See Examples 7.3.26 and 7.3.27 in <span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span> for a comparison in the estimation of the population variance <span class="math inline">\sigma^2</span> under the quadratic loss and the Stein loss.</p></div></div></section>
<section id="example-mse-of-binomial-estimators-i" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="example-mse-of-binomial-estimators-i">Example: MSE of binomial estimators I 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be iid Bernoulli random variables with probability <span class="math inline">p</span>. Moreover, let <span class="math inline">n_1 = \sum_{i=1}^n y_i</span> be the <span class="blue">number of successes</span> out of <span class="math inline">n</span> trials.</p></li>
<li><p>The <span class="orange">proportion</span> <span class="math inline">\hat{p} = \bar{y} = n_1/n</span> is the <span class="orange">maximum likelihood</span> (and method of moments) estimator. Simple calculations yield <span class="math display">
R(p; \hat{p}) = \mathbb{E}_p\{(\hat{p} - p)^2\} = \text{var}_p(\hat{p}) = \frac{p(1-p)}{n}.
</span></p></li>
<li><p>Let us consider a <span class="blue">Bayesian estimator</span> for <span class="math inline">p</span> under a beta prior with parameters <span class="math inline">a = b = 0.5\sqrt{n}</span>, yielding <span class="math display">
\hat{p}_\text{minimax} = \frac{n_1 + 0.5\sqrt{n}}{n + \sqrt{n}}, \qquad R(p; \hat{p}_\text{minimax}) = \mathbb{E}_p\{(\hat{p}_\text{minimax} - p)^2\} = \frac{n}{4 ( n + \sqrt{n})^2}.
</span> This Bayesian estimator has <span class="orange">constant risk</span>, that is, <span class="math inline">R(p; \hat{p}_\text{minimax})</span> does not depend on <span class="math inline">p</span>.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;Some subjective Bayesians may criticize this estimator for not being “truly Bayesian” as the hyperparameters depend on the sample size <span class="math inline">n</span>, making the prior data-dependent. However, here we are evaluating <span class="math inline">\hat{p}_\text{Bayes}</span> from a frequentist perspective.</p></div></div></section>
<section id="example-mse-of-binomial-estimators-ii" class="level2">
<h2 class="anchored" data-anchor-id="example-mse-of-binomial-estimators-ii">Example: MSE of binomial estimators II</h2>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="1350"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="example-mse-of-binomial-estimators-iii" class="level2">
<h2 class="anchored" data-anchor-id="example-mse-of-binomial-estimators-iii">Example: MSE of binomial estimators III</h2>
<ul>
<li><p>Neither <span class="math inline">\hat{p}</span> dominates <span class="math inline">\hat{p}_\text{minimax}</span> nor vice versa. In fact, both estimators are <span class="blue">admissible</span>.</p></li>
<li><p>For small values of <span class="math inline">n</span>, <span class="math inline">\hat{p}_\text{minimax}</span> is the <span class="orange">better choice</span> unless one believes that <span class="math inline">p</span> is very close to one of the extremes <span class="math inline">0</span> or <span class="math inline">1</span>.</p></li>
<li><p>Conversely, for large values of <span class="math inline">n</span>, the maximum likelihood estimator <span class="math inline">\hat{p}</span> is the better choice unless one strongly believes that <span class="math inline">p</span> is very close to <span class="math inline">0.5</span>.</p></li>
<li><p>This information, <span class="blue">combined</span> with the <span class="blue">knowledge of the problem</span> at hand, can lead to choosing the better estimator for the situation.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li><p>We will get back to this example, as both <span class="math inline">\hat{p}</span> and <span class="math inline">\hat{p}_\text{minimax}</span> are <span class="blue">optimal</span> in some sense.</p></li>
<li><p>In fact, <span class="math inline">\hat{p}</span> is the <span class="orange">UMVU estimator</span> (uniform minimum variance unbiased estimator), and <span class="math inline">\hat{p}_\text{minimax}</span> is the <span class="orange">minimax</span> estimator.<br>
</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="james-stein-estimator-i" class="level2">
<h2 class="anchored" data-anchor-id="james-stein-estimator-i">James-Stein estimator I</h2>
<ul>
<li><p>Let <span class="math inline">\bm{Y} = (Y_1,\dots,Y_p)</span> be a <span class="blue">Gaussian random vector</span>, <span class="math inline">\bm{Y} \sim \text{N}_p(\mu, I_p)</span>, with unknown means <span class="math inline">\mu = (\mu_1,\dots,\mu_p)</span> and fixed variance. That is, <span class="math display">
Y_j \overset{\text{ind}}{\sim} \text{N}(\mu_j, 1), \qquad j=1,\dots,p.
</span></p></li>
<li><p>We are interested in <span class="orange">estimating</span> the <span class="orange">means</span> <span class="math inline">\mu</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li>Intuitively, the most “natural” approach is the maximum likelihood / method of moments estimator, which in this case with <span class="math inline">n = 1</span> is simply <span class="math display">
\hat{\mu}_j = Y_j, \qquad j=1,\dots,p.
</span></li>
<li>This estimator is “optimal” in the sense that it is the UMVUE, as we shall see in the next sections. Moreover, its <span class="orange">frequentist risk</span> under a squared loss is <span class="math display">
R(\mu; \hat{\mu}) = \mathbb{E}_\mu(||\hat{\mu} - \mu||_2^2) = p,
</span> because <span class="math inline">||\hat{\mu} - \mu||_2^2 = ||\bm{Y} - \mu||_2^2 \sim \chi^2_p</span>.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="james-stein-estimator-ii" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="james-stein-estimator-ii">James-Stein estimator II</h2>
<ul>
<li><p>By the early 1950s, three proofs had emerged to show that <span class="math inline">\hat{\mu}</span> is <span class="blue">admissible</span> for squared error loss <span class="orange">when</span> <span class="math inline">p = 1</span>.</p></li>
<li><p>Nevertheless, <span class="citation" data-cites="Stein1956">Stein (<a href="#ref-Stein1956" role="doc-biblioref">1956</a>)</span><a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> <span class="blue">stunned the statistical world</span> when he proved that although <span class="math inline">\hat{\mu}</span> is admissible for squared error loss when <span class="math inline">p = 2</span>, it is <span class="orange">inadmissible</span> <span class="orange">when</span> <span class="math inline">p \geq 3</span>.</p></li>
<li><p>In fact, <span class="citation" data-cites="James1961">James and Stein (<a href="#ref-James1961" role="doc-biblioref">1961</a>)</span><a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> showed that the estimator<br>
<span class="math display">
\hat{\mu}_\text{JS} = \left(1 - \frac{p-2}{||\bm{Y}||_2^2}\right)\bm{Y}
</span> strictly <span class="orange">dominates</span> <span class="math inline">\hat{\mu}</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;<span class="citation" data-cites="Stein1956">Stein (<a href="#ref-Stein1956" role="doc-biblioref">1956</a>)</span>, Inadmissibility of the usual estimator of the mean of a multivariate normal distribution, <em>Proc. Third Berkeley Symposium</em>, 1, 197–206, Univ. California Press.</p></div><div id="fn10"><p><sup>10</sup>&nbsp;<span class="citation" data-cites="James1961">James and Stein (<a href="#ref-James1961" role="doc-biblioref">1961</a>)</span>, Estimation with quadratic loss, <em>Proc. Fourth Berkeley Symposium</em>, 1, 361–380, Univ. California Press.</p></div></div></section>
<section id="james-stein-estimator-iii" class="level2">
<h2 class="anchored" data-anchor-id="james-stein-estimator-iii">James-Stein estimator III 📖</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (James-Stein, 1961)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\bm{Y} \sim \text{N}_p(\mu, I_p)</span>. The frequentist risk of the James-Stein estimator <span class="math inline">\hat{\mu}_\text{JS}</span> under quadratic loss is <span class="math display">
R(\mu; \hat{\mu}_\text{JS}) = \mathbb{E}_\mu(||\hat{\mu}_\text{JS} - \mu||_2^2) = p - (p -2)^2\mathbb{E}_\mu\left(\frac{1}{||\bm{Y}||_2^2}\right).
</span> Thus, <span class="math inline">\hat{\mu}_\text{JS}</span> <span class="blue">strictly dominates</span> <span class="math inline">\hat{\mu}</span>, meaning that <span class="math inline">R(\mu; \hat{\mu}_\text{JS}) &lt; R(\mu; \hat{\mu}) = p</span>. Moreover, <span class="math display">
R(\mu; \hat{\mu}_\text{JS}) \le p - \frac{(p-2)^2}{p - 2 + ||\mu||_2^2}.
</span></p>
</div>
</div>
<ul>
<li><p>Geometrically, the James-Stein estimator <span class="orange">shrinks</span> each component of <span class="math inline">\bm{Y}</span> towards the origin.</p></li>
<li><p>The biggest <span class="blue">improvement</span> occurs when <span class="math inline">\mu</span> is close to zero. For <span class="math inline">\mu = 0</span>, we have <span class="math inline">R(0; \hat{\mu}_\text{JS}) = 2</span> for all <span class="math inline">p \geq 2</span>. As <span class="math inline">||\mu||_2^2 \rightarrow \infty</span>, the risk approaches <span class="math inline">R(\mu; \hat{\mu}_\text{JS}) \rightarrow p</span>.</p></li>
</ul>
</section>
<section id="james-stein-estimator-iv" class="level2">
<h2 class="anchored" data-anchor-id="james-stein-estimator-iv">James-Stein estimator IV</h2>
<!-- - The James-Stein estimator is also [inadmissible]{.orange}. Note that $(1 - (p-2)/||\bm{Y}||_2^2)$ can be negative. Using its positive part, $(1 - (p-2)/||\bm{Y}||_2^2)_+$, gives a [uniformly better]{.blue} (inadmissible!) estimator.   -->
<ul>
<li><p>A useful <span class="orange">generalization</span> of the <span class="blue">James-Stein estimator</span> consists in <span class="orange">shrinking</span> the mean towards a <span class="blue">common value</span> <span class="math inline">m \in \mathbb{R}</span> rather than <span class="math inline">0</span>, giving <span class="math display">
\hat{\mu}_\text{JS}(m) = m + \left(1 - \frac{p-2}{||\bm{Y} - m||_2^2}\right)(\bm{Y} - m).
</span> It holds that <span class="math inline">R(\mu; \hat{\mu}_\text{JS}(m)) \le p - (p-2)^2/ (p - 2 + ||\mu - m||_2^2)</span>, therefore <span class="math inline">\hat{\mu}_\text{JS}(m)</span> <span class="blue">dominates</span> <span class="math inline">\hat{\mu}</span>.</p></li>
<li><p>Even better, if we <span class="orange">estimate</span> <span class="math inline">m</span> with the <span class="blue">arithmetic mean</span> of <span class="math inline">\bm{Y}</span>, that is <span class="math inline">\bar{Y} = (1/p) \sum_{j=1}^p Y_j</span>, we can obtain the following <span class="orange">shrinkage estimator</span>: <span class="math display">
\hat{\mu}_\text{shrink} = \bar{Y} + \left(1 - \frac{p-3}{||\bm{Y} - \bar{Y}||_2^2}\right)(\bm{Y} - \bar{Y}).
</span> Intuitively, <span class="math inline">p-3</span> is the appropriate constant as an additional parameter is estimated. Moreover, in <span class="citation" data-cites="Efron1975">Efron and Morris (<a href="#ref-Efron1975" role="doc-biblioref">1975</a>)</span> it is proved that <span class="math display">
R(\mu; \hat{\mu}_\text{shrink}) \le p - \frac{(p-3)^2}{p - 3 + ||\mu - \bar{\mu}||_2^2},
</span> with <span class="math inline">\bar{\mu} = (1/p)\sum_{j=1}^p\mu_j</span>, again <span class="blue">dominating</span> the maximum likelihood <span class="math inline">\hat{\mu}</span>.</p></li>
</ul>
</section>
<section id="james-stein-estimator-v" class="level2">
<h2 class="anchored" data-anchor-id="james-stein-estimator-v">James-Stein estimator V</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The James-Stein estimator is an empirical <span class="blue">Bayes estimator</span> in disguise. Let <span class="math inline">\bm{Y} \sim \text{N}_p(\mu, I_p)</span> and consider the <span class="orange">prior</span> <span class="math inline">\bm{\mu} \sim N_p(m, \tau^2 I_p)</span>. Then the <span class="orange">posterior mean</span> is <span class="math display">
\hat{\mu}_\text{Bayes} = m + \left(1 - \frac{1}{1 + \tau^2}\right)(\bm{Y} - m).
</span> James-Stein is an <span class="blue">empirical Bayes</span> approach: the quantity <span class="math inline">1/(1 + \tau^2)</span> is <span class="orange">estimated</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>In particular, under the Bayesian model <span class="math display">
||\bm{Y} - m||_2^2 = \sum_{j=1}^p(Y_j - m)^2 \sim (1 + \tau^2)\chi^2_p
</span> and therefore <span class="math inline">(p - 2)/(||\bm{Y} - m||_2^2)</span> is an an <span class="orange">unbiased estimate</span> for <span class="math inline">1 / (1 + \tau^2)</span>. In fact: <span class="math display">
\mathbb{E}\left(\frac{p - 2}{||\bm{Y} - m||_2^2}\right) = \frac{1}{1 + \tau^2}.
</span></p></li>
<li><p>Alternative estimates for <span class="math inline">1/(1 + \tau^2)</span> leads to <span class="blue">refined</span> James-Stein estimators.</p></li>
</ul>
</section>
<section id="efron-and-morris-1975" class="level2">
<h2 class="anchored" data-anchor-id="efron-and-morris-1975">Efron and Morris (1975)</h2>
<div class="columns">
<div class="column" style="width:35%;">
<p><img src="img/efron.png" class="img-fluid"></p>
</div><div class="column" style="width:65%;">
<ul>
<li><p><span class="citation" data-cites="Efron1975">Efron and Morris (<a href="#ref-Efron1975" role="doc-biblioref">1975</a>, JASA)</span> is a <span class="blue">classical paper</span> on the <span class="orange">practical relevance</span> of James-Stein’s estimator.</p></li>
<li><p>This approach was used in <span class="orange">sports analytics</span> to predict the batting averages of 18 major league players in 1970.</p></li>
<li><p>As expected, <span class="blue">shrinkage estimators</span> significantly improve upon the maximum likelihood estimator.</p></li>
<li><p>Stein’s estimator was <span class="math inline">3.50</span> times <span class="orange">more efficient</span> than the MLE in this case.</p></li>
<li><p>It also showcases a useful practical demonstration of <span class="orange">variance-stabilizing</span> transformations:</p>
<ul>
<li>The original data are proportions, i.e.&nbsp;arguably not Gaussians.</li>
<li>After a suitable transformations they can be approximately regarded as normal.</li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="efron-and-morris-1975-1" class="level2">
<h2 class="anchored" data-anchor-id="efron-and-morris-1975-1">Efron and Morris (1975)</h2>
<div class="smaller">
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 18%">
<col style="width: 16%">
<col style="width: 21%">
<col style="width: 20%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: right;">At Bats <span class="math inline">n_i</span></th>
<th style="text-align: right;">Hits <span class="math inline">x_i</span></th>
<th style="text-align: right;">Mean <span class="math inline">\hat{p_i}</span></th>
<th style="text-align: right;">James-Stein</th>
<th style="text-align: right;">Rem. mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Clemente</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">0.400</td>
<td style="text-align: right;">0.290</td>
<td style="text-align: right;">0.346</td>
</tr>
<tr class="even">
<td style="text-align: left;">Robinson</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">0.378</td>
<td style="text-align: right;">0.286</td>
<td style="text-align: right;">0.298</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Howard</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">0.356</td>
<td style="text-align: right;">0.282</td>
<td style="text-align: right;">0.276</td>
</tr>
<tr class="even">
<td style="text-align: left;">Johnstone</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">0.333</td>
<td style="text-align: right;">0.277</td>
<td style="text-align: right;">0.222</td>
</tr>
<tr class="odd">
<td style="text-align: left;">…</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Williams</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">0.222</td>
<td style="text-align: right;">0.254</td>
<td style="text-align: right;">0.330</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Campaneris</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">0.200</td>
<td style="text-align: right;">0.249</td>
<td style="text-align: right;">0.285</td>
</tr>
<tr class="even">
<td style="text-align: left;">Munson</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">0.178</td>
<td style="text-align: right;">0.244</td>
<td style="text-align: right;">0.316</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Alvis</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">0.156</td>
<td style="text-align: right;">0.239</td>
<td style="text-align: right;">0.200</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p>The batting averages <span class="math inline">\hat{p}_i</span> out of <span class="math inline">n_i</span> trials for <span class="math inline">n = 18</span> players have been <span class="orange">transformed</span> via <span class="math inline">y_i = \sqrt{n_i} \arcsin(2 p_i - 1)</span>. The <span class="blue">James-Stein estimator</span> is applied to <span class="math inline">y_i</span> and then transformed back.</p></li>
<li><p>The <span class="orange">shrinkage effect</span> is evident and provides a better estimate of the <span class="blue">remaining batting average</span> for the season.</p></li>
</ul>
</section>
<section id="james-stein-estimator-vi" class="level2">
<h2 class="anchored" data-anchor-id="james-stein-estimator-vi">James-Stein estimator VI</h2>
<ul>
<li><p>James-Stein estimators were initially seen with suspicion:</p>
<ul>
<li>How come that deliberately introducing <span class="orange">bias</span> improves the estimates?<br>
</li>
<li>How come that <span class="orange">learning from the experience of other</span> points can modify and even improve the individual estimates?</li>
</ul></li>
<li><p>These ideas are nowadays well established:</p>
<ul>
<li>Modern <span class="blue">shrinkage estimators</span> such as <span class="orange">ridge regression</span> and <span class="orange">lasso</span> are widely used.<br>
</li>
<li>The notion of <span class="blue">borrowing of information</span> is at the heart of <span class="blue">random effects models</span>.</li>
</ul></li>
<li><p>The James-Stein theorem rigorously confirms the theoretical relevance of <span class="blue">indirect information</span>. Remarkably, this is based on <span class="orange">frequentist criteria</span> rather than Bayesian ones.</p></li>
<li><p>A <span class="blue">simple proof</span> of the James-Stein theorem can be found in <span class="citation" data-cites="Efron2010">Efron (<a href="#ref-Efron2010" role="doc-biblioref">2010</a>)</span>; see also <span class="citation" data-cites="Efron2016">Efron and Hastie (<a href="#ref-Efron2016" role="doc-biblioref">2016</a>)</span> for a modern and accessible perspective or <a href="https://www.statslab.cam.ac.uk/~rjs57/SteinParadox.pdf">this divulgative article</a>.</p></li>
</ul>
</section>
<section id="criticism-to-the-risk-function-approach" class="level2">
<h2 class="anchored" data-anchor-id="criticism-to-the-risk-function-approach">Criticism to the risk function approach</h2>
<ul>
<li>Some authors, such as <span class="citation" data-cites="Robert1994">Robert (<a href="#ref-Robert1994" role="doc-biblioref">1994</a>)</span>, have <span class="orange">criticized</span> the risk function approach for comparing estimators. The main arguments are the following:</li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li><p>The frequentist paradigm evaluates estimators based on their <span class="blue">long-run performance</span>, without accounting for the given observations <span class="math inline">\bm{Y}</span>. A client may wish for <span class="blue">optimal results</span> for the observed data, and <span class="orange">not someone else’s data</span>.</p></li>
<li><p>The risk function approach implicitly assumes the <span class="blue">repeatability</span> of the <span class="blue">experiment</span>, which has sparked controversy, especially among Bayesian statisticians.</p></li>
<li><p>For instance, if new observations come to the statistician, she should make use of them, potentially modifying the way the experiment is conducted, as in medical trials.</p></li>
<li><p>The risk function depends on <span class="math inline">\theta</span>, preventing a <span class="blue">total ordering</span> of the estimators. Comparisons are difficult unless a <span class="blue">uniformly optimal</span> procedure exists, which is <span class="orange">rare</span>.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="integrated-risk" class="level2">
<h2 class="anchored" data-anchor-id="integrated-risk">Integrated risk</h2>
<ul>
<li>Let us begin by finding a criterion that induces <span class="orange">total ordering</span> among estimators. A potential solution is taking the <span class="blue">average</span> risk function over values of <span class="math inline">\theta</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">R(\theta, \hat{\theta}) = \mathbb{E}_\theta\{\mathscr{L}(\theta,\hat{\theta})\}</span> be the <span class="orange">risk function</span>, and let <span class="math inline">\pi(\mathrm{d}\theta)</span> be a probability measure <span class="orange">weighting</span> the relevance of each <span class="math inline">\theta</span>. Then<br>
<span class="math display">
r(\pi, \hat{\theta}) = \int_\Theta R(\theta; \hat{\theta}) \pi(\mathrm{d}\theta),
</span> is called <span class="blue">integrated risk</span> or <span class="orange">Bayes risk</span>.</p>
</div>
</div>
</div>
<ul>
<li>Thus, an estimator <span class="math inline">\hat{\theta}</span> is <span class="blue">preferable</span> over another <span class="math inline">\tilde{\theta}</span> if <span class="math inline">r(\pi, \hat{\theta}) &lt; r(\pi, \tilde{\theta})</span>. Moreover, if <span class="math inline">\hat{\theta}</span> <span class="orange">dominates</span> <span class="math inline">\tilde{\theta}</span>, that is if <span class="math inline">\tilde{\theta}</span> is <span class="orange">inadmissible</span>, then <span class="math inline">r(\pi, \hat{\theta}) &lt; r(\pi, \tilde{\theta})</span> for any choice of weights.</li>
</ul>
</section>
<section id="example-integrated-risk-of-binomial-estimators" class="level2">
<h2 class="anchored" data-anchor-id="example-integrated-risk-of-binomial-estimators">Example: integrated risk of binomial estimators 📖</h2>
<ul>
<li><p>Let us consider the estimation of the <span class="blue">probability</span> <span class="math inline">p</span> from a <span class="binomial">binomial experiment</span> using the maximum likelihood <span class="math inline">\hat{p} = n_1/n</span> and the Bayesian <span class="math inline">\hat{p}_\text{minimax} = (n_1 + 0.5\sqrt{n})/(n + \sqrt{n})</span> estimators.</p></li>
<li><p>We previously computed the <span class="orange">mean squared error</span> for both estimators. Let us now compute the integrated risk, assuming <span class="blue">uniform weights</span> <span class="math inline">\pi(\mathrm{d}p) = \mathrm{d}p</span>, i.e., a <span class="orange">simple average</span> of the MSE.</p></li>
<li><p>The <span class="blue">integrated risk</span> of the <span class="orange">maximum likelihood</span> estimator is<br>
<span class="math display">
r(\pi, \hat{p}) = \int_0^1 \mathbb{E}_p\{(\hat{p} - p)^2\}\mathrm{d}p = \frac{1}{n}\int_0^1 p(1-p)\mathrm{d}p= \frac{1}{6n}.
</span></p></li>
<li><p>The <span class="blue">integrated risk</span> of the <span class="orange">Bayes estimator</span> coincides with the risk function, the latter being <span class="orange">constant</span> over <span class="math inline">p</span>:<br>
<span class="math display">
r(\pi, \hat{p}_\text{minimax}) = \int_0^1 \mathbb{E}_p\{(\hat{p}_\text{minimax} - p)^2\}\mathrm{d}p = \int_0^1\frac{n}{4 ( n + \sqrt{n})^2}\mathrm{d}p= \frac{n}{4 ( n + \sqrt{n})^2}.
</span></p></li>
<li><p>Depending on the sample size <span class="math inline">n</span>, one estimator may be preferable over the other.</p></li>
</ul>
</section>
<section id="example-integrated-risk-of-binomial-estimators-1" class="level2">
<h2 class="anchored" data-anchor-id="example-integrated-risk-of-binomial-estimators-1">Example: integrated risk of binomial estimators</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1350"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>According to the integrated risk and using constant weights <span class="math inline">\pi(\mathrm{d}p) = \mathrm{d}p</span> one should <span class="orange">prefer</span> <span class="math inline">\hat{p}</span> over <span class="math inline">\hat{p}_\text{minimax}</span> when <span class="math inline">n \ge 20</span> and viceversa.</li>
</ul>
</section>
<section id="bayesian-estimators-minimize-the-integrated-risk" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-estimators-minimize-the-integrated-risk">Bayesian estimators minimize the integrated risk 📖</h2>
<ul>
<li><p>The integrated risk is a sensible criterion for comparing estimators, provided a suitable set of weights <span class="math inline">\pi</span> is selected. Hence, we may wish to find its <span class="orange">minimizer</span>.</p></li>
<li><p>There is a surprisingly simple and elegant answer to this apparently difficult question.</p></li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span>, Theorem 1.1 in Chap. 4)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\pi(\mathrm{d}\theta)</span> be the <span class="orange">prior distribution</span> for <span class="math inline">\theta</span>. The <span class="orange">minimizer</span> of the <span class="blue">posterior expected loss</span>, if a unique solution exists, is called <span class="blue">Bayes estimator</span>. Moreover: <span class="math display">
\hat{\theta}_\text{Bayes}(\bm{Y}) = \arg\min_{\tilde{\theta} \in \Theta} \int_\Theta \mathscr{L}(\theta, \tilde{\theta}(\bm{Y}))\pi(\mathrm{d}\theta \mid \bm{Y}) = \arg\min_{\tilde{\theta} \in \Theta} r(\pi, \tilde{\theta}(\bm{Y})),
</span> which means <span class="math inline">\hat{\theta}_\text{Bayes}</span> coincides with minimizer of the <span class="orange">integrated risk</span>, provided <span class="math inline">r(\pi, \hat{\theta}_\text{Bayes}) &lt; \infty</span>.</p>
</div>
</div>
<ul>
<li>This fundamental theorem provides a <span class="blue">decision-theoretic justification</span> to Bayesian estimators as well as a practical recipe for finding them.</li>
</ul>
</section>
<section id="decision-theoretic-justification-of-the-posterior-mean" class="level2">
<h2 class="anchored" data-anchor-id="decision-theoretic-justification-of-the-posterior-mean">Decision-theoretic justification of the posterior mean 📖</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Corollary (<span class="citation" data-cites="Robert1994">Robert (<a href="#ref-Robert1994" role="doc-biblioref">1994</a>)</span>, Proposition 2.5.1)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\Theta \subseteq \mathbb{R}^p</span>, then the <span class="blue">Bayes estimator</span> associated with prior distribution <span class="math inline">\pi</span> and <span class="blue">quadratic loss</span> <span class="math inline">\mathscr{L}(\theta, \hat{\theta}) = ||\hat{\theta} -\theta||_2^2</span> is the <span class="orange">posterior mean</span>: <span class="math display">
\hat{\theta}_\text{Bayes}(\bm{Y}) = \arg\min_{\tilde{\theta} \in \Theta} \int_\Theta ||\tilde{\theta}(\bm{Y}) - \theta||_2^2\pi(\mathrm{d}\theta \mid \bm{Y}) = \mathbb{E}(\theta \mid \bm{Y}).
</span> If the posterior mean exists, the Bayes estimator is <span class="blue">unique</span>.</p>
</div>
</div>
<ul>
<li><p>Hence, the <span class="orange">posterior mean</span> is <span class="blue">optimal</span> in the sense that minimizes the posterior expected loss and therefore also the integrated risk.</p></li>
<li><p>This theorem extends to various loss functions. For instance if <span class="math inline">\mathscr{L}(\theta, \hat{\theta}) = ||\hat{\theta} - \theta||_1</span> then the <span class="blue">Bayes estimator</span> is the <span class="orange">posterior median</span>.</p></li>
</ul>
</section>
<section id="example-integrated-risk-of-binomial-estimators-2" class="level2">
<h2 class="anchored" data-anchor-id="example-integrated-risk-of-binomial-estimators-2">Example: integrated risk of binomial estimators 📖</h2>
<ul>
<li><p>We previously considered the estimators <span class="math inline">\hat{p} = n_1/n</span> and <span class="math inline">\hat{p}_\text{minimax} = (n_1 + 0.5\sqrt{n})/(n + \sqrt{n})</span>. Assuming <span class="blue">uniform weights</span>, neither <span class="math inline">\hat{p}</span> nor <span class="math inline">\hat{p}_\text{minimax}</span> minimizes the integrated risk.</p></li>
<li><p>In fact, theoretical results indicate that the <span class="blue">unique minimizer</span> is the <span class="orange">posterior mean</span> of <span class="math inline">p</span> for a binomial model under a <span class="orange">uniform prior</span> <span class="math inline">\pi(\mathrm{d}p) = \mathrm{d}p</span>. The optimal estimator is: <span class="math display">
p \mid \bm{Y} \sim \text{Beta}(n_1 + 1, n - n_1 + 1) \implies \hat{p}_\text{Bayes} = \mathbb{E}(p \mid \bm{Y}) = \frac{n_1 + 1}{n + 2}.
</span></p></li>
<li><p>After some simple but tedious calculations, we obtain the associated <span class="orange">mean squared error</span>:<br>
<span class="math display">
R(p; \hat{p}_\text{Bayes}) = \left(\frac{2}{n+2}\right)^2(1/2 - p)^2 + \left(\frac{n}{n+2}\right)^2 \frac{p(1-p)}{n}.
</span></p></li>
<li><p>Integrating with respect to the <span class="blue">uniform prior</span> distribution, we obtain the <span class="orange">integrated risk</span>: <span class="math display">
r(\pi, \hat{p}_\text{Bayes}) = \int_0^1 R(p; \hat{p}_\text{Bayes})\mathrm{d}p = \frac{1}{6 (n+2)},
</span> which is indeed <span class="orange">smaller</span> than <span class="math inline">r(\pi, \hat{p})</span> and <span class="math inline">r(\pi, \hat{p}_\text{minimax})</span>.</p></li>
</ul>
</section>
<section id="example-integrated-risk-of-binomial-estimators-3" class="level2">
<h2 class="anchored" data-anchor-id="example-integrated-risk-of-binomial-estimators-3">Example: integrated risk of binomial estimators</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1350"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="admissibility-of-bayesian-estimators" class="level2">
<h2 class="anchored" data-anchor-id="admissibility-of-bayesian-estimators">Admissibility of Bayesian estimators 📖</h2>
<ul>
<li>The next theorem provides a strong <span class="orange">frequentist justification</span> for <span class="blue">Bayesian estimators</span>.</li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span>, Theorem 2.4 in Chap. 5)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Any unique Bayesian estimator is <span class="blue">admissible</span>.</p>
</div>
</div>
<ul>
<li><p>The uniqueness assumption is a technical condition often satisfied in practice. Under <span class="blue">squared loss</span>, or any other strictly convex loss, this holds automatically, provided the <span class="orange">estimator exists</span>.</p></li>
<li><p>If the loss is strictly convex, such as the squared error loss, and the integrated risk is finite, this theorem remains valid even when using <span class="orange">improper priors</span> (<span class="citation" data-cites="Robert1994">Robert (<a href="#ref-Robert1994" role="doc-biblioref">1994</a>)</span>, Proposition 2.4.25).</p></li>
<li><p><span class="blue">Admissibility</span> is a <span class="orange">minimum requirement</span>: even constant estimators are admissible.</p></li>
<li><p>Nonetheless, the risk function approach dictates that inadmissible estimators should be discarded. This is non-trivial in practice, as evidenced by the James-Stein saga.</p></li>
</ul>
</section>
<section id="minimax-estimators" class="level2">
<h2 class="anchored" data-anchor-id="minimax-estimators">Minimax estimators</h2>
<ul>
<li><p>The <span class="blue">minimax</span> criterion is an alternative to the integrated risk for <span class="orange">discriminating</span> among (admissible) <span class="blue">estimators</span>. It comes from game theory, where two adversaries are competing.</p></li>
<li><p>Instead of considering the “average” risk over <span class="math inline">\theta</span> (integrated risk), the minimax criterion evaluates the <span class="blue">risk function</span> of estimators in the <span class="orange">worst-case scenario</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>An estimator <span class="math inline">\hat{\theta}_\text{minimax}</span> which <span class="blue">minimizes</span> the <span class="orange">maximum</span> risk, that is, which satisfies <span class="math display">
\sup_{\theta \in \Theta} R(\theta; \hat{\theta}_\text{minimax}) =\inf_{\hat{\theta}} \sup_{\theta \in \Theta} R(\theta; \hat{\theta}),
</span> is called <span class="orange">minimax estimator</span>. The quantity <span class="math inline">\sup_\theta R(\theta; \hat{\theta}_\text{minimax})</span> is called <span class="blue">minimax risk</span>.</p>
</div>
</div>
</div>
<ul>
<li>In general, <span class="orange">finding minimax</span> estimators is <span class="orange">difficult</span>. Moreover, the resulting estimators are not necessarily very appealing in practice.</li>
</ul>
</section>
<section id="example-mse-of-binomial-estimators-minimax" class="level2">
<h2 class="anchored" data-anchor-id="example-mse-of-binomial-estimators-minimax">Example: MSE of binomial estimators (minimax)</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1350"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p>We will show that <span class="math inline">\hat{p}_\text{minimax} = (n_1 + 0.5\sqrt{n})/(n + \sqrt{n})</span> is indeed the <span class="orange">minimax estimator</span>. However, this is a very <span class="blue">conservative</span> choice.</p></li>
<li><p>One could argue that the maximum likelihood <span class="math inline">\hat{p} = n_1/n</span> is the preferred choice in practice, especially when <span class="math inline">n</span> is large enough, even though it has slightly higher risk when <span class="math inline">p \approx 1/2</span>.</p></li>
</ul>
<!-- ## Minimax and Bayesian estimators -->
<!-- :::callout-note -->
<!-- #### Least favorable prior -->
<!-- Let $\hat{\theta}_\text{Bayes}$ and $\tilde{\theta}_\text{Bayes}$ be [optimal Bayes estimators]{.orange} for a given model under [prior]{.orange} distributions $\pi(\mathrm{d}\theta)$ and $\tilde{\pi}(\mathrm{d}\theta)$, respectively. We will say  $\pi(\mathrm{d}\theta)$ is [least favorable]{.blue} if its itegrated risk -->
<!-- $$ -->
<!-- r(\pi, \hat{\theta}_\text{Bayes}) = \int_\Theta R(\theta; \hat{\theta}_\text{Bayes})\pi(\mathrm{d}\theta) \ge \int_\Theta R(\theta; \tilde{\theta}_\text{Bayes})\tilde{\pi}(\mathrm{d}\theta) = r(\tilde{\pi}, \tilde{\theta}_\text{Bayes}). -->
<!-- $$ -->
<!-- for any other prior distribution $\tilde{\pi}(\mathrm{d}\theta)$.  -->
<!-- ::: -->
<!-- ::: callout-warning -->
<!-- #### Lemma (@Robert1994, Lemma 2.4.10) -->
<!-- Let $\pi(\mathrm{d}\theta)$ be the [least favorable]{.blue}  prior. Then the integrated risk is smaller than the minimax risk: -->
<!-- $$ -->
<!-- r(\pi, \hat{\theta}_\text{Bayes}) \le \inf_{\hat{\theta}} \sup_{\theta \in \Theta} R(\theta; \hat{\theta}) \le \sup_{\theta \in \Theta} R(\theta; \hat{\theta}_\text{Bayes}). -->
<!-- $$ -->
<!-- Clearly, the same hold for any other prior choice $\tilde{\pi}$.  -->
<!-- ::: -->
</section>
<section id="minimax-and-bayesian-estimators" class="level2">
<h2 class="anchored" data-anchor-id="minimax-and-bayesian-estimators">Minimax and Bayesian estimators 📖</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span>, Theorem 1.4, Chap. 5)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\pi(\mathrm{d}\theta)</span> be a prior distribution and <span class="math inline">\hat{\theta}_\text{Bayes}</span> be the <span class="blue">unique Bayes</span> estimator. If it holds <span class="math display">
r(\pi, \hat{\theta}_\text{Bayes}) = \sup_\theta R(\theta; \hat{\theta}_\text{Bayes}),
</span> then:</p>
<ul>
<li><p>the Bayes estimator <span class="math inline">\hat{\theta}_\text{Bayes}</span> is also the <span class="orange">unique minimax</span> estimator.</p></li>
<li><p>the prior <span class="math inline">\pi(\mathrm{d}\theta)</span> is <span class="blue">least favorable</span>, meaning that <span class="math inline">r(\pi, \hat{\theta}_\text{Bayes})\ge r(\tilde{\pi}, \tilde{\theta}_\text{Bayes})</span> for any other prior <span class="math inline">\tilde{\pi}(\mathrm{d}\theta)</span> and Bayes estimator <span class="math inline">\tilde{\theta}_\text{Bayes}</span>.</p></li>
</ul>
</div>
</div>
<ul>
<li><p><span class="orange">Remark</span>. The above condition states the average of <span class="math inline">R(\theta; \hat{\theta}_\text{Bayes})</span> is equal to its maximum. This will be the case when the risk function <span class="orange">constant</span> over <span class="math inline">\theta</span>.</p></li>
<li><p>More generally, if an <span class="blue">admissible estimator</span> has <span class="orange">constant risk</span>, is the unique <span class="orange">minimax</span> estimator (<span class="citation" data-cites="Robert1994">Robert (<a href="#ref-Robert1994" role="doc-biblioref">1994</a>)</span>, Proposition 2.4.21).</p></li>
</ul>
</section>
</section>
<section id="unbiasedness" class="level1 page-columns page-full">
<h1>Unbiasedness</h1>
<section id="unbiased-estimators" class="level2">
<h2 class="anchored" data-anchor-id="unbiased-estimators">Unbiased estimators</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>An estimator <span class="math inline">\hat{\theta}</span> is <span class="blue">unbiased</span> for <span class="math inline">\theta</span> if <span class="math inline">\mathbb{E}_\theta(\hat{\theta}) = \theta</span>, that is, if <span class="math inline">\text{bias}_\theta(\hat{\theta}) = \mathbb{E}_\theta(\hat{\theta} - \theta) = 0</span> for all <span class="math inline">\theta \in \Theta</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>We often teach that <span class="orange">unbiasedness</span> is a natural and appealing property of an estimator. If <span class="math inline">\Theta \subseteq \mathbb{R}</span> and under <span class="blue">squared error loss</span>, unbiasedness implies that<br>
<span class="math display">R(\theta;\hat{\theta}) = \mathbb{E}_\theta\{(\hat{\theta} - \theta)^2\} = \text{var}_\theta(\hat{\theta}).</span></p></li>
<li><p>There are two main reasons for emphasizing unbiasedness:</p>
<ul>
<li>It is often possible to find the uniformly “best” unbiased estimator, e.g., the one with the lowest variance (<span class="orange">UMVU estimator</span>).</li>
<li>For an estimator to be <span class="grey">consistent</span>, it must be at least <span class="blue">asymptotically unbiased</span>.</li>
</ul></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Unbiasedness is not a negative property per se. However, one may overlook better estimators by focusing too narrowly on this special class. Indeed, the <span class="blue">UMVUE</span> can even be <span class="orange">inadmissible</span>.</p>
</div>
</div>
</div>
</section>
<section id="nonexistence-of-unbiased-estimators" class="level2">
<h2 class="anchored" data-anchor-id="nonexistence-of-unbiased-estimators">Nonexistence of unbiased estimators</h2>
<ul>
<li><p>Certain quantities <span class="orange">do not admit unbiased</span> estimators, even though they can be accurately estimated using <span class="blue">slightly biased</span> estimators.</p></li>
<li><p>Let <span class="math inline">Y \sim \text{Bin}(n, p)</span> and suppose we wish to find an estimator <span class="math inline">\hat{\psi}(Y)</span> for the reparametrization <span class="math inline">\psi = 1 / p</span>. Then unbiasedness of an estimator <span class="math inline">\hat{\psi}(Y)</span> would require <span class="math display">
\sum_{k=0}^n \hat{\psi}(k)\binom{n}{k}p^k(1-p)^{n -k} = \frac{1}{p}, \qquad p \in (0, 1).
</span> Such an estimator <span class="orange">does not exist</span>!</p></li>
<li><p>Indeed, the left hand side of this equation is a <span class="blue">polynomial</span> <span class="math inline">p</span> with degree at most <span class="math inline">n</span>. However, <span class="math inline">1/p</span> cannot be written as a polynomial.</p></li>
<li><p>Nonetheless, the <span class="blue">slightly biased</span> estimator <span class="math inline">\hat{\psi} = n / n_1</span> will be close to <span class="math inline">1/p</span> with high probability as <span class="math inline">n</span> increases.</p></li>
</ul>
</section>
<section id="bayesian-estimators-and-unbiasedness" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-estimators-and-unbiasedness">Bayesian estimators and unbiasedness 📖</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span>, Theorem 2.3, Chap. 4)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\Theta \subseteq \mathbb{R}^p</span> and <span class="math inline">\hat{\theta}_\text{Bayes}(\bm{Y})</span> be the unique Bayes estimator with prior <span class="math inline">\pi</span> under a <span class="orange">squared error loss</span>. If <span class="math inline">\hat{\theta}_\text{Bayes}</span> is <span class="blue">unbiased</span> then its integrated risk is <span class="math display">
r(\pi, \hat{\theta}_\text{Bayes}) = 0.
</span></p>
</div>
</div>
<ul>
<li><p>The above theorem is a formal way of saying that, apart from trivial cases, posterior means are <span class="orange">never unbiased</span> estimators.</p></li>
<li><p>However, the <span class="orange">bias</span> comes with a <span class="blue">reduced variance</span>, therefore the trade-off could be favorable. This is guaranteed to occur because Bayesian estimators are admissible.</p></li>
<li><p>Moreover, under mild regularity conditions, Bayesian estimators are <span class="blue">asymptotically unbiased</span>.</p></li>
</ul>
</section>
<section id="example-poisson-unbiased-estimation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="example-poisson-unbiased-estimation">Example: Poisson unbiased estimation</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be i.i.d. <span class="math inline">\text{Poisson}(\lambda)</span>, and let <span class="math inline">\bar{Y} = n^{-1} \sum_{i=1}^n Y_i</span> and <span class="math inline">S^2 = (n-1)^{-1} \sum_{i=1}^n (Y_i - \bar{Y})^2</span> be the <span class="blue">sample mean</span> and <span class="orange">sample variance</span>, respectively.</p></li>
<li><p>It can be shown<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> that both estimators are <span class="orange">unbiased</span>, meaning <span class="math display">
\mathbb{E}(\bar{Y}) = \mathbb{E}(S^2) = \lambda, \quad \text{for all } \lambda.
</span></p></li>
<li><p>To determine which estimator, <span class="math inline">\bar{Y}</span> or <span class="math inline">S^2</span>, is preferable, we should <span class="blue">compare their variances</span>. It is also well known that <span class="math inline">\text{var}_\lambda(\bar{Y}) = \lambda / n</span>, whereas computing <span class="math inline">\text{var}_\lambda(S^2)</span> can be <span class="orange">lengthy</span>.</p></li>
<li><p>It holds that <span class="math inline">\text{var}_\lambda(\bar{Y}) &lt; \text{var}_\lambda(S^2)</span> for all <span class="math inline">\lambda</span>. This implies that <span class="math inline">S^2</span> is <span class="orange">inadmissible</span>.</p></li>
<li><p>However, we can construct <span class="blue">infinitely</span> many <span class="blue">unbiased estimators</span> of <span class="math inline">\lambda</span>: <span class="math display">
\hat{\lambda}_a = a \bar{Y} + (1 - a) S^2, \quad 0 &lt; a &lt; 1.
</span> Is there a value of <span class="math inline">a</span> such that <span class="math inline">\text{var}_\lambda(\hat{\lambda}_a) \leq \text{var}_\lambda(\bar{Y})</span>? What about other unbiased estimators?</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;These are basic and well-known results: see Theorem 5.2.6 in <span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>.</p></div></div></section>
<section id="umvu-estimators" class="level2">
<h2 class="anchored" data-anchor-id="umvu-estimators">UMVU estimators</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In this subsection on <span class="orange">unbiasedness</span>, we will often assume that <span class="math inline">\Theta \subseteq \mathbb{R}</span>. All the results presented here extend to the <span class="blue">vector case</span>, though at the cost of heavier notation.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">\Theta \subseteq \mathbb{R}</span>. An estimator <span class="math inline">\hat{\theta}</span> is a <span class="blue">best unbiased estimator</span> of <span class="math inline">\theta</span> if it satisfies <span class="math inline">\mathbb{E}_\theta(\hat{\theta}) = \theta</span> for all <span class="math inline">\theta</span> (unbiasdness) and, for any <span class="orange">other unbiased</span> estimator <span class="math inline">\tilde{\theta}</span>, we have <span class="math display">
\text{var}_\theta(\hat{\theta}) \le \text{var}_\theta(\tilde{\theta}), \qquad \text {for all } \theta \in \Theta.
</span> The estimator <span class="math inline">\hat{\theta}</span> is also called <span class="orange">uniform minimum variance unbiased estimator</span> (UMVUE) of <span class="math inline">\theta</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>The UMVUE <span class="orange">does not necessarily exist</span>. If it does, <span class="blue">finding it</span> is not easy. And even if a unique UMVUE exists, it could still be <span class="orange">inadmissible</span>–recall the James-Stein saga.</p></li>
<li><p>The success of UMVUE estimators is tied to two illuminating and elegant theorems: <span class="blue">Cramér-Rao</span> and <span class="orange">Rao-Blackwell</span>, which connect likelihood theory, sufficiency, and unbiasedness.</p></li>
</ul>
</section>
<section id="cramér-rao-inequality" class="level2">
<h2 class="anchored" data-anchor-id="cramér-rao-inequality">Cramér-Rao inequality 📖</h2>
<ul>
<li>The Cramér-Rao theorem establishes a <span class="blue">lower bound</span> for the variance of an estimator. Thus, if the variance of an unbiased estimator <span class="math inline">\hat{\theta}</span> attains the lower bound for all <span class="math inline">\theta</span>, then <span class="math inline">\hat{\theta}</span> is <span class="orange">UMVUE</span>.</li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Cramér-Rao, Theorem 7.3.9 in <span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">Y_1,\dots,Y_n</span> be a sample from a joint probability measure <span class="math inline">f(\bm{y} \mid \theta)\nu(\mathrm{d}\bm{y})</span> and let <span class="math inline">\Theta \subseteq \mathbb{R}</span>. Moreover, let <span class="math inline">\hat{\theta}(\bm{Y})</span> be an estimator of <span class="math inline">\theta</span> satisfying <span class="math display">
1 + b^*(\theta) := \frac{\partial}{\partial \theta}\int \hat{\theta}(\bm{y})f(\bm{y} \mid \theta)\nu(\mathrm{d}\bm{y}) = \int \frac{\partial}{\partial \theta} \hat{\theta}(\bm{y})f(\bm{y} \mid \theta)\nu(\mathrm{d}\bm{y}),
</span> and with finite variance <span class="math inline">\text{var}_\theta(\hat{\theta}(\bm{Y})) &lt; \infty</span>. Then <span class="math display">
\text{var}_\theta(\hat{\theta}(\bm{Y})) \ge \frac{[1 + b^*(\theta)]^2}{\mathbb{E}_\theta(\ell^*(\theta)^2)}.
</span> Moreover, if <span class="math inline">\hat{\theta}</span> is an <span class="orange">unbiased</span> estimator for <span class="math inline">\theta</span>, then <span class="math inline">b^*(\theta) = 0</span> and <span class="math inline">\text{var}(\hat{\theta}(\bm{Y})) \ge 1/\mathbb{E}_\theta(l^*(\theta)^2)</span> .</p>
</div>
</div>
</section>
<section id="cramér-rao-considerations" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="cramér-rao-considerations">Cramér-Rao: considerations</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li><p>The interchange of the derivative under the integral sign is an important <span class="orange">condition</span>, not merely a technical artifact of the proof.</p></li>
<li><p>For example, if the <span class="blue">sample space</span> of i.i.d. random variables <span class="math inline">Y_i</span> <span class="blue">depends on <span class="math inline">\theta</span></span>, such condition is violated, and the Cramér-Rao lower bound may not hold.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p></li>
</ul>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;See Example 7.3.13 in <span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>, for a simple and illuminating example.</p></div></div><ul>
<li>The Cramér-Rao inequality is sometimes called <span class="orange">information inequality</span>. In fact <span class="math inline">I(\theta)</span> defined as: <span class="math display">
I(\theta) := \mathbb{E}_\theta(\ell^*(\theta)^2),
</span> is called <span class="blue">Fisher information</span> or information number. This reflects the fact that as more information become available, the bound on the variance gets smaller.</li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>If <span class="math inline">W(\bm{Y})</span> is an unbiased estimator of a <span class="blue">transformation</span> <span class="math inline">g(\theta)</span>, then the Cramér-Rao theorem holds as stated, but the term <span class="math inline">\frac{\partial}{\partial \theta}\mathbb{E}_\theta(W(\bm{Y})) = 1 + b^*(\theta)</span> is not related to the “bias”.</p>
</div>
</div>
</div>
</section>
<section id="bartlett-identities-i" class="level2">
<h2 class="anchored" data-anchor-id="bartlett-identities-i">Bartlett identities I 📖</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
First Bartlett identity
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">Y_1,\dots,Y_n</span> be a sample from a joint probability measure <span class="math inline">f(\bm{y} \mid \theta)\nu(\mathrm{d}\bm{y})</span> and let <span class="math inline">\Theta \subseteq \mathbb{R}</span>. If we can interchange derivation and integration, namely <span class="math display">\frac{\partial}{\partial \theta}\int f(\bm{y} \mid \theta)\nu(\mathrm{d}\bm{y}) = \int \frac{\partial}{\partial \theta} f(\bm{y} \mid \theta)\nu(\mathrm{d}\bm{y}), \tag{C.1}
</span> then <span class="math display">
\mathbb{E}_\theta(\ell^*(\theta)) = 0,\qquad \text{implying}\qquad  I(\theta) = \mathbb{E}_\theta(\ell^*(\theta)^2) = \text{var}_\theta(\ell^*(\theta)).
</span></p>
</div>
</div>
<ul>
<li>Thus, the regularity condition of Cramér-Rao implies that the score function <span class="math inline">\ell^*(\theta)</span> is un <span class="blue">unbiased estimating equation</span>.</li>
</ul>
</section>
<section id="bartlett-identities-ii" class="level2">
<h2 class="anchored" data-anchor-id="bartlett-identities-ii">Bartlett identities II 📖</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Second Bartlett identity
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">Y_1,\dots,Y_n</span> be a sample from a joint probability measure <span class="math inline">f(\bm{y} \mid \theta)\nu(\mathrm{d}\bm{y})</span> and let <span class="math inline">\Theta \subseteq \mathbb{R}</span>. If we can interchange <span class="orange">twice</span> derivation and integration, namely <span class="math display">
\frac{\partial}{\partial \theta}\int f(\bm{y} \mid \theta)\nu(\mathrm{d}\bm{y}) = \int \frac{\partial}{\partial \theta} f(\bm{y} \mid \theta)\nu(\mathrm{d}\bm{y}), \tag{C.1}
</span> <span class="math display">
\frac{\partial^2}{\partial^2 \theta}\int f(\bm{y} \mid \theta)\nu(\mathrm{d}\bm{y}) = \int \frac{\partial^2}{\partial^2 \theta}f(\bm{y} \mid \theta)\nu(\mathrm{d}\bm{y}), \tag{C.2}
</span> then <span class="math display">
I(\theta) = \mathbb{E}_\theta(\ell^*(\theta)^2) =\text{var}_\theta(\ell^*(\theta))= \mathbb{E}_\theta\left(-\frac{\partial^2}{\partial^2 \theta}\ell(\theta)\right).
</span></p>
</div>
</div>
<ul>
<li>Both conditions are true in <span class="orange">regular exponential</span> families. This also clarifies that, in regular models, Fisher information relates to the <span class="blue">curvature</span> of the log-likelihood.</li>
</ul>
</section>
<section id="cramér-rao-iid-and-regular-case" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="cramér-rao-iid-and-regular-case">Cramér-Rao: iid and regular case</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Cramér-Rao, simplified)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">Y_1,\dots,Y_n</span> be an iid sample from <span class="math inline">f(y \mid \theta)\mathrm{d}y</span> satisfying conditions <span class="blue">C.1</span> and <span class="blue">C.2</span>, and let <span class="math inline">\Theta \subseteq \mathbb{R}</span>. Moreover, let <span class="math inline">\hat{\theta}(\bm{Y})</span> be an <span class="orange">unbiased</span> estimator of <span class="math inline">\theta</span> with finite variance <span class="math inline">\text{var}(\hat{\theta}(\bm{Y})) &lt; \infty</span>. Then <span class="math display">
\text{var}_\theta(\hat{\theta}(\bm{Y})) \ge \frac{1}{n i(\theta)}, \qquad i(\theta) = -\int \left[\frac{\partial^2}{\partial^2 \theta}\log{f(y \mid \theta)}\right]f(y\mid\theta)\mathrm{d}y.
</span></p>
</div>
</div>
<ul>
<li><p>The Fisher information is the <span class="blue">sum</span> of <span class="blue">individual contributions</span> <span class="math inline">I(\theta) = i(\theta) + \cdots + i(\theta) = n i(\theta)</span>.</p></li>
<li><p>It can be shown<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> that <span class="blue">attainment</span>, that is the equality <span class="math inline">\text{var}(\hat{\theta}(\bm{Y})) = 1/ni(\theta)</span>, occurs if and only if <span class="math inline">f(y \mid \theta)</span> is the density of an <span class="orange">exponential family</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;See Theorem 5.12, Chap.2, <span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span>.</p></div></div></section>
<section id="example-poisson-unbiased-estimation-1" class="level2">
<h2 class="anchored" data-anchor-id="example-poisson-unbiased-estimation-1">Example: Poisson unbiased estimation 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be i.i.d. <span class="math inline">\text{Poisson}(\lambda)</span>. The sample mean <span class="math inline">\bar{Y}</span> is <span class="orange">unbiased</span> for <span class="math inline">\lambda</span> and <span class="math inline">\text{var}_\lambda(\bar{Y}) = \lambda / n</span>.</p></li>
<li><p>We can use Cramér-Rao to show this estimator is a <span class="blue">UMVUE</span>. The regularity conditions are satisfied and therefore, after some calculus <span class="math display">
i(\lambda) = -\mathbb{E}_\lambda\left(\frac{\partial}{\partial \lambda^2}\log{f(y \mid \lambda)}\right) = \mathbb{E}_\lambda\left(\frac{Y}{\lambda^2}\right) = \frac{1}{\lambda}.
</span></p></li>
<li><p>Hence, <span class="orange">Cramér-Rao</span> theorem states that for any unbiased estimator <span class="math inline">\hat{\lambda}(\bm{Y})</span> <span class="math display">
\text{var}_\lambda(\hat{\lambda}) \ge \frac{\lambda}{n},
</span> implying that <span class="math inline">\bar{Y}</span> is a <span class="blue">UMVUE</span> because <span class="math inline">\text{var}_\lambda(\bar{Y}) = \lambda/n</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li><p>Cramér-Rao theorem does not imply that <span class="math inline">\bar{Y}</span> is the <span class="orange">unique</span> UMVUE.</p></li>
<li><p>However, this is guaranteed by Theorem 7.3.19 in <span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>: if a <span class="blue">UMVUE</span> exists, it is <span class="orange">unique</span>.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="cramér-rao-multiparameter-case" class="level2">
<h2 class="anchored" data-anchor-id="cramér-rao-multiparameter-case">Cramér-Rao, multiparameter case</h2>
<ul>
<li><p>The Cramér-Rao theorem naturally extends to the <span class="orange">vector case</span>, that is, when <span class="math inline">\Theta \subseteq \mathbb{R}^p</span>.</p></li>
<li><p>The <span class="orange">regularity conditions</span> <span class="blue">C.1</span> and <span class="blue">C.2</span> extend to the vector case, leading to the multiparameter <span class="blue">Bartlett identities</span>: <span class="math display">
\mathbb{E}_\theta(\ell^*(\theta)) = \bm{0}, \qquad I(\theta) = \mathbb{E}_\theta(\ell^*(\theta)\ell^*(\theta)^T) = \mathbb{E}_\theta\left(- \frac{\partial^2}{\partial \theta \partial \theta^T} \ell(\theta)\right),
</span> where <span class="math inline">I(\theta)</span> is called the <span class="blue">Fisher information matrix</span>, which is <span class="orange">positive definite</span>.</p></li>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be an iid sample from <span class="math inline">f(y \mid \theta)\mathrm{d}y</span> satisfying the above regularity conditions. Moreover, let <span class="math inline">\hat{\theta}(\bm{Y})</span> be an <span class="orange">unbiased</span> estimator of <span class="math inline">\theta</span> with a finite covariance matrix. Then, <span class="math display">
\text{var}_\theta(\hat{\theta}(\bm{Y})) \ge I(\theta)^{-1}, \quad I(\theta) = n i(\theta), \quad i(\theta) = - \int\frac{\partial^2}{\partial \theta \partial \theta^T} \log f(y \mid \theta)\mathrm{d}y,
</span> corresponding to the multiparameter Cramér-Rao theorem.</p></li>
<li><p>Refer to Theorem 6.6, Chap. 2 in <span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span> for a proof.</p></li>
</ul>
</section>
<section id="rao-blackwell" class="level2">
<h2 class="anchored" data-anchor-id="rao-blackwell">Rao-Blackwell 📖</h2>
<ul>
<li>The Rao-Blackwell theorem is <span class="orange">contructive strategy</span> for improving estimators that emphasizes the pivotal role of <span class="orange">sufficiency</span> in finding UMVU estimators.</li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Rao-Blackwell, <span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>, Theorem 7.3.17)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\Theta \subseteq \mathbb{R}</span> and <span class="math inline">\tilde{\theta}(\bm{Y})</span> be an <span class="orange">unbiased</span> estimator of <span class="math inline">\theta</span>. Moreover, let <span class="math inline">S = s(\bm{Y})</span> be a <span class="blue">sufficient statistic</span> for <span class="math inline">\theta</span> and <span class="math inline">\hat{\theta} = \mathbb{E}_\theta(\tilde{\theta}(\bm{Y}) \mid S)</span>. Then <span class="math inline">\hat{\theta}</span> is an estimator such that <span class="math inline">\mathbb{E}_\theta(\hat{\theta}) = \theta</span> and <span class="math display">
\text{var}_\theta(\hat{\theta}) \le \text{var}_\theta(\tilde{\theta}).
</span> That is, <span class="math inline">\hat{\theta}</span> is a <span class="blue">uniformly better unbiased estimator</span> of <span class="math inline">\theta</span>.</p>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li><p>As we shall see later, Rao-Blackwell holds for any <span class="blue">convex loss function</span>; see <span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span>, Theorem 7.8, Chap. 1. Note that <span class="orange">unbiasedness</span> will not play any role.</p></li>
<li><p>This means that conditioning of a sufficient statistic <span class="orange">reduces</span> the <span class="orange">mean squared error</span> of a biased estimator <span class="math inline">\tilde{\theta}</span>, albeit the resulting <span class="math inline">\hat{\theta}</span> would also be biased.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="en-route-to-finding-unique-umvue-i" class="level2">
<h2 class="anchored" data-anchor-id="en-route-to-finding-unique-umvue-i"><em>En route</em> to finding unique UMVUE I 📖</h2>
<ul>
<li><p>In looking for UMVUE we should only consider those based on a <span class="blue">sufficient statistic</span> <span class="math inline">S</span>. However, if both <span class="math inline">\hat{\theta}</span> and <span class="math inline">\tilde{\theta}</span> are unbiased and based on <span class="math inline">S</span>, how do we know if <span class="math inline">\hat{\theta}</span> is best unbiased?</p></li>
<li><p>The next theorem is a <span class="orange">partial answer</span>, which is useful if <span class="math inline">\hat{\theta}</span> attains the <span class="blue">Cramér-Rao</span> lower bound.</p></li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>, Theorem 7.3.19)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\Theta \subseteq \mathbb{R}</span>. If <span class="math inline">\hat{\theta}</span> is a best unbiased estimator of <span class="math inline">\theta</span>, then <span class="math inline">\hat{\theta}</span> is <span class="blue">unique</span>.</p>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example (Example 7.3.13 in <span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">Y_1,\dots,Y_n</span> be iid sample from a <span class="math inline">\text{Uniform}(0, \theta)</span>. Then the estimator <span class="math display">
\hat{\theta} = \frac{n + 1}{n} \max\{Y_1,\dots,Y_n\}
</span> is <span class="orange">unbiased</span> for <span class="math inline">\theta</span> and is based on a <span class="blue">sufficient statistic</span> <span class="math inline">S = \max\{Y_1,\dots,Y_n\}</span>. However, Cramér-Rao cannot be applied because the regularity conditions are not met. Is <span class="math inline">\hat{\theta}</span> UMVUE?</p>
</div>
</div>
</section>
<section id="en-route-to-finding-unique-umvue-ii" class="level2">
<h2 class="anchored" data-anchor-id="en-route-to-finding-unique-umvue-ii"><em>En route</em> to finding unique UMVUE II 📖</h2>
<ul>
<li>Suppose we wish to improve on an unbiased estimator <span class="math inline">\hat{\theta}</span>. Then, we could consider <span class="math inline">U = U(\bm{Y})</span> such that <span class="math inline">\mathbb{E}_\theta(U) = 0</span>, i.e.&nbsp;<span class="math inline">U</span> is an <span class="blue">unbiased estimator or 0</span>, and let <span class="math display">
\tilde{\theta} = \hat{\theta} + a U, \qquad a \in \mathbb{R}.
</span> Clearly, <span class="math inline">\tilde{\theta}</span> is also <span class="orange">unbiased</span> and its <span class="orange">variance</span> is <span class="math display">
\text{var}_\theta(\tilde{\theta}) = \text{var}_\theta(\hat{\theta}) + 2 a \text{cov}_\theta(\hat{\theta}, U) + a^2\text{var}_\theta(U).
</span></li>
<li>If <span class="math inline">\text{cov}_\theta(\hat{\theta}, U) &lt; 0</span> for some <span class="math inline">\theta</span>, then choosing <span class="math inline">a \in (0, - 2  \text{cov}_\theta(\hat{\theta}, U) / \text{var}_\theta(U))</span> gives a <span class="blue">better estimator</span> for <span class="math inline">\theta</span>, implying that <span class="math inline">\hat{\theta}</span> is not UMVUE. This actually <span class="orange">characterizes</span> UMVU estimators.</li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>, Theorem 7.3.20)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\Theta \subseteq \mathbb{R}</span> and <span class="math inline">\hat{\theta}</span> be an unbiased estimator of <span class="math inline">\theta</span>. Then <span class="math inline">\hat{\theta}</span> is UMVUE <span class="orange">if and only if</span> <span class="math inline">\hat{\theta}</span> is <span class="blue">uncorrelated</span> with all <span class="blue">unbiased estimators of <span class="math inline">0</span></span>, that is <span class="math display">
\text{cov}_\theta(\hat{\theta}, U) = 0, \quad \text{ for all } \quad U = U(\bm{Y}) \quad \text{ such that } \quad \mathbb{E}_\theta(U) = 0.
</span></p>
</div>
</div>
</section>
<section id="completeness" class="level2">
<h2 class="anchored" data-anchor-id="completeness">Completeness</h2>
<ul>
<li><p>Proving that an estimator <span class="math inline">\hat{\theta}</span> is uncorrelated with all unbiased estimators of 0 is very hard, limitating the practical usefulness of the former theorem.</p></li>
<li><p>However, if we assume <span class="math inline">S</span> is <span class="orange">complete</span>, we can finally see the light at the end of the tunnel.</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>A sufficient statistic <span class="math inline">S = s(\bm{Y})</span> for a statistical model <span class="math inline">\mathcal{F} = \{f(\mathrm{d}y \mid \theta) : \theta \in \Theta\}</span> is said to be <span class="blue">complete</span> if <span class="orange">no nonconstant</span> function <span class="math inline">g(\cdot)</span> of <span class="math inline">S</span> is <span class="orange">first-order ancillary</span>, that is, <span class="math display">
\mathbb{E}_\theta(g(S)) = c \quad \text{for all } \theta \quad \text{implies} \quad g(S) = c \:\text{ a.s.}
</span> In other words, any non-trivial transformation of <span class="math inline">S</span> conveys information about <span class="math inline">\theta</span> in expectation.</p>
</div>
</div>
</div>
<ul>
<li><p>Because of Rao-Blackwell, the UMVUE <span class="math inline">\hat{\theta} = \hat{\theta}(S)</span> must be a function of a <span class="blue">sufficient</span> statistic <span class="math inline">S</span>.</p></li>
<li><p>If <span class="math inline">S</span> is <span class="orange">complete</span>, then (using <span class="math inline">c = 0</span>), there exists no estimator <span class="math inline">U = U(S)</span> such that <span class="math inline">\mathbb{E}_\theta(U(S)) = 0</span>, with the only exception of <span class="math inline">U = 0</span>, which is uncorrelated with <span class="math inline">\hat{\theta}(S)</span>.</p></li>
</ul>
</section>
<section id="lehmann-scheffé-theorem" class="level2">
<h2 class="anchored" data-anchor-id="lehmann-scheffé-theorem">Lehmann-Scheffé theorem</h2>
<ul>
<li>We summarise these finding into a single statement, which is arguably the most relevant result of the Rao-Blackwell saga.</li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Lehmann-Scheffé-Rao-Blackwell, <span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>, Theorem 7.3.23)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\Theta \subseteq \mathbb{R}</span> and <span class="math inline">S</span> be a <span class="orange">complete</span> sufficient statistic for a parameter <span class="math inline">\theta</span>, and let <span class="math inline">\hat{\theta}</span> be an unbiased estimator of <span class="math inline">\theta</span> based only on <span class="math inline">S</span>. Then, <span class="math inline">\hat{\theta}</span> is <span class="blue">unique</span> best unbiased estimator (UMVUE) of <span class="math inline">\theta</span>.</p>
</div>
</div>
<ul>
<li><p>The <span class="blue">Lehmann-Scheffé theorem</span> is implicitly contained in the previous results. Its <span class="orange">original formulation</span> is: “<em>unbiased estimators based on complete sufficient statistics are unique.</em>”</p></li>
<li><p>The Lehmann-Scheffé theorem represents a <span class="blue">major achievement</span> in mathematical statistics, tying together <span class="orange">sufficiency</span>, <span class="grey">completeness</span> and <span class="blue">uniqueness</span>.</p></li>
</ul>
</section>
<section id="example-binomial-best-unbiased-estimation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="example-binomial-best-unbiased-estimation">Example: binomial best unbiased estimation 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be iid <span class="math inline">\textup{Bin}(N, p)</span> and we assume that <span class="math inline">N</span> is <span class="blue">known</span> and <span class="math inline">p</span> is <span class="orange">unknown</span>. We are interested in estimating the <span class="orange">reparametrization</span>: <span class="math display">
\psi = \mathbb{P}(Y_i = 1) = N p (1 - p)^{N-1}.
</span></p></li>
<li><p>We know that <span class="math inline">S = \sum_{i=1}^n Y_i \sim \textup{Bin}(n N, p)</span> is a <span class="grey">complete</span> and <span class="blue">sufficient</span> statistic<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>. However, no obvious unbiased estimator based on <span class="math inline">S</span> is immediately available.</p></li>
<li><p>Let us begin noting that <span class="math inline">\tilde{\psi} = \mathbb{I}(Y_1 = 1)</span> is an <span class="orange">unbiased</span> estimator of <span class="math inline">\psi</span>, where <span class="math inline">\mathbb{I}</span> is the <span class="blue">indicator function</span>. In fact: <span class="math inline">\mathbb{E}(\tilde{\psi}) = \mathbb{E}(\mathbb{I}(Y_1 = 1)) = \mathbb{P}(Y_1 = 1) = \psi</span>.</p></li>
<li><p>The Lehmann-Scheffé-Rao-Blackwell theorem then implies that the unique and best unbiased estimator (UMVUE) is <span class="math display">
\hat{\psi} = \mathbb{E}(\tilde{\psi} \mid S) = N \binom{N n - N}{S-1}/ \binom{N n}{S}.
</span> which can be obtained after simple calculations.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;See Example 6.2.22 of <span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>. This is also follows from general result of exponential families.</p></div></div></section>
<section id="rao-blackwell-multiparameter-case" class="level2">
<h2 class="anchored" data-anchor-id="rao-blackwell-multiparameter-case">Rao-Blackwell, multiparameter case</h2>
<ul>
<li><p>Generalizations of Rao-Blakwell theory to the multiparameter case <span class="math inline">\Theta \subseteq \mathbb{R}^p</span> are possible.</p></li>
<li><p>If one is interested in estimating <span class="math inline">\psi = g(\theta)</span> for some <span class="blue">function</span> <span class="math inline">g: \mathbb{R}^p \to \mathbb{R}</span>, then the previously developed <span class="orange">theory applies almost directly</span>, with minor modifications to the statements.</p></li>
<li><p>Note that a special case of the above is <span class="math inline">g(\theta) = \theta_j</span>, meaning that the developed theory can be <span class="orange">separately</span> applied to <span class="blue">each coordinate</span> of <span class="math inline">\theta</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Rao-Blackwell, <span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span>, Theorem 7.8, Chap. 1)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\Theta \subseteq \mathbb{R}^p</span>, and let <span class="math inline">\tilde{\theta}(\bm{Y})</span> be an estimator of <span class="math inline">\theta</span> with finite risk <span class="math inline">R(\theta; \tilde{\theta})</span> under a <span class="orange">strictly convex loss</span> function. Moreover, let <span class="math inline">S = s(\bm{Y})</span> be a <span class="blue">sufficient statistic</span> for <span class="math inline">\theta</span>, and set <span class="math inline">\hat{\theta} = \mathbb{E}_\theta(\tilde{\theta}(\bm{Y}) \mid S)</span>. Then, <span class="math display">
R(\theta; \hat{\theta}) \le R(\theta; \tilde{\theta}),
</span> unless <span class="math inline">\tilde{\theta} = \hat{\theta}</span> with probability 1.</p>
</div>
</div>
</section>
</section>
<section id="alternative-notions-of-optimality" class="level1 page-columns page-full">
<h1>Alternative notions of optimality</h1>
<section id="unbiased-estimating-equations-i" class="level2">
<h2 class="anchored" data-anchor-id="unbiased-estimating-equations-i">Unbiased estimating equations I</h2>
<ul>
<li>A <span class="blue">Z-estimator</span> is the <span class="orange">solution</span> over <span class="math inline">\Theta</span> of a system of equations function <span class="math inline">Q(\theta) = \bm{0}</span> of the type: <span class="math display">
Q(\theta) = \sum_{i=1}^n q(\theta; Y_i) = \bm{0},
</span> where <span class="math inline">q(\theta) = q(\theta; y)</span> are known vector-valued maps. These are called <span class="blue">estimating equations</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The estimating equations <span class="math inline">Q(\theta) = \sum_{i=1}^n q(\theta; Y_i)</span> are <span class="orange">unbiased</span> if they satisfy <span class="math display">
\mathbb{E}_\theta(Q(\theta)) = \bm{0}, \qquad \text{ for all } \qquad \theta \in \Theta.
</span></p>
</div>
</div>
</div>
<ul>
<li>Under iid sampling, the <span class="orange">score function</span> can be written as <span class="math inline">\ell^*(\theta) = \sum_{i=1}^n \frac{\partial}{\partial \theta}\log{f(y_i; \theta)}</span> and therefore is a Z-estimator. Moreover, under regularity conditions, it is <span class="orange">unbiased</span>: <span class="math inline">\mathbb{E}_\theta(\ell^*(\theta)) = \bm{0}</span>.</li>
</ul>
</section>
<section id="unbiased-estimating-equations-ii" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="unbiased-estimating-equations-ii">Unbiased estimating equations II</h2>
<ul>
<li><p>Remarkably, the unbiasedness of <span class="math inline">Q(\theta)</span>, combined with a few regularity conditions, is often enough to prove <span class="orange">consistency</span> of the resulting estimator <span class="math inline">\hat{\theta}</span> as <span class="math inline">n\rightarrow \infty</span>. <a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></p></li>
<li><p>Unbiasedness of <span class="math inline">Q(\theta)</span> does not imply that the solution <span class="math inline">\hat{\theta}</span> is an unbiased estimator of <span class="math inline">\theta</span>, unless <span class="math inline">Q(\theta)</span> is a <span class="blue">linear function</span>.</p></li>
<li><p>The unbiasedness of <span class="math inline">Q(\theta)</span> holds for any <span class="orange">reparametrization</span> <span class="math inline">\psi = \psi(\theta)</span>. Moreover, Z-estimators, by construction, satisfy <span class="blue">equivariance</span>, meaning that <span class="math inline">\hat{\psi} = \psi(\hat{\theta})</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;Refer to <span class="citation" data-cites="Davison2003">Davison (<a href="#ref-Davison2003" role="doc-biblioref">2003</a>)</span>, Section 7.2, for an intuitive argument and <span class="citation" data-cites="vandervaart1998">van der Vaart (<a href="#ref-vandervaart1998" role="doc-biblioref">1998</a>)</span>, Chap. 5 for a rigorous proof.</p></div></div><div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Having defined the class of unbiased estimating functions, the question naturally arises which of them we should use.</p>
</div>
</div>
</div>
</section>
<section id="asymptotic-behavior-of-unbiased-estimating-equations" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-behavior-of-unbiased-estimating-equations">Asymptotic behavior of unbiased estimating equations</h2>
<ul>
<li><p>Guidance on the choice of <span class="math inline">Q(\theta)</span> can be found by investigating its <span class="blue">asymptotic behavior</span>. We provide an <span class="orange">informal argument</span> for <span class="math inline">\theta \in \Theta \subseteq \mathbb{R}</span>.</p></li>
<li><p>Under <span class="blue">iid sampling</span> and further <span class="orange">regularity conditions</span>, implying that <span class="math inline">\mathbb{E}_\theta(q(\theta; Y_i)) = 0</span>, a Taylor series expansion of <span class="math inline">Q(\theta)</span> gives <span class="math display">
0 = Q(\hat{\theta}) \approx \sum_{i=1}^n q(\theta; Y_i) + (\hat{\theta}- \theta) \sum_{i=1}^n\frac{\partial}{\partial \theta}q(\theta; Y_i).
</span></p></li>
<li><p>Thus, re-arranging, the following approximations hold <span class="orange">for large <span class="math inline">n</span></span>: <span class="math display">
\hat{\theta} - \theta \approx \frac{\sum_{i=1}^n q(\theta; Y_i)}{- \sum_{i=1}^n \frac{\partial}{\partial \theta}q(\theta; Y_i)} \approx \frac{Q(\theta)}{\mathbb{E}_\theta\left(- \frac{\partial}{\partial \theta}Q(\theta)\right)} .
</span></p></li>
<li><p>This suggests Z-estimators are <span class="blue">asymptotically unbiased</span> and the <span class="orange">asymptotic variance</span> of <span class="math inline">\hat{\theta}</span> is <span class="math display">
\text{var}(\hat{\theta}) \approx \frac{\text{var}(Q(\theta))}{\mathbb{E}_\theta\left(- \frac{\partial}{\partial \theta}Q(\theta)\right)^2} =  n^{-1}\frac{\text{var}(q(\theta))}{\mathbb{E}_\theta\left(-\frac{\partial}{\partial \theta}q(\theta)\right)^2}.
</span></p></li>
</ul>
</section>
<section id="godambe-information" class="level2">
<h2 class="anchored" data-anchor-id="godambe-information">Godambe information</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">\bm{Y}</span> be a sample from a statistical model with parameter <span class="math inline">\theta \in \Theta \subseteq \mathbb{R}</span>. Let <span class="math inline">Q(\theta)</span> be an <span class="orange">unbiased estimating equation</span>. We define the <span class="blue">sensitivity</span> as <span class="math display">
H(\theta) := \mathbb{E}_\theta\left(-\frac{\partial}{\partial \theta}Q(\theta; \bm{Y})\right),
</span> and the <span class="grey">variability</span> as <span class="math display">
J(\theta) := \text{var}_\theta(Q(\theta; \bm{Y})).
</span> Then, the <span class="blue">Godambe information</span> is defined as <span class="math display">
V(\theta) := \frac{H(\theta)^2}{J(\theta)}.
</span></p>
</div>
</div>
</div>
<ul>
<li>An estimating equation <span class="math inline">Q(\theta)</span> which has <span class="math inline">H(\theta) = J(\theta)</span> is called <span class="blue">information unbiased</span>.</li>
</ul>
</section>
<section id="godambe-efficiency" class="level2">
<h2 class="anchored" data-anchor-id="godambe-efficiency">Godambe efficiency</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">Q(\theta)</span> and <span class="math inline">\tilde{Q}(\theta)</span> be two <span class="orange">estimating equations</span> with Godambe information <span class="math inline">V(\theta)</span> and <span class="math inline">\tilde{V}(\theta)</span>. Then <span class="math inline">Q(\theta)</span> is <span class="blue">uniformly more efficient</span> than <span class="math inline">\tilde{Q}(\theta)</span> if <span class="math display">
V(\theta) \ge \tilde{V}(\theta) \qquad\text{ for all } \qquad \theta \in \Theta \subseteq \mathbb{R}.
</span></p>
</div>
</div>
</div>
<ul>
<li><p>This criterion is appropriate because the <span class="orange">inverse</span> of the <span class="blue">Godambe information</span> <span class="math inline">V(\theta)^{-1}</span>, under regularity conditions, coincides with the <span class="orange">asymptotic variance</span> of <span class="math inline">\hat{\theta}</span>.</p></li>
<li><p>Moreover, although the <span class="blue">variance</span> <span class="math inline">J(\theta)</span> is a natural basis for comparing estimating functions, <span class="math inline">\tilde{Q}(\theta) = a Q(\theta; \bm{Y})</span> is also unbiased with variance <span class="math inline">\tilde{J}(\theta) = a^2 J(\theta)</span>.</p></li>
<li><p>Hence, a fair comparison is possible only after removing this <span class="orange">arbitrary scaling</span>. Indeed, Godambe information is <span class="blue">invariant</span> to scaling of <span class="math inline">Q(\theta)</span>.</p></li>
</ul>
</section>
<section id="godambe-information-of-the-score-function" class="level2">
<h2 class="anchored" data-anchor-id="godambe-information-of-the-score-function">Godambe information of the score function</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li><p>Let <span class="math inline">\bm{Y}</span> be a sample from a statistical model with parameter <span class="math inline">\theta \in \Theta \subseteq \mathbb{R}</span> and with <span class="blue">score function</span> <span class="math inline">\ell^*(\theta)</span>.</p></li>
<li><p>If the first Bartlett identity holds, the score function <span class="math inline">\ell^*(\theta)</span> is an <span class="orange">unbiased estimating equation</span>.</p></li>
<li><p>Moreover, if the second Bartlett identity holds, <span class="math inline">\ell^*(\theta)</span> is <span class="blue">information unbiased</span>, that is <span class="math display">
H(\theta) = \mathbb{E}_\theta\left(-\frac{\partial}{\partial \theta}\ell^*(\theta)\right)= \text{var}_\theta(\ell^*(\theta)) = J(\theta).
</span> This implies that <span class="math display">
V(\theta) =  I(\theta),
</span> that is, the <span class="blue">Godambe information</span> of <span class="math inline">\ell^*(\theta)</span> coincides with the usual <span class="orange">Fisher information</span>.</p></li>
</ul>
</div>
</div>
</div>
<ul>
<li>Hence, Godambe information is a generalization of Fisher information.</li>
</ul>
</section>
<section id="godambe-efficiency-1" class="level2">
<h2 class="anchored" data-anchor-id="godambe-efficiency-1">Godambe efficiency</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="Godambe1960">Godambe (<a href="#ref-Godambe1960" role="doc-biblioref">1960</a>)</span>)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">Y_1,\dots,Y_n</span> be a sample from a joint probability measure <span class="math inline">f(\bm{y} \mid \theta)\nu(\mathrm{d}\bm{y})</span> and let <span class="math inline">\Theta \subseteq \mathbb{R}</span>. Moreover, let <span class="math inline">Q(\theta)</span> be an unbiased estimating equation with <span class="blue">Godambe information</span> <span class="math inline">V(\theta)</span>. Then under <span class="orange">regularity conditions</span> <span class="blue">C.1</span> and <span class="blue">C.2</span>: <span class="math display">
\frac{1}{V(\theta)} \ge \frac{1}{I(\theta)}.
</span></p>
</div>
</div>
<ul>
<li><p>This result is the equivalent of the Cramér-Rao theorem for unbiased estimating equations.</p></li>
<li><p>It implies that the <span class="blue">score functions</span> are <span class="orange">optimal</span> (Godambe efficient) among unbiased estimating equations. However, Z-estimators may have appealing <span class="grey">robustness</span> properties.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Godambe information generalizes to the vector case <span class="math inline">\theta \in \Theta \subseteq \mathbb{R}^p</span> and is defined as <span class="math inline">V(\theta) = H(\theta) J(\theta)^{-1}H(\theta)</span>, sometimes called <span class="orange">sandwich</span> matrix. Optimality results generalize as well.</p>
</div>
</div>
</div>
</section>
<section id="linear-estimating-equations-i" class="level2">
<h2 class="anchored" data-anchor-id="linear-estimating-equations-i">Linear estimating equations I</h2>
<ul>
<li><p>Let <span class="math inline">Y_1, \dots, Y_n</span> be <span class="blue">independent</span> random variables with <span class="orange">mean</span> <span class="math inline">\mathbb{E}(Y_i) = \mu_i(\theta)</span> and <span class="orange">variance</span> <span class="math inline">\text{var}(Y_i) = V_i(\mu)</span>, depending on a scalar parameter <span class="math inline">\theta \in \Theta \subseteq \mathbb{R}</span>.</p></li>
<li><p>Suppose the unbiased estimating equation <span class="math inline">Q(\theta)</span> has a <span class="blue">linear form</span>: <span class="math display">
Q(\theta) = \sum_{i=1}^n q(\theta; Y_i) = \sum_{i=1}^n w_i(\theta)(Y_i - \mu_i(\theta)),
</span> for some set of positive <span class="orange">weights</span> <span class="math inline">w_i(\theta)</span>. Can we find the <span class="blue">optimal</span> set of weights, according to the <span class="blue">Godambe information</span>?</p></li>
<li><p>The <span class="blue">sensitivity</span> <span class="math inline">H(\theta)</span> and the <span class="orange">variability</span> <span class="math inline">J(\theta)</span> of <span class="math inline">Q(\theta)</span> are readily available: <span class="math display">
H(\theta) = \sum_{i=1}^n w_i(\theta)\mu_i^*(\theta), \qquad J(\theta) = \sum_{i=1}^n w_i^2(\theta) V_i(\theta).
</span> Therefore, the optimal weights are those that <span class="orange">maximize</span> the <span class="blue">Godambe information</span>: <span class="math display">
V(\theta) = \frac{\left(\sum_{i=1}^n w_i(\theta)\mu_i(\theta)\right)^2}{\sum_{i=1}^n w_i^2(\theta) V_i(\theta)}.
</span></p></li>
</ul>
</section>
<section id="linear-estimating-equations-ii" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="linear-estimating-equations-ii">Linear estimating equations II</h2>
<ul>
<li>Using Lagrange multipliers, it can be shown<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> that the <span class="orange">optimal weights</span> are <span class="math display">
w_i(\theta) \propto \frac{\mu_i^*(\theta)}{V_i(\theta)}, \qquad \mu_i^*(\theta) = \frac{\partial}{\partial \theta}\mu(\theta), \qquad i=1,\dots,n,
</span> which means the following unbiased <span class="orange">linear</span> estimating equation is <span class="blue">Godambe efficient</span>: <span class="math display">
Q(\theta) = \sum_{i=1}^n \frac{\mu_i^*(\theta)}{V_i(\theta)}(Y_i - \mu_i(\theta)).
</span></li>
<li>This optimality property holds for a special class of <span class="blue">linear</span> unbiased estimating equations but does <span class="orange">not</span> make <span class="orange">assumptions</span> of the distribution of <span class="math inline">Y_i</span> other than <span class="math inline">\mu_i(\theta)</span> and <span class="math inline">V_i(\theta)</span>.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;See <span class="citation" data-cites="Davison2003">Davison (<a href="#ref-Davison2003" role="doc-biblioref">2003</a>)</span>, Section 7.2.2, for a proof.</p></div></div><div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">Y_1,\dots,Y_n</span> be independent random variables such that <span class="math inline">\mu_i(\theta) = \lambda e_i</span> and <span class="math inline">V_i(\theta) = \lambda e_i</span>, where <span class="math inline">e_i</span> are <span class="orange">known constants</span> e.g.&nbsp;representing exposure. Then the optimal weights are <span class="math inline">w_i(\theta) = 1</span> and <span class="math inline">\hat{\theta} = \bar{Y}/ \bar{e}</span>. Note the <span class="math inline">Y_i</span> is <span class="blue">not</span> necessarily <span class="blue">Poisson</span>.</p>
</div>
</div>
</div>
</section>
<section id="blue-estimators-i" class="level2">
<h2 class="anchored" data-anchor-id="blue-estimators-i">BLUE estimators I</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Gauss-Markov, <span class="citation" data-cites="Agresti2015">Agresti (<a href="#ref-Agresti2015" role="doc-biblioref">2015</a>)</span>, Section 2.7.1)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\bm{Y} \in \mathbb{R}^n</span> be a random vector satisfying <span class="math inline">\mathbb{E}(\bm{Y}) = \bm{X}\beta</span> and <span class="math inline">\text{var}(\bm{Y}) = \sigma^2 I_p</span>, where <span class="math inline">\bm{X}</span> is an <span class="math inline">n \times p</span> known matrix with <span class="orange">full rank</span>, <span class="math inline">\beta \in \mathbb{R}^p</span> is an unknown vector, and <span class="math inline">\sigma^2 &gt; 0</span> is an unknown parameter. Then the <span class="blue">best linear unbiased estimator</span> (BLUE) of <span class="math inline">\beta</span> is <span class="math display">
\hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{Y}.
</span> meaning that <span class="math inline">\hat{\beta}</span> is <span class="orange">unbiased</span> and has the <span class="blue">minimum variance</span> among all unbiased linear estimators of <span class="math inline">\beta</span>.</p>
</div>
</div>
<ul>
<li><p>The Gauss-Markov theorem does not make specific assumptions on the distribution of <span class="math inline">\bm{Y}</span>, only on the <span class="orange">mean</span> and <span class="orange">variance</span>.</p></li>
<li><p>If we strengthen the assumptions to <span class="math inline">\bm{Y} \sim \textup{N}_p(\bm{X}\beta, \sigma^2 I_p)</span>, then <span class="math inline">\hat{\beta}</span> is the <span class="blue">UMVUE</span> of <span class="math inline">\beta</span> among all estimators, including non-linear ones; see <span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span>, Chap. 3, Sec. 4.</p></li>
<li><p>As a special case of Gauss-Markov, if <span class="math inline">Y_i</span> are iid with mean <span class="math inline">\mu</span> and variance <span class="math inline">\sigma</span>, then <span class="math inline">\bar{Y}</span> is BLUE.</p></li>
</ul>
</section>
<section id="blue-estimators-ii" class="level2">
<h2 class="anchored" data-anchor-id="blue-estimators-ii">BLUE estimators II</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Aitken, <span class="citation" data-cites="Agresti2015">Agresti (<a href="#ref-Agresti2015" role="doc-biblioref">2015</a>)</span>, Section 2.7.1)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\bm{Y} \in \mathbb{R}^n</span> be random vector satisfying <span class="math inline">\mathbb{E}(\bm{Y}) = \bm{X}\beta</span> and <span class="math inline">\text{var}(\bm{Y}) = \Sigma</span>, where <span class="math inline">\bm{X}</span> is <span class="math inline">n \times p</span> known matrix with <span class="orange">full rank</span>, <span class="math inline">\beta \in \mathbb{R}^p</span> an unknown vector, and <span class="math inline">\Sigma</span> an <span class="orange">known</span> covariance matrix. Then the <span class="blue">best linear unbiased estimator</span> (BLUE) of <span class="math inline">\beta</span> is <span class="math display">
\hat{\beta} = (\bm{X}^T\Sigma \bm{X})^{-1}\bm{X}^T\Sigma^{-1}\bm{Y}.
</span> corresponding to the <span class="orange">generalized least squares</span> estimator.</p>
</div>
</div>
<ul>
<li><p>This interesting <span class="blue">generalization</span> of Gauss-Markov is often not applicable in practice, because the covariance matrix <span class="math inline">\Sigma</span> is typically <span class="orange">unknown</span>.</p></li>
<li><p>When <span class="math inline">\Sigma = \text{diag}(\sigma_1^2,\dots,\sigma_n^2)</span> is <span class="orange">diagonal</span>, i.e.&nbsp;in presence of heteroschedasticity, the Aitken estimator reduces to the <span class="blue">weighted least squares</span> estimator.</p></li>
<li><p>Moreover, if <span class="math inline">Y_i</span> are independent random variables with mean <span class="math inline">\mu</span> and variance <span class="math inline">\sigma^2_i</span>, then the <span class="orange">weighted mean</span> <span class="math inline">\hat{\mu} = \sum_{i=1}^n w_i Y_i / \sum_{j=1}^n w_j</span> is BLUE, where <span class="math inline">w_i = (1/\sigma_i^2)</span>.</p></li>
</ul>
</section>
<section id="blue-estimators-and-unbiased-estimating-equations-i" class="level2">
<h2 class="anchored" data-anchor-id="blue-estimators-and-unbiased-estimating-equations-i">BLUE estimators and unbiased estimating equations I</h2>
<ul>
<li><p>We discuss here <span class="orange">connections</span> between the <span class="blue">BLUE estimators</span> and the <span class="orange">unbiased estimating equations</span>, aimed at providing a <span class="orange">unified view</span> of these concepts.</p></li>
<li><p>Under the same assumptions of Gauss-Markov theorem, let us consider: <span class="math display">
Q(\beta) = \bm{A}^T(\bm{Y} - \bm{X}\beta),
</span> for some <span class="math inline">n \times p</span> matrix <span class="math inline">\bm{A}</span> having full rank. This is a <span class="orange">linear unbiased</span> estimating equation, and the <span class="blue">optimal</span> choice of <span class="math inline">\bm{A}</span> maximizes the <span class="orange">Godambe information</span>.</p></li>
<li><p>Solving this estimating equation we obtain to a linear unbiased estimator, which is <span class="math display">
\hat{\beta} = (\bm{A}^T \bm{X})^{-1}\bm{A}^T \bm{Y}, \qquad \mathbb{E}(\hat{\beta}) = (\bm{A}^T \bm{X})^{-1}\bm{A}^T \bm{X}\beta = \beta.
</span></p></li>
<li><p>The <span class="blue">sensitivity</span> and <span class="orange">variability</span> matrices of <span class="math inline">Q(\beta)</span> are <span class="math display">
J(\beta) = \text{var}(Q(\beta)) = \sigma^2 \bm{A}^T \bm{A}, \qquad H(\beta) = \mathbb{E}\left(-\frac{\partial}{\partial \beta}Q(\beta)\right) = (\bm{A}^T \bm{X})^T.
</span> Consequently, the <span class="blue">Godambe information</span> is <span class="math inline">V(\beta) =  \sigma^{-2} (\bm{A}^T \bm{X})^T(\bm{A}^T \bm{A})^{-1}(\bm{A}^T \bm{X})^T</span>.</p></li>
</ul>
</section>
<section id="blue-estimators-and-unbiased-estimating-equations-ii" class="level2">
<h2 class="anchored" data-anchor-id="blue-estimators-and-unbiased-estimating-equations-ii">BLUE estimators and unbiased estimating equations II</h2>
<ul>
<li><p>The <span class="orange">optimal</span> choice of <span class="math inline">\bm{A}</span> equivalently minimizes the inverse of the <span class="blue">Godambe information</span>, which after a few algebraic manipulation is equal to <span class="math display">
V(\beta)^{-1} =  \sigma^2 \left[(\bm{A}^T \bm{X})^{-1}\bm{A}^T\right] \left[(\bm{A}^T \bm{X})^{-1}\bm{A}^T\right]^T.
</span></p></li>
<li><p>The key remark is the following: a direct calculation shows that the variance of <span class="math inline">\hat{\beta}</span> is <span class="math display">
\text{var}(\beta) = \sigma^2 \left[(\bm{A}^T \bm{X})^{-1}\bm{A}^T\right] \left[(\bm{A}^T \bm{X})^{-1}\bm{A}^T\right]^T = V(\beta)^{-1},
</span> that is, the <span class="orange">variance</span> of <span class="math inline">\hat{\beta}</span> coincides with the inverse of the <span class="blue">Godambe information</span>. This is a consequence of the linearity of <span class="math inline">Q(\beta)</span>, otherwise the property holds only asymptotically.</p></li>
<li><p>Thus, the same proof of Gauss-Markov theorem can be used to show that the <span class="blue">BLUE estimator</span> is <span class="orange">Godambe efficient</span> and the optimal matrix is <span class="math inline">\bm{A} = \bm{X}</span>, giving <span class="math display">
\hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{Y}, \qquad V(\beta) = \frac{1}{\sigma^2}\bm{X}^T\bm{X}.
</span></p></li>
<li><p>Moreover, if <span class="math inline">\bm{Y} \sim \textup{N}_p(\bm{X}\beta, \sigma^2 I_p)</span>, then <span class="math inline">V(\beta)</span> also coincides with the <span class="blue">Fisher information</span> <span class="math inline">I(\beta)</span>.</p></li>
</ul>
</section>
</section>
<section id="asymptotic-evaluations" class="level1 page-columns page-full">
<h1>Asymptotic evaluations</h1>
<section id="asymptotic-evaluations-preliminaries" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-evaluations-preliminaries">Asymptotic evaluations: preliminaries</h2>
<ul>
<li><p>Asymptotic evaluations of estimators are taught in basic courses of inferential statistics. We focus on two main properties: <span class="blue">consistency</span> and <span class="orange">asymptotic normality</span>.</p></li>
<li><p>An estimator <span class="math inline">\hat{\theta}_n</span> is <span class="orange">consistent</span> if it converges <span class="blue">in probability</span> to the true value <span class="math inline">\theta_0</span> as the sample size increases, i.e., <span class="math inline">\hat{\theta}_n \overset{p}{\longrightarrow} \theta_0</span> as <span class="math inline">n\rightarrow \infty</span>. Classical <span class="blue">sufficient conditions</span> are given below.</p></li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>, Theorem 10.1.3)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\Theta \subseteq \mathbb{R}</span> and <span class="math inline">\hat{\theta}_n</span> be a sequence of estimators such that the <span class="orange">asymptotic bias</span> and <span class="blue">variance</span> are zero, that is for every <span class="math inline">\theta \in \Theta</span> <span class="math display">
\lim_{n\to\infty}\text{bias}_\theta(\hat{\theta}_n)=0, \qquad \lim_{n\to\infty}\text{var}_\theta(\hat{\theta}_n)=0.
</span> Then <span class="math inline">\hat{\theta}_n</span> is a <span class="blue">consistent estimator</span> of <span class="math inline">\theta_0</span>.</p>
</div>
</div>
<ul>
<li>Checking this condition case-by-case is difficult. Instead, we seek <span class="blue">general sufficient conditions</span> to establish the consistency of broad estimator classes, including <span class="orange">maximum likelihood</span>.</li>
</ul>
</section>
<section id="asymptotic-evaluations-preliminaries-1" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-evaluations-preliminaries-1">Asymptotic evaluations: preliminaries</h2>
<ul>
<li><p>The <span class="orange">asymptotic normality</span> of an estimator <span class="math inline">\hat{\theta}_n</span> is a stronger property than consistency, implying that the estimator is <span class="blue">normally distributed</span> around the true value <span class="math inline">\theta_0</span> for large <span class="math inline">n</span>.</p></li>
<li><p>Let <span class="math inline">\Theta \subseteq \mathbb{R}</span> and let <span class="math inline">\hat{\theta}_n</span> be a sequence of estimators. Under “regularity conditions”: <span class="math display">
\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{\text{d}}{\longrightarrow} \text{N}(0, v(\theta_0)^{-1}), \qquad n \rightarrow \infty,
</span> where <span class="math inline">v(\theta_0)^{-1}</span> is the so-called <span class="blue">asymptotic variance</span>.</p></li>
<li><p>If <span class="math inline">\hat{\theta}_n</span> is the <span class="orange">maximum likelihood</span>, then under regularity conditions <span class="math inline">n v(\theta)</span> equals the <span class="orange">Fisher information</span> <span class="math inline">I(\theta) = n i(\theta)</span> and the <span class="blue">asymptotic variance</span> is <span class="math inline">i(\theta_0)^{-1}</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In regular problems, the maximum likelihood estimator <span class="math inline">\hat{\theta}_n</span> is <span class="orange">asymptotically efficient</span>, that is, for <span class="math inline">n</span> large enough, its variance attains the Cramér-Rao lower bound. Informally, <span class="math display">
\hat{\theta}_n \:\dot{\sim}\: \text{N}(\theta_0, I(\theta_0)^{-1}).
</span></p>
</div>
</div>
</div>
<ul>
<li><span class="orange">Remark</span>: there exist infinitely many asymptotically efficient estimators.</li>
</ul>
</section>
<section id="example-poisson-with-unknown-mean-1" class="level2">
<h2 class="anchored" data-anchor-id="example-poisson-with-unknown-mean-1">Example: Poisson with unknown mean</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be a iid random sample from a <span class="orange">Poisson</span> distribution of mean parameter <span class="math inline">\lambda &gt; 0</span>. The maximum likelihood estimator <span class="math inline">\hat{\lambda}_n</span> is the sample mean <span class="math display">
\hat{\lambda}_n = \bar{Y}.
</span></p></li>
<li><p>One could invoke the strong <span class="blue">law of large numbers</span> to show that <span class="math inline">\hat{\lambda}_n \overset{\text{a.s.}}{\longrightarrow} \lambda</span>. Alternatively: <span class="math display">
\mathbb{E}_\lambda(\hat{\lambda}) = \lambda, \qquad \text{var}_\lambda(\hat{\lambda}_n) = \frac{\lambda}{n} = I(\lambda)^{-1},
</span> which implies <span class="math inline">\text{bias}_\lambda(\hat{\lambda}_n)=0</span> and <span class="math inline">\lim_{n\to\infty}\text{var}_\lambda(\hat{\lambda}_n)=0</span>, from which <span class="orange">consistency</span> follows.</p></li>
<li><p>Moreover, as a direct application of the <span class="orange">central limit theorem</span>, we also obtain that <span class="math display">
\sqrt{n}(\hat{\lambda}_n - \lambda) \overset{\text{d}}{\longrightarrow} \text{N}(0, \lambda), \qquad i(\lambda) = \frac{1}{\lambda}.
</span></p></li>
<li><p>In order to construct confidence intervals, one typically estimate the asymptotic variance <span class="math inline">i(\lambda)^{-1}</span> with a consistent estimator <span class="math inline">i(\hat{\lambda}_n)^{-1} = \hat{\lambda}_n</span>. Then <span class="blue">Slutsky theorem</span> ensures that <span class="math display">
\sqrt{n}\:i(\hat{\lambda}_n)^{1/2}(\hat{\lambda}_n - \lambda)\overset{\text{d}}{\longrightarrow} \text{N}(0, 1).
</span></p></li>
</ul>
</section>
<section id="the-classical-regularity-conditions" class="level2">
<h2 class="anchored" data-anchor-id="the-classical-regularity-conditions">The classical “regularity conditions”</h2>
<div class="callout callout-style-simple callout-warning no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>(<strong>A1</strong>) We observe an <span class="orange">iid</span> sample <span class="math inline">Y_1,\dots,Y_n</span> from a density <span class="math inline">f(y ; \theta_0)</span> with true value <span class="math inline">\theta_0 \in \Theta \subseteq \mathbb{R}^p</span>.</p>
<p>(<strong>A2</strong>) The model is <span class="blue">identifiable</span>, that is, the densities <span class="math inline">f(\cdot ; \theta)</span> and <span class="math inline">f(\cdot ; \theta')</span> are different for <span class="math inline">\theta \neq \theta'</span>.</p>
<p>(<strong>A3</strong>) The distributions <span class="math inline">f(y ; \theta)</span> have <span class="orange">common support</span>.</p>
<p>(<strong>A4</strong>) The parameter space <span class="math inline">\Theta</span> contains an <span class="orange">open set</span> of which <span class="math inline">\theta_0</span> is an <span class="blue">interior point</span>.</p>
</div>
</div>
</div>
<p>We will sometimes need the additional conditions:</p>
<div class="callout callout-style-simple callout-warning no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>(<strong>A5</strong>) There is a neighbourhood <span class="math inline">\mathcal{N}</span> of the true value <span class="math inline">\theta_0</span> within which the first <span class="orange">three derivatives</span> of <span class="math inline">\ell(\theta)</span> exist a.s., and there exist functions <span class="math inline">m_{rst}(y)</span> such that <span class="math inline">|\partial^3f(y;\theta)/\partial\theta_r \partial\theta _s \partial \theta_t| \le m_{rst}(y)</span> and <span class="math inline">\mathbb{E}_\theta(M_{rst}(\bm{Y})) &lt; \infty</span> for <span class="math inline">r,s,t = 1,\dots,p</span> and <span class="math inline">\theta \in \mathcal{N}</span>.</p>
<p>(<strong>A6</strong>) The <span class="blue">Fisher information matrix</span> is finite and <span class="orange">positive definite</span>, and <span class="math display">
\mathbb{E}_\theta\left[\frac{\partial}{\partial\theta_r}\ell(\theta)\right] = 0, \qquad [I(\theta)]_{rs} = -\mathbb{E}_\theta\left[\frac{\partial^2}{\partial \theta_r \partial \theta_s}\ell(\theta)\right] = \mathbb{E}_\theta\left[\frac{\partial\ell(\theta)}{\partial \theta_r}\frac{\partial\ell(\theta)}{\partial \theta_s}\right],
</span> for <span class="math inline">r,s = 1,\dots,p</span>, that this, the <span class="orange">first</span> and <span class="blue">second Bartlett identities</span>.</p>
</div>
</div>
</div>
</section>
<section id="wald-inequality" class="level2">
<h2 class="anchored" data-anchor-id="wald-inequality">Wald inequality 📖</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Wald inequality, <span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span>, Theorem 3.2, Chap. 6)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Under assumptions <span class="blue">(A1)-(A3)</span>, by the strong law of large numbers: <span class="math display">
\frac{1}{n}\ell(\theta; \bm{Y}) - \frac{1}{n}\ell(\theta_0; \bm{Y}) = \frac{1}{n}\sum_{i=1}^n\log{\frac{f(Y_i; \theta)}{f(Y_i; \theta_0)}} \overset{\text{a.s.}}{\longrightarrow} - \text{KL}(f(\cdot; \theta_0) \mid f(\cdot; \theta)), \qquad n\rightarrow \infty,
</span> where <span class="math inline">\text{KL}</span> is the <span class="blue">Kullback-Leibler divergence</span>, which is <span class="orange">strictly positive</span> for <span class="math inline">\theta \neq \theta_0</span>. Thus: <span class="math display">
\ell(\theta; \bm{Y}) - \ell(\theta_0; \bm{Y}) \overset{\text{a.s.}}{\longrightarrow} - \infty.
</span> Moreover, by the properties of the <span class="math inline">\text{KL}</span>, or using Jensen’s inequality, we deduce: <span class="math display">
\mathbb{E}_{\theta_0}\left(\ell(\theta; \bm{Y})\right) &lt; \mathbb{E}_{\theta_0}\left(\ell(\theta_0; \bm{Y})\right), \qquad \theta \neq \theta_0,
</span> which is commonly known as <span class="orange">Wald inequality</span>.</p>
</div>
</div>
</section>
<section id="consistency-for-the-mle" class="level2">
<h2 class="anchored" data-anchor-id="consistency-for-the-mle">Consistency for the MLE</h2>
<ul>
<li><p>Wald inequality is the main workhorse for proving <span class="blue">consistency</span> of the <span class="orange">maximum likelihood</span>.</p></li>
<li><p>Broadly speaking, <span class="blue">on average</span>, the likelihood is <span class="orange">maximized</span> at the true value <span class="math inline">\theta_0</span>, and this holds almost surely for large <span class="math inline">n</span>, suggesting that <span class="math inline">\hat{\theta}_n \rightarrow \theta_0</span> almost surely as <span class="math inline">n\rightarrow \infty</span>.</p></li>
<li><p>We may be tempted to conclude that conditions <span class="blue">(A1)-(A3)</span> are enough to prove consistency. Unfortunately, there are <span class="orange">complications</span> on general spaces <span class="math inline">\Theta</span>. An exception is given below:</p></li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span>, Cor. 3.5, Chap. 6)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Under assumptions <span class="blue">(A1)-(A3)</span> and if <span class="math inline">\Theta = (\theta_0,\theta_1,\dots,\theta_k)</span> is <span class="orange">finite</span>, then <span class="math inline">\hat{\theta}_n</span> exists, is unique and is a <span class="blue">consistent estimator</span> of <span class="math inline">\theta_0</span>.</p>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><span class="grey">Proof</span>. Let <span class="math inline">A_{j,n} = \{\ell(\theta_0) &gt; \ell(\theta_j)\}</span> for <span class="math inline">j=1,\dots,k</span>. It holds <span class="math inline">\mathbb{P}(A_{j,n}) \rightarrow 1</span> as <span class="math inline">n\rightarrow \infty</span>. Hence: <span class="math display">
\mathbb{P}(\ell(\theta_0) &gt; \ell(\theta_j) \text { for all } j=1,\dots,k) = \mathbb{P}(\cap_{j=1}^k A_{j,n}) \ge 1 - \sum_{j=1}^k\mathbb{P}(A_{j,n}^C)\rightarrow 1, \quad n\rightarrow \infty.
</span></p>
</div>
</div>
</div>
</section>
<section id="what-could-go-wrong" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="what-could-go-wrong">What could go wrong?</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be an iid sample from the following univariate density<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> <span class="math display">
f(y; \theta) = \frac{1}{2}\phi(y; 0, 1) + \frac{1}{2}\phi(y; \theta, e^{-2/\theta^2}),  \qquad \theta \in \mathbb{R},
</span> where <span class="math inline">\phi(x; \mu, \sigma^2)</span> is the <span class="orange">normal density</span>. The density <span class="math inline">f(y; \theta)</span> satisfies conditions <span class="blue">(A1)-(A4)</span>. Moreover <span class="math inline">\ell(\theta)</span> is <span class="blue">differentiable</span>.</p></li>
<li><p>However, the log-likelihood present <span class="blue">spykes</span> in correspondence of the observed data and the maximum likelihood concentrates around <span class="math inline">0</span> rather than <span class="math inline">\theta_0</span>, i.e <span class="math inline">\hat{\theta}_n</span> is <span class="orange">inconsistent</span>.</p></li>
<li><p>Indeed, there are <span class="blue">multiple roots</span> of the score equation <span class="math inline">\ell^*(\theta)</span>, corresponding to the spykes. The “correct” solution close <span class="math inline">\theta_0</span> is among them, but is not identified by the data even for large <span class="math inline">n</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn17"><p><sup>17</sup>&nbsp;This example is taken from a <a href="https://radfordneal.wordpress.com/2008/08/09/inconsistent-maximum-likelihood-estimation-an-ordinary-example/">blogpost</a> of Radford Neal.</p></div></div><div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Note this example does not contradict Wald inequality, because the spykes occur on a <span class="orange">set of measure zero</span>. Indeed for any fixed <span class="math inline">\theta</span> the probability of observing <span class="math inline">Y_i = \theta</span> is zero.</p>
</div>
</div>
</div>
</section>
<section id="what-could-go-wrong-1" class="level2">
<h2 class="anchored" data-anchor-id="what-could-go-wrong-1">What could go wrong?</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1200"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>These are <span class="blue">simulated data</span> when the true value is <span class="math inline">\theta_0 = 0.6</span>. We plot the <span class="orange">log-density</span> <span class="math inline">\log{f(y;\theta_0)}</span> and the <span class="blue">log-likelihood</span> <span class="math inline">\ell(\theta)</span> for <span class="math inline">n=10,30,100</span>.</li>
</ul>
</section>
<section id="what-else-could-go-wrong" class="level2">
<h2 class="anchored" data-anchor-id="what-else-could-go-wrong">What else could go wrong?</h2>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="1050"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>The picture depict <span class="math inline">- \text{KL}(f(\cdot; \theta_0) \mid f(\cdot; \theta))</span> in another <span class="orange">problematic</span> situation. The true value is <span class="math inline">\theta_0 = \pi/2</span>, but the presence of the asymptote may cause <span class="math inline">\hat{\theta}_n</span> to <span class="orange">diverge</span>.</li>
</ul>
</section>
<section id="consistency-for-the-mle-1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="consistency-for-the-mle-1">Consistency for the MLE 📖</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span>, Th. 5.1, Chap. 6)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Under assumptions <span class="blue">(A1)-(A6)</span>, with probability tending to <span class="math inline">1</span> as <span class="math inline">n\rightarrow \infty</span>, the equation <span class="math inline">\ell^*(\theta) = \bm{0}</span> has at least one root <span class="math inline">\hat{\theta}_n</span>, and there exists a sequence of roots <span class="math inline">\hat{\theta}_n</span> such that <span class="math inline">\hat{\theta}_n \overset{p}{\longrightarrow} \theta_0</span>.</p>
</div>
</div>
<ul>
<li><p>This standard result of the literature requires several regularity conditions and yet it delivers <span class="orange">less</span> than <span class="orange">what it seems</span>.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></p></li>
<li><p>The claim is that a <span class="blue">clairvoyant statistician</span>, with knowlegde of <span class="math inline">\theta_0</span>, could choose a consistent sequence of roots. In reality, it may be impossibile to choose the right solution.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn18"><p><sup>18</sup>&nbsp;See also <span class="citation" data-cites="vandervaart1998">van der Vaart (<a href="#ref-vandervaart1998" role="doc-biblioref">1998</a>)</span>, Theorem 5.42, for slightly less stringent conditions and a careful discussion about the issue of multiple roots.</p></div></div><div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let us further require that, with probability tending to 1 as <span class="math inline">n\to \infty</span>, there is a <span class="blue">unique solution</span> to the <span class="orange">score equation</span>. In that case <span class="math inline">\hat{\theta}_n</span> is <span class="orange">consistent</span> and is also the <span class="blue">global maximizer</span> of <span class="math inline">\ell(\theta)</span>.</p>
</div>
</div>
</div>
</section>
<section id="asymptotic-normality-of-the-mle" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-normality-of-the-mle">Asymptotic normality of the MLE 📖</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span>, Th. 5.1, Chap. 6)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Under assumptions <span class="blue">(A1)-(A6)</span>, suppose the maximum likelihood estimator <span class="math inline">\hat{\theta}_n</span> exists and is <span class="orange">consistent</span> for the true value <span class="math inline">\theta_0</span>. Then <span class="math display">
\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{\text{d}}{\longrightarrow} \text{N}(0, i(\theta_0)^{-1}), \quad i(\theta) = -\int\left(\frac{\partial^2}{\partial \theta \partial \theta^T}\log{f(y \mid \theta)}\right)f(y \mid \theta)\mathrm{d}y,
</span> where <span class="math inline">n i(\theta)</span> is the <span class="blue">Fisher information matrix</span>.</p>
</div>
</div>
<ul>
<li><p>Informally, we say that the maximum likelihood estimator is <span class="orange">asymptotically efficient</span> because, roughly speaking, <span class="math inline">\text{var}_\theta(\hat{\theta}_n) \approx I(\theta)^{-1}</span>, the latter being the <span class="blue">Cramér-Rao lower bound</span>.</p></li>
<li><p>Rigorously, the above theorem does not establish the convergence of <span class="math inline">\mathbb{E}_\theta(\hat{\theta}_n)</span> and <span class="math inline">\text{var}_\theta(\hat{\theta}_n)</span>, nor have we introduced an asymptotic version of the Cramér-Rao bound.</p></li>
<li><p>Nevertheless, the statement that maximum likelihood estimators are <span class="blue">asymptotically efficient</span> is <span class="orange">correct</span>, as rigorously discussed in Chapter 8 of <span class="citation" data-cites="vandervaart1998">van der Vaart (<a href="#ref-vandervaart1998" role="doc-biblioref">1998</a>)</span>.</p></li>
</ul>
</section>
<section id="observed-vs-fisher-information" class="level2">
<h2 class="anchored" data-anchor-id="observed-vs-fisher-information">Observed vs Fisher information</h2>
<ul>
<li>For the <span class="blue">practical</span> construction of <span class="orange">confidence intervals</span>, or simply to empirically assess the variance of an estimator, we need to consider a consistent estimator of <span class="math inline">I(\theta) = n i(\theta)</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The negative <span class="orange">Hessian matrix</span> of the log-likelihood is called <span class="blue">observed information</span> and equals: <span class="math display">
\mathcal{I}(\theta) := - \frac{\partial^2}{\partial \theta \partial \theta^T} \ell(\theta; \bm{Y}).
</span> If the second Bartlett identity holds, the <span class="orange">Fisher information</span> is <span class="math inline">I(\theta) = \mathbb{E}_\theta(\mathcal{I}(\theta))</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>There are two natural candidates for estimating <span class="math inline">I(\theta_0) = n i(\theta_0)</span>, namely <span class="math inline">I(\hat{\theta}_n)</span> and <span class="math inline">\mathcal{I}(\hat{\theta}_n)</span>. These two quantities may coincide, but in general, this is not guaranteed.</p></li>
<li><p>Following Fisher’s original work, <span class="citation" data-cites="Efron1978">Efron and Hinkley (<a href="#ref-Efron1978" role="doc-biblioref">1978</a>)</span> suggested using <span class="math inline">\mathcal{I}(\hat{\theta}_n)</span> because it approximates the <span class="blue">conditional variance</span> of <span class="math inline">\hat{\theta}_n</span> given an appropriate <span class="orange">ancillary statistic</span>.</p></li>
<li><p>Moreover, <span class="math inline">\mathcal{I}(\hat{\theta}_n)</span> can be computed numerically via differentiation, whereas <span class="math inline">I(\hat{\theta}_n)</span> requires analytical derivations.</p></li>
</ul>
</section>
<section id="consistency-for-m-estimators-i" class="level2">
<h2 class="anchored" data-anchor-id="consistency-for-m-estimators-i">Consistency for M-estimators I</h2>
<ul>
<li><p>We now discuss a broader and modern theory for <span class="orange">consistency</span>. Let <span class="math inline">M_n(\theta) = \sum_{i=1}^n m(\theta; Y_i)</span> be an <span class="blue">M-estimator</span> and recall that the <span class="orange">maximum likelihood</span> is a special instance, with <span class="math display">
M_n(\theta) = \sum_{i=1}^n \ell(\theta;Y_i),
</span></p></li>
<li><p>In order to simplify the subsequent exposition, it is convenient to consider <span class="math display">
M_n(\theta) = \textcolor{red}{\frac{1}{n}}\sum_{i=1}^n [\ell(\theta;Y_i) \textcolor{red}{- \ell(\theta_0; Y_i)}] = \frac{1}{n}\sum_{i=1}^n\log{\frac{f(Y_i; \theta)}{f(Y_i; \theta_0)}},
</span> where the red terms are ininfluential because the maximizer <span class="math inline">\hat{\theta}_n</span> of <span class="math inline">M_n(\theta)</span> is the same.</p></li>
<li><p>Under conditions <span class="blue">(A1)-(A3)</span> the law of large numbers guarantees that <span class="math inline">M_n(\theta) \overset{\textup{p}}{\longrightarrow} M(\theta)</span> <span class="orange">pointwise</span> for every <span class="math inline">\theta</span>, where <span class="math inline">M(\theta) = - \text{KL}(f(\cdot; \theta_0) \mid f(\cdot; \theta))</span>. However, this was <span class="orange">not enough</span>.</p></li>
<li><p>On the other hand, we will see that <span class="math inline">\hat{\theta}_n</span> does not need to maximize <span class="math inline">M_n(\theta)</span>. Indeed, it is sufficient that is <span class="blue">nearly maximizes</span> it, in the sense that <span class="math inline">M_n(\hat{\theta}_n) \ge \sup_{\theta \in \Theta}M(\theta) - o_p(1)</span>.</p></li>
</ul>
</section>
<section id="consistency-for-m-estimators-ii" class="level2">
<h2 class="anchored" data-anchor-id="consistency-for-m-estimators-ii">Consistency for M-estimators II 📖</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="vandervaart1998">van der Vaart (<a href="#ref-vandervaart1998" role="doc-biblioref">1998</a>)</span>, Theorem 5.7)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\theta \subseteq \mathbb{R}^p</span>, <span class="math inline">M_n(\theta)</span> be random functions and <span class="math inline">M(\theta)</span> be a function of <span class="math inline">\theta</span> such that for every <span class="math inline">\epsilon &gt; 0</span> <span class="math display">
\begin{aligned}
\sup_{\theta \in \Theta} \left|M_n(\theta) - M(\theta)\right| \overset{\textup{p}}{\longrightarrow} 0, \qquad \text{(Uniform convergence)} \\
\sup_{\theta : ||\theta- \theta_0||_2 \ge \epsilon} M(\theta) &lt; M(\theta_0). \qquad \text{(Strong identifiability)}
\end{aligned}
</span> Then any sequence of estimators <span class="math inline">\hat{\theta}_n</span> with <span class="math inline">M_n(\hat{\theta}_n) \ge M_n(\theta_0) - o_p(1)</span> <span class="blue">converges in probability</span> to <span class="math inline">\theta_0</span> as <span class="math inline">n\rightarrow \infty</span>.</p>
</div>
</div>
<ul>
<li><p>The <span class="orange">uniform convergence</span> assumption strengthen the pointwise convergence typically ensured by the law of large numbers, that is, <span class="math inline">m(\theta)</span> should be <span class="blue">Glivenko-Cantelli</span>.</p></li>
<li><p>This holds if <span class="math inline">\Theta</span> is <span class="blue">compact</span>, <span class="math inline">m(\theta)</span> is continuous and dominated by an integrable function.</p></li>
<li><p>The strong identifiability condition, also called <span class="orange">well separability</span> ensures that only point close to <span class="math inline">\theta_0</span> are close to the maximum value <span class="math inline">M(\theta_0)</span>, strengthening Wald inequality.</p></li>
</ul>
</section>
<section id="consistency-for-z-estimators" class="level2">
<h2 class="anchored" data-anchor-id="consistency-for-z-estimators">Consistency for Z-estimators 📖</h2>
<ul>
<li><p>The former theorem can be also expressed in terms of Z-estimators <span class="math inline">Q_n(\theta)</span>, that is, on a set of <span class="orange">estimating equations</span>. An example is the score function <span class="math display">
Q_n(\theta) = \textcolor{red}{\frac{1}{n}}\sum_{i=1}^n \ell^*(\theta; Y_i).
</span></p></li>
<li><p>We require <span class="math inline">\hat{\theta}_n</span> to <span class="blue">nearly solve</span> <span class="math inline">Q_n(\theta) = \bm{0}</span> and that <span class="math inline">\lim_{n\to\infty} Q_n(\theta_0) = \bm{0}</span>. Intuitively, this comes from the <span class="blue">LLN</span> in <span class="orange">unbiased estimating equations</span>, in which case <span class="math inline">\mathbb{E}_{\theta_0}(Q_n(\theta_0)) = \bm{0}</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="vandervaart1998">van der Vaart (<a href="#ref-vandervaart1998" role="doc-biblioref">1998</a>)</span>, Theorem 5.9)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\theta \subseteq \mathbb{R}^p</span>, <span class="math inline">Q_n(\theta)</span> be random vector-valued functions and <span class="math inline">Q(\theta)</span> be a vector-valued function of <span class="math inline">\theta</span> such that for every <span class="math inline">\epsilon &gt; 0</span> <span class="math display">
\sup_{\theta \in \Theta} \left ||Q_n(\theta) - Q(\theta)\right||_2 \overset{\textup{p}}{\longrightarrow} 0, \qquad\inf_{\theta : ||\theta- \theta_0||_2 \ge \epsilon} ||Q(\theta)||_2 &gt; ||Q(\theta_0)||_2 = 0.
</span> Then any sequence of estimators <span class="math inline">\hat{\theta}_n</span> such that <span class="math inline">Q_n(\hat{\theta}_n) = o_p(1)</span> <span class="blue">converges in probability</span> to <span class="math inline">\theta_0</span>.</p>
</div>
</div>
</section>
<section id="asymptotic-normality-of-m-estimators-i" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-normality-of-m-estimators-i">Asymptotic normality of M-estimators I</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (<span class="citation" data-cites="vandervaart1998">van der Vaart (<a href="#ref-vandervaart1998" role="doc-biblioref">1998</a>)</span>, Theorem 5.21)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\Theta \subseteq \mathbb{R}^p</span> and <span class="math inline">M_n(\theta)</span> be an M-estimator whose vector-valued <span class="blue">derivative</span> is <span class="math inline">Q_n(\theta)</span>, so that <span class="math display">
M_n(\theta) = \sum_{i=1}^n m(\theta; Y_i), \qquad Q_n(\theta) = \sum_{i=1}^nq(\theta;Y_i).
</span> Let <span class="math inline">\hat{\theta}_n</span> be a sequence of <span class="orange">consistent</span> estimators such that <span class="math inline">Q_n(\hat{\theta}_n) = 0</span>. Under assumptions <span class="blue">(A1)-(A2)</span> and further <span class="blue">mild regularity conditions</span> <span class="math display">
\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{\text{d}}{\longrightarrow} \text{N}(0, v(\theta_0)^{-1}), \qquad v(\theta) = h(\theta)j(\theta)^{-1}h(\theta),
</span> where <span class="math inline">V(\theta) = n v(\theta)</span> is the <span class="orange">Godambe information matrix</span> and <span class="math display">
h(\theta) = \mathbb{E}_\theta\left(- \frac{\partial}{\partial \theta}q(\theta)\right), \qquad j(\theta) = \mathbb{E}_\theta\left(q(\theta)q(\theta)^T\right).
</span></p>
</div>
</div>
</section>
<section id="asymptotic-normality-of-m-estimators-ii" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-normality-of-m-estimators-ii">Asymptotic normality of M-estimators II</h2>
<ul>
<li><p>The previous theorem is very powerful as it applies to the broad class of M-estimators. Moreover, its statement has been substantially <span class="orange">simplified</span> compared to <span class="citation" data-cites="vandervaart1998">van der Vaart (<a href="#ref-vandervaart1998" role="doc-biblioref">1998</a>)</span>.</p></li>
<li><p>The actual statement does not even require <span class="math inline">Y_1,\dots,Y_n</span> to be independent or identically distributed.</p></li>
<li><p>Moreover, the <span class="blue">regularity conditions</span> are much weaker than <span class="blue">(A1)-(A6)</span> and essentially ensure that the involved quantities are well-defined. <span class="orange">However</span>, note that <span class="math inline">\hat{\theta}_n</span> must be <span class="orange">consistent</span>.</p></li>
<li><p>An alternative proof relying on more <span class="orange">classical conditions</span>, e.g.&nbsp;based on bounding third derivatives as in <span class="blue">(A1)-(A6)</span>, is given in <span class="citation" data-cites="vandervaart1998">van der Vaart (<a href="#ref-vandervaart1998" role="doc-biblioref">1998</a>)</span>, Theorem 5.41.</p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In regular problems, an M-estimator <span class="math inline">\hat{\theta}_n</span> is <span class="orange">consistent</span> and <span class="blue">asymptotically normal</span>. Moreover, for <span class="math inline">n</span> large enough, its variance is the inverse of the <span class="blue">Godambe information</span>. Informally, <span class="math display">
\hat{\theta}_n \:\dot{\sim}\: \text{N}(\theta_0, V(\theta_0)^{-1}).
</span></p>
</div>
</div>
</div>
</section>
<section id="first-order-bias-correction" class="level2">
<h2 class="anchored" data-anchor-id="first-order-bias-correction">First-order bias-correction</h2>
<ul>
<li><p>In a regular model with <span class="math inline">\Theta \subseteq \mathbb{R}^p</span> and independent samples, the <span class="orange">bias</span> of the <span class="orange">maximum likelihood estimator</span> can be <span class="orange">expanded</span> as follows: <span class="math display">
\text{bias}_\theta(\hat{\theta}_n) = \frac{b_1(\theta)}{n} + \frac{b_2(\theta)}{n^2} + \mathcal{O}(n^{-3}).
</span></p></li>
<li><p>The quantity <span class="math inline">\text{bias}_\theta(\hat{\theta}_n)</span> is often unavailable, but the <span class="blue">first-order</span> term <span class="math inline">b_1(\theta)</span> might be computable, e.g., in <span class="orange">exponential families</span> (<span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#ref-Pace1997" role="doc-biblioref">1997</a>)</span>, Chap. 9).</p></li>
<li><p>The <span class="orange">jackknife</span> is a popular strategy for removing the first-order bias. Alternatively, the <span class="blue">first-order bias-corrected</span> maximum likelihood estimator is obtained via plug-in as <span class="math display">
\hat{\theta}_\text{bc} = \hat{\theta}_n - \frac{b_1(\hat{\theta}_n)}{n}.
</span></p></li>
<li><p>If <span class="math inline">\tilde{\theta}_n</span> is an estimator of <span class="math inline">\theta</span> with bias of order <span class="math inline">\mathcal{O}(n^{-2})</span>, then <span class="math display">
\text{var}_\theta(\tilde{\theta}_n) = \text{var}_\theta(\hat{\theta}_\text{bc}) + \Delta^2(\theta) + \mathcal{O}(n^{-3}),
</span> where <span class="math inline">\Delta^2(\theta) \geq 0</span>, with equality if and only if <span class="math inline">\hat{\theta}_\text{bc} = \tilde{\theta}_n</span>. In this case, <span class="math inline">\hat{\theta}_\text{bc}</span> is said to be <span class="orange">second-order efficient</span>; see <span class="citation" data-cites="Efron1975b">Efron (<a href="#ref-Efron1975b" role="doc-biblioref">1975</a>)</span>.</p></li>
</ul>
</section>
<section id="bias-reduction-using-firths-correction-i" class="level2">
<h2 class="anchored" data-anchor-id="bias-reduction-using-firths-correction-i">Bias reduction using Firth’s correction I</h2>
<ul>
<li><p>The jackknife and <span class="math inline">\hat{\theta}_\text{bs}</span> are “corrective” rather than “preventive”. That is, the maximum likelihood <span class="math inline">\hat{\theta}</span> is first calculated, then corrected. A practical requirement is the <span class="orange">existence</span> of <span class="math inline">\hat{\theta}</span>.</p></li>
<li><p>Motivated by this, <span class="citation" data-cites="Firth1993">Firth (<a href="#ref-Firth1993" role="doc-biblioref">1993</a>)</span> proposed a <span class="blue">modified score equation</span>. The idea is that the <span class="orange">bias</span> in <span class="math inline">\hat{\theta}</span> can be <span class="orange">reduced</span> by introducing a <span class="blue">small bias</span> into the <span class="blue">score function</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In a regular model with <span class="math inline">\Theta \subseteq \mathbb{R}^p</span>, let <span class="math inline">\ell^*(\theta)</span> be the score function, <span class="math inline">I(\theta)</span> the Fisher information matrix and <span class="math inline">b(\theta)</span> the <span class="orange">bias</span> of the maximum likelihood <span class="math inline">\hat{\theta}_n</span>. <span class="citation" data-cites="Firth1993">Firth (<a href="#ref-Firth1993" role="doc-biblioref">1993</a>)</span> estimating equation is <span class="math display">
Q(\theta) = \ell^*(\theta) + A(\theta),
</span> where <span class="math inline">A(\theta) = A(\theta; \bm{Y})</span> is any vector such that <span class="math display">
\mathbb{E}_\theta(A(\theta)) = - I(\theta)\frac{b_1(\theta)}{n} + \mathcal{O}(n^{-1/2}).
</span> Clearly, a natural candidate for <span class="math inline">A(\theta; \bm{Y})</span> is indeed <span class="math inline">A(\theta) = - I(\theta)b_1(\theta)/n</span>.</p>
</div>
</div>
</div>
</section>
<section id="bias-reduction-using-firths-correction-ii" class="level2">
<h2 class="anchored" data-anchor-id="bias-reduction-using-firths-correction-ii">Bias reduction using Firth’s correction II</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/firth.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:6in"></p>
</figure>
</div>
</section>
<section id="bias-reduction-using-firths-correction-iii" class="level2">
<h2 class="anchored" data-anchor-id="bias-reduction-using-firths-correction-iii">Bias reduction using Firth’s correction III 📖</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">Y_1,\dots,Y_n</span> be an iid sample from a Poisson with parameter <span class="math inline">\lambda</span> and consider the reparametrization <span class="math inline">\psi = 1/\lambda</span>. The score and the Fisher information are <span class="math display">
\ell^*(\psi) = \frac{n}{\psi^2} - \frac{\bar{y}}{n \psi}, \qquad I(\psi) = \frac{n}{\psi^3}.
</span> The <span class="orange">maximum likelihood</span> is <span class="math inline">\hat{\psi} = 1/\bar{y}</span>, with <span class="blue">first-order bias</span> <span class="math inline">b_1(\psi)/n = \psi^2/n</span>. Thus <span class="math display">
Q(\psi) = \ell^*(\psi) - I(\psi)b_1(\psi)/n \quad \implies \quad \hat{\psi}_\text{Firth} = \frac{1}{\bar{y} + 1/n}.
</span></p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let <span class="math inline">Y_1,\dots,Y_n</span> be an iid sample from a Bernoulli with parameter <span class="math inline">p</span> and consider the reparametrization <span class="math inline">\beta = \log{p/(1-p)}</span>. Application of <span class="citation" data-cites="Firth1993">Firth (<a href="#ref-Firth1993" role="doc-biblioref">1993</a>)</span> method gives <span class="math display">
\hat{\beta}_\text{Firth} = \log\left(\frac{n_1 + 1/2}{n - n_1 + 1/2}\right), \quad \text{whereas}\quad \hat{\beta} = \log\left(\frac{n_1}{n - n_1}\right),
</span> which is a well-know <span class="orange">bias-reducing</span> correction of the empirical logit.</p>
</div>
</div>
</div>
</section>
</section>
<section id="robustness" class="level1 page-columns page-full">
<h1>Robustness</h1>
<section id="robustness-preliminaries" class="level2">
<h2 class="anchored" data-anchor-id="robustness-preliminaries">Robustness: preliminaries</h2>
<ul>
<li><p>Thus far, we have assumed that <span class="math inline">\mathcal{F} = \{f(\cdot;\theta) : \theta \in \Theta\}</span> is <span class="orange">correctly specified</span>, that is, there exists a <span class="math inline">\theta_0</span> that generates the data <span class="math inline">(Y_1,\dots,Y_n) \sim f(\cdot;\theta_0)</span> and <span class="math inline">f(\cdot; \theta_0) \in \mathcal{F}</span>.</p></li>
<li><p>Under this <span class="blue">assumption</span>, we have derived estimators that are <span class="orange">optimal</span> in some sense. However, if the underlying model is not correct, everything breaks down.</p></li>
<li><p>The term “<span class="orange">robustness</span>” is intentionally vague, but let us say that any statistical procedure:</p>
<ol type="a">
<li>Should have nearly optimal efficiency if the model is correctly specified</li>
<li>Small deviations from the model assumptions should impact the model only slightly</li>
<li>Somewhat larger deviations from the model should not cause a catastrophe</li>
</ol></li>
<li><p>We also distinguish among two kinds of robustness:</p>
<ol type="i">
<li>robustness with respect to <span class="blue">contamination</span> of the data (i.e.&nbsp;<span class="math inline">y_i = 10^{32}</span> is an <span class="orange">outlier</span>)</li>
<li>robustness with respect to <span class="orange">model misspecification</span>, that is, we specify a class of models <span class="math inline">\mathcal{F}</span> but in reality <span class="math inline">(Y_1,\dots,Y_n) \sim f_0(\cdot)</span> and <span class="math inline">f_0(\cdot) \notin \mathcal{F}</span>.</li>
</ol></li>
<li><p>Case i. is sometimes called <span class="orange">resistence</span> and relies on the notion of <span class="blue">influence functions</span>. For instance, the median is resistent, the mean is not.</p></li>
</ul>
</section>
<section id="example-huber-estimators-i" class="level2">
<h2 class="anchored" data-anchor-id="example-huber-estimators-i">Example: Huber estimators I 📖</h2>
<ul>
<li><p>Recall that a Huber estimator <span class="math inline">\hat{\theta}_n</span> for the <span class="orange">mean</span> <span class="math inline">\theta_0</span> is defined as the solution of <span class="math display">
Q(\theta) = \sum_{i=1}^n q(Y_i - \theta)= 0, \qquad q(y) = \begin{cases} -k \quad &amp;\text{ if }\: y \le -k\\
y \quad &amp;\text{ if  }\: |y| \le k \\
k \quad &amp;\text{ if }\: y \ge k\end{cases}.
</span></p></li>
<li><p>We assume <span class="math inline">Y_i \overset{\textup{iid}}{\sim} f_0</span> where <span class="math inline">f_0(y)</span> is a <span class="orange">continuous</span> and <span class="blue">symmetric</span> density around <span class="math inline">\theta_0</span>. Hence, there exists a density <span class="math inline">\tilde{f}_0</span> symmetric around <span class="math inline">0</span> such that <span class="math inline">\tilde{f}_0(y - \theta_0) = f_0(y)</span>.</p></li>
<li><p>First of all, provided <span class="math inline">f_0</span> is symmetric, the estimating equation <span class="math inline">Q(\theta)</span> is <span class="orange">unbiased</span> <span class="math display">
\mathbb{E}_0\{Q(\theta)\} = n \mathbb{E}_0\{q(Y_1 - \theta_0)\} = 0.</span> Broadly speaking, this means that Huber estimator <span class="math inline">\hat{\theta}_n</span> is <span class="orange">consistent</span> for <span class="math inline">\theta_0</span>. Moreover <span class="math display">
\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{\text{d}}{\longrightarrow} \text{N}(0, v^{-1}(\theta_0)),\quad v(\theta)^{-1} = j(\theta)/h(\theta)^2.
</span></p></li>
<li><p>Huber estimator is <span class="orange">robust</span> but <span class="blue">less efficient</span> than the maximum likelihood. For instance, if <span class="math inline">f_0</span> is a Gaussian with mean <span class="math inline">\theta_0</span> and variance <span class="math inline">\sigma^2</span>, then <span class="math inline">\bar{Y}</span> has asymptotic variance <span class="math inline">\sigma^2</span> and <span class="math inline">\sigma^2 \le v(\theta_0)^{-1}</span>.</p></li>
</ul>
</section>
<section id="example-huber-estimators-ii" class="level2">
<h2 class="anchored" data-anchor-id="example-huber-estimators-ii">Example: Huber estimators II 📖</h2>
<ul>
<li>Let <span class="math inline">Z \sim \tilde{f}_0</span>, and note such density <span class="orange">does not depend</span> on <span class="math inline">\theta_0</span>. After some calculations, we find <span class="math display">
j(\theta) = \mathbb{E}_0\{q(Y_1 - \theta)^2\}= \mathbb{E}\{Z^2 I(|Z|&lt; k)\} + 2 k^2 \mathbb{P}(Z &gt; k),
</span> and <span class="math display">
h(\theta) = \mathbb{E}_0\left(-\frac{\partial}{\partial\theta}q(Y_1 - \theta)\right) =  \mathbb{P}(|Z| \le k).
</span></li>
<li>Remarkably, the asymptotic variance does not depend on <span class="math inline">\theta_0</span> therefore its relative efficiency compared to the <span class="orange">normal model</span> is also <span class="blue">constant</span> over <span class="math inline">\theta_0</span>.</li>
</ul>
<ul>
<li>Below we show the <span class="blue">asymptotic relative efficiency</span> (ARE) of the Huber estimator compared to <span class="math inline">\bar{Y}</span> for different values of <span class="math inline">k</span>, assuming <span class="math inline">f_0</span> is Gaussian with <span class="math inline">\sigma^2 = 1</span> and arbitrary <span class="math inline">\theta_0</span></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 19%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><span class="math inline">k</span></th>
<th style="text-align: right;">0 (Median)</th>
<th style="text-align: right;">0.5</th>
<th style="text-align: right;">1</th>
<th style="text-align: right;">1.5</th>
<th style="text-align: right;">2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\text{ARE} = \sigma^2/v(\theta_0)^{-1} = v(\theta_0)</span></td>
<td style="text-align: right;">0.637</td>
<td style="text-align: right;">0.792</td>
<td style="text-align: right;">0.903</td>
<td style="text-align: right;">0.964</td>
<td style="text-align: right;">0.99</td>
</tr>
</tbody>
</table>
<ul>
<li>The ARE does not depend on <span class="math inline">\sigma^2</span> if we use <span class="math inline">k = \sigma \tilde{k}</span>. This is the default of the <code>huber</code> function of the <code>MASS</code> R package with <span class="math inline">\tilde{k} = 1.5</span>, where <span class="math inline">\sigma</span> is robustly estimated using the MAD.</li>
</ul>
</section>
<section id="example-omitted-variable-in-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="example-omitted-variable-in-linear-regression">Example: omitted variable in linear regression 📖</h2>
<ul>
<li><p>Let <span class="math inline">y_1,\dots,y_n</span> be realizations from the model <span class="math inline">Y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \epsilon_i</span> for <span class="math inline">i=1,\dots,n</span>, where <span class="math inline">x_i</span> and <span class="math inline">z_i</span> are linearly independent <span class="blue">covariates</span> and <span class="math inline">\epsilon_i \overset{\textup{iid}}{\sim} \text{N}(0, \sigma^2)</span> is the <span class="orange">error term</span>.</p></li>
<li><p>If we do not include <span class="math inline">z_i</span> in our model, the class <span class="math inline">\mathcal{F}</span> is <span class="orange">misspecified</span>. The <span class="orange">maximum likelihood</span> estimate of <span class="math inline">\beta_2</span> under the misspecified <span class="math inline">\mathcal{F}</span> is: <span class="math display">
\hat{\beta}_2 = \frac{1}{\sum_{j=1}^n (x_j - \bar{x})^2}\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}).
</span></p></li>
<li><p>Thus, the estimator <span class="math inline">\hat{\beta}_2</span>, <span class="blue">under the true model</span> <span class="math inline">f_0</span>, is distributed as a Gaussian with mean <span class="math display">
\mathbb{E}_0(\hat{\beta}_2) = \beta_2 + \beta_3 \frac{1}{\sum_{j=1}^n (x_j - \bar{x})^2}\sum_{i=1}^n (x_i - \bar{x})(z_i - \bar{z}),
</span> and variance <span class="math inline">\sigma^2 / \sum_{j=1}^n (x_j - \bar{x})^2</span>.</p></li>
<li><p>In other words, <span class="math inline">\hat{\beta}_2</span> is <span class="orange">biased</span> and <span class="orange">inconsistent</span> unless <span class="math inline">x</span> and <span class="math inline">z</span> are <span class="blue">uncorrelated</span> or, obviously, if the model is correctly specified, that is if <span class="math inline">\beta_3 = 0</span>.</p></li>
</ul>
</section>
<section id="maximum-likelihood-under-a-misspecified-model" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-under-a-misspecified-model">Maximum likelihood under a misspecified model 📖</h2>
<ul>
<li><p>Let <span class="math inline">f_0(\cdot)</span> denote the true probability model, and let <span class="math inline">\mathcal{F} = \{f(\cdot;\theta) : \theta \in \Theta\}</span> be an <span class="orange">incorrectly specified</span> statistical model, such that <span class="math inline">f_0 \notin \mathcal{F}</span>.</p></li>
<li><p>Suppose <span class="math inline">Y_1, \dots, Y_n</span> are <span class="blue">iid</span> under <span class="math inline">f_0</span>, and define the log-likelihood as <span class="math inline">\ell(\theta) = \sum_{i=1}^n \log{f(y_i;\theta)}</span>.</p></li>
<li><p>Let <span class="math inline">\mathbb{E}_0(\cdot)</span> denote expectation under the true model <span class="math inline">f_0</span>. If the problem is <span class="blue">sufficiently regular</span>, the <span class="orange">maximum likelihood</span> estimator <span class="math inline">\hat{\theta}_n</span> <span class="orange">converges</span> in probability to some value <span class="math inline">\theta_0</span> such that <span class="math display">
\mathbb{E}_0\{\ell(\theta)\} &lt; \mathbb{E}_0\{\ell(\theta_0)\}, \quad \text{for all } \theta \in \Theta, \quad \theta \neq \theta_0,
</span> as shown in <span class="citation" data-cites="Huber1967">Huber (<a href="#ref-Huber1967" role="doc-biblioref">1967</a>)</span>. That is, <span class="math inline">\hat{\theta}_n</span> converges to a value satisfying <span class="orange">Wald’s inequality</span>.</p></li>
<li><p>An alternative formulation of this result is the following: <span class="math display">
\mathrm{KL}(f(\cdot; \theta_0) \mid f_0) &lt; \mathrm{KL}(f(\cdot; \theta) \mid f_0), \quad \text{for all } \theta \in \Theta, \quad \theta \neq \theta_0.
</span></p></li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In other words, the <span class="orange">maximum likelihood</span> converges to <span class="math inline">\theta_0</span>, which represents the parameter value that makes <span class="math inline">f(\cdot;\theta)</span> <span class="blue">as close as possible</span> to the true <span class="math inline">f_0</span>, albeit <span class="math inline">\mathrm{KL}(f(\cdot; \theta_0) \mid f_0) &gt; 0</span>.</p>
<p>The maximum likelihood makes our predictions “the best they can be” given the chosen model.</p>
</div>
</div>
</div>
</section>
<section id="example-misspecified-exponential-model-i" class="level2">
<h2 class="anchored" data-anchor-id="example-misspecified-exponential-model-i">Example: misspecified exponential model I</h2>
<ul>
<li><p>Let <span class="math inline">\mathcal{F} = \{\mu^{-1}e^{-y/\mu} : \mu \in \mathbb{R}^+ \}</span> and suppose the data <span class="math inline">Y_1,\dots,Y_n</span> are iid from an <span class="orange">exponential</span> with mean <span class="math inline">\mu_0</span>. Then <span class="math inline">\bar{Y}</span> is a <span class="blue">consistent</span> and <span class="orange">efficient</span> estimator for <span class="math inline">\mu_0</span>.</p></li>
<li><p>However, <span class="math inline">\bar{Y}</span> is a <span class="orange">robust</span> estimator, in the sense that if instead <span class="math inline">Y_i \overset{\textup{iid}}{\sim} f_0(\cdot)</span> with <span class="math inline">f_0(\cdot) \notin \mathcal{F}</span>, then <span class="math inline">\bar{Y}</span> remains <span class="blue">consistent</span> for the mean <span class="math inline">\mu_0</span> under minimal assumptions on <span class="math inline">f_0</span> (i.e.&nbsp;<span class="math inline">\mu_0</span> must exist).</p></li>
<li><p>On the other hand, the central limit theorem shows that the asymptotic distribution is <span class="math display">
\sqrt{n}(\bar{Y} - \mu) \overset{\textup{d}}{\longrightarrow} \textup{N}(0, \sigma^2).
</span> where <span class="math inline">\sigma^2</span> is the variance of <span class="math inline">Y_i</span> under <span class="math inline">f_0(\cdot)</span>, provided <span class="math inline">\sigma^2 &lt; \infty</span>.</p></li>
<li><p>Thus, <span class="blue">confidence intervals</span> are robust if <span class="math inline">\sigma^2</span> is estimated in a <span class="orange">robust</span> way, e.g.&nbsp;using the <span class="blue">sample variance</span>, but <span class="orange">not</span> if we use the usual <span class="math inline">\hat{\sigma} = i(\hat{\mu})^{-1} = \bar{Y}^2</span> implied by the exponential specification.</p></li>
<li><p>This example can be read under the lenses of estimating equations. The score function <span class="math display">
\ell^*(\mu) = \sum_{i=1}^n(Y_i - \mu)
</span> is an <span class="blue">unbiased estimating equation</span> under <span class="math inline">f_0(\cdot)</span> for a broad class of models beyond <span class="math inline">\mathcal{F}</span>.</p></li>
</ul>
</section>
<section id="robustness-and-unbiased-estimating-equations" class="level2">
<h2 class="anchored" data-anchor-id="robustness-and-unbiased-estimating-equations">Robustness and unbiased estimating equations</h2>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>An essential requirement is that estimands must have the <span class="blue">same interpretation</span> under all the potential models. In the former exponential example <span class="math inline">\mu_0</span> represents the mean of <span class="math inline">f_0</span>.</p>
<p>Formally, this means we can write the parameter <span class="math inline">\theta_0</span> as a <span class="orange">functional of interest</span> <span class="math inline">T(\cdot)</span> of <span class="math inline">f_0</span>: <span class="math display">
\theta_0 = T(f_0).
</span> For example, we might have <span class="math inline">\theta_0 = \int_\mathcal{Y} \bm{y} f_0(\bm{y}) \nu(\mathrm{d}\bm{y})</span>, that is, the mean of <span class="math inline">f_0</span>.</p>
<p>On the other hand, for instance the parameters <span class="math inline">\alpha</span> and <span class="math inline">\beta</span> of a Gamma distribution are not robust to interpretation, because they are meaningless for models other than the Gamma.</p>
</div>
</div>
</div>
<ul>
<li><p>More broadly, if the <span class="orange">score function</span> <span class="math inline">\ell^*(\theta) = \sum_{i=1}^n \ell^*(\theta; Y_i)</span> is <span class="blue">unbiased</span> under <span class="math inline">f_0(\cdot)</span>, the estimator is <span class="blue">consistent</span> under mild conditions.</p></li>
<li><p>Moreover, even if the maximum likelihood <span class="math inline">\hat{\theta}_n</span> is consistent, the <span class="blue">asymptotic variance</span> is not anymore the one induced by the Fisher information: <span class="orange">adjustments</span> are needed.</p></li>
</ul>
</section>
<section id="misspecified-likelihoods-are-m-estimators" class="level2">
<h2 class="anchored" data-anchor-id="misspecified-likelihoods-are-m-estimators">Misspecified likelihoods are M-estimators</h2>
<ul>
<li><p>The <span class="blue">theory</span> of <span class="blue">M- and Z-estimators</span> can be directly applied to investigate the asymptotic behavior the maximum likelihood estimator <span class="orange">under model misspecification</span>.</p></li>
<li><p>If <span class="math inline">\mathcal{F}</span> is misspecified then the maximizer of the log-likelihood <span class="math inline">\ell(\theta) = \sum_{i=1}^n \log{f(y_i;\theta)}</span> should be regarded as an <span class="orange">M-estimator</span> while the score function <span class="math inline">\ell^*(\theta)</span> is a <span class="blue">Z-estimator</span>.</p></li>
<li><p><span class="orange">Consistency</span> and <span class="blue">asymptotic normality</span> of the maximum likelihood estimator <span class="math inline">\hat{\theta}_n</span> for misspecified models hold under the assumptions in <span class="citation" data-cites="vandervaart1998">van der Vaart (<a href="#ref-vandervaart1998" role="doc-biblioref">1998</a>)</span>, Theorems 5.7, 5.9, 5.21.</p></li>
<li><p>Note that under misspecification, <span class="blue">Bartlett identity</span> does not hold anymore. However, under the assumptions of Theorem 5.21 of <span class="citation" data-cites="vandervaart1998">van der Vaart (<a href="#ref-vandervaart1998" role="doc-biblioref">1998</a>)</span>, the maximum likelihood is such that <span class="math display">
\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{\text{d}}{\longrightarrow} \text{N}(0, v(\theta_0)^{-1}), \qquad v(\theta) = h(\theta)j(\theta)^{-1}h(\theta),
</span> where <span class="math inline">V(\theta) = n v(\theta)</span> is the <span class="orange">Godambe information matrix</span> and <span class="math display">
h(\theta) = \mathbb{E}_0\left(- \frac{\partial}{\partial \theta}\ell^*(\theta)\right), \qquad j(\theta) = \mathbb{E}_0\left(\ell^*(\theta)\ell^*(\theta)^T\right).
</span> where expectations are taken over <span class="math inline">f_0</span> and not the misspecified <span class="math inline">f(\cdot; \theta)</span>, so that <span class="math inline">h(\theta) \neq j(\theta)</span>.</p></li>
</ul>
</section>
<section id="sandwich-estimators" class="level2">
<h2 class="anchored" data-anchor-id="sandwich-estimators">Sandwich estimators</h2>
<ul>
<li><p>In order to compute <span class="orange">confidence intervals</span> and test hypotheses, we need to estimate the <span class="blue">asymptotic variance</span> of the maximum likelihood estimator <span class="math inline">\hat{\theta}_n</span> under model misspecification.</p></li>
<li><p>Informally, recall that for <span class="math inline">n</span> large enough and under regularity conditions, we have <span class="math display">
\text{var}_0(\hat{\theta}_n) \approx V(\theta_0)^{-1}.
</span></p></li>
<li><p>If the model is correctly specified, then typical <span class="orange">estimators</span> of the <span class="orange">variance</span> of <span class="math inline">\hat{\theta}_n</span> are <span class="math inline">\mathcal{I}(\hat{\theta}_n)</span> and <span class="math inline">I(\hat{\theta}_n)</span>. Unfortunately, we cannot use <span class="math inline">V(\hat{\theta}_n)</span> because it depends on <span class="math inline">f_0</span>, which is unknown!</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The <span class="blue">sandwich estimator</span> is a popular choice for estimating the <span class="orange">asymptotic variance</span> of <span class="math inline">\hat{\theta}_n</span> under model misspecification. The sandwich estimator for <span class="math inline">V(\theta)</span> is <span class="math display">
\hat{V}(\hat{\theta}_n) = \mathcal{I}(\hat{\theta}_n)\left(\sum_{i=1}^n\ell^*(\hat{\theta}_n; y_i)\ell^*(\hat{\theta}_n; y_i)^T\right)^{-1}\mathcal{I}(\hat{\theta}_n),
</span> recalling that <span class="math inline">\mathcal{I}(\theta) = - \partial/\partial \theta  \:\ell^*(\theta; \bm{y})</span> is the <span class="blue">observed information</span> matrix. If the model is correctly specified then <span class="math inline">\hat{V}(\hat{\theta}_n) = \mathcal{I}(\hat{\theta}_n) + o_p(1)</span>.</p>
</div>
</div>
</div>
</section>
<section id="example-misspecified-exponential-model-ii" class="level2">
<h2 class="anchored" data-anchor-id="example-misspecified-exponential-model-ii">Example: misspecified exponential model II 📖</h2>
<ul>
<li><p>Let us consider again <span class="math inline">Y_i \overset{\textup{iid}}{\sim} f_0(\cdot)</span> with <span class="math inline">f_0(\cdot) \notin \mathcal{F}</span>, where <span class="math inline">\mathcal{F}</span> is a <span class="orange">misspecified exponential</span> model.</p></li>
<li><p>The score function is <span class="orange">unbiased</span> for <span class="math inline">\mu</span> under <span class="math inline">f_0</span>. The score and the observed observation matrix are, respectively <span class="math display">
\ell^*(\mu) = \sum_{i=1}^n \left(-\frac{1}{\mu} + \frac{y_i}{\mu^2}\right) = - \frac{n}{\mu} + \frac{n \bar{y}}{\mu^2}, \qquad \mathcal{I}(\mu) = -\frac{\partial}{\partial \mu}\ell^*(\mu) = -\frac{n}{\mu^2} + \frac{2 n \bar{y}}{\mu^3}.
</span></p></li>
<li><p>The inverse of the observed information matrix, evaluated at <span class="math inline">\hat{\mu} = \bar{y}</span>, is an estimate of the asymptotic variance <span class="orange">assuming</span> the model is <span class="blue">correctly specified</span>: <span class="math display">
\text{var}_\theta(\hat{\mu}) \approx \mathcal{I}(\hat{\mu})^{-1} = \frac{\bar{y}^2}{n}.
</span></p></li>
<li><p>If the model is <span class="orange">misspecified</span>, then we can use the <span class="orange">sandwich estimator</span>: <span class="math display">
\text{var}_0(\hat{\mu}) \approx \mathcal{I}(\hat{\mu})^{-2} \sum_{i=1}^n\ell^*(\hat{\mu}; y_i)^2 = \frac{\bar{y}^4}{n^2} \sum_{i=1}^n\frac{1}{\bar{y}^4}(y_i - \bar{y})^2 = \frac{1}{n}\left(\frac{1}{n}\sum_{i=1}^n (y_i - \bar{y})^2\right),
</span> which is the <span class="math inline">n^{-1}</span> times the usual method of moments estimator for the variance.</p></li>
</ul>
</section>
<section id="example-correlated-observations" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="example-correlated-observations">Example: correlated observations</h2>
<ul>
<li><p>Let <span class="math inline">y_1,\dots,y_n</span> be realizations of <span class="orange">dependent</span> random variables from the density <span class="math inline">f(\bm{y};\theta)</span> with <span class="math inline">\Theta \subseteq \mathbb{R}^p</span>. Moreover, let <span class="math inline">f(y_i;\theta)</span> be the <span class="blue">marginal density</span> of <span class="math inline">y_i</span>.</p></li>
<li><p>If the data are dependent, the log-likelihood <span class="math inline">\ell_c(\theta) = \sum_{i=1}^n \log{f(y_i;\theta)}</span> is <span class="orange">misspecified</span>, because it assumes independence. Indeed, <span class="math inline">\ell_c(\theta)</span> is an instance of <span class="blue">composite likelihood</span>.</p></li>
<li><p>Nonetheless, under mild regularity conditions, the <span class="grey">misspecified score function</span> is <span class="orange">unbiased</span> under the true model <span class="math inline">f(\bm{y};\theta)</span>, thanks to the <span class="blue">linearity</span> of the expectation operator: <span class="math display">
\ell^*_c(\theta) = \sum_{i=1}^n \frac{\partial}{\partial \theta}\log{f(y_i;\theta)} \implies \mathbb{E}_0\{\ell^*_c(\theta)\} = \bm{0}.
</span></p></li>
<li><p>Unbiasedness of <span class="math inline">\ell^*_c(\theta)</span> does not guarantee consistency nor asymptotic normality, but both are recovered under mild assumptions on the dependence structure, relying on <span class="blue">ergodic</span> theorems or <span class="orange">martingales</span>. Remarkably, a Godambe-like asymptotic variance is obtained at the limit<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;Refer to Section 7.2.3 in <span class="citation" data-cites="Davison2003">Davison (<a href="#ref-Davison2003" role="doc-biblioref">2003</a>)</span> for a discussion on general estimating equations with dependent data and sufficient conditions for consistency and asymptotic normality.</p></div></div></section>
<section id="example-the-probability-of-observing-a-zero" class="level2">
<h2 class="anchored" data-anchor-id="example-the-probability-of-observing-a-zero">Example: the probability of observing a zero 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be an <span class="blue">iid</span> sample from a <span class="orange">discrete</span> distribution with pdf <span class="math inline">f_0</span>. We are interested in estimating the following functional <span class="math display">
\psi_0 = \mathbb{P}(Y_i = 0) = f_0(0),
</span></p></li>
<li><p>Under a Poisson model <span class="math inline">\mathcal{F}</span> with mean <span class="math inline">\lambda</span>, <span class="math inline">\psi</span> is reparametrization: <span class="math inline">\psi = e^{-\lambda}</span>. Two estimators are: <span class="math display">
\hat{\psi}_\text{ML} = e^{-\bar{y}} \quad (\text{maximum likelihood}), \qquad \hat{\psi} = \left(\frac{n-1}{n}\right)^{n\bar{y}}, \quad (\text{UMVU}).
</span></p></li>
<li><p>Both estimators are <span class="orange">not robust</span> for <span class="math inline">\psi_0</span> under model misspecification, unless under <span class="math inline">\psi_0</span> we have <span class="math inline">\psi_0 = e^{-\mu_0}</span>. Indeed, the score equation is <span class="orange">biased</span>, being equal to <span class="math display">
\ell^*(\psi) = \frac{n}{\psi}\left(1 - \frac{\bar{y}}{-\log{\psi}}\right) \implies \mathbb{E}_0\{\ell^*(\psi_0)\} = \frac{n}{\psi_0}\left(1 - \frac{\mu_0}{-\log{\psi_0}}\right).
</span></p></li>
<li><p>A <span class="orange">robust alternative</span> is given by the <span class="blue">empirical proportion</span> of zero: <span class="math display">
\hat{\psi}_\text{MM} = \frac{1}{n}\sum_{i=1}^n \mathbb{I}(y_i = 0).
</span></p></li>
</ul>
</section>
<section id="example-linear-models-with-misspecified-variance-i" class="level2">
<h2 class="anchored" data-anchor-id="example-linear-models-with-misspecified-variance-i">Example: linear models with misspecified variance I 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_i = \bm{x}_i^T\beta + \epsilon_i</span>, where the <span class="orange">errors</span> <span class="math inline">\epsilon_i</span> are random variables distributed according to <span class="math inline">f_0</span>, whereas <span class="math inline">\bm{x}_i</span> are <span class="blue">known covariates</span> and <span class="math inline">\beta \in \mathbb{R}^p</span> is a <span class="orange">parameter</span> of interest.</p></li>
<li><p>If we further assume that <span class="math inline">\epsilon_i \overset{\textup{iid}}{\sim} \text{N}(0, \sigma^2)</span>, then the <span class="orange">maximum likelihood estimator</span> <span class="math inline">\hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}</span> is the <span class="blue">least squares estimator</span>. Moreover, the <span class="blue">score function</span> is <span class="math display">
\ell^*(\beta) = \frac{1}{\sigma^2}\sum_{i=1}^n \bm{x}_i(y_i - \bm{x}_i^T\beta) = \frac{1}{\sigma^2}\bm{X}^T(\bm{y} - \bm{X}\beta),
</span> whereas the Fisher/observed <span class="orange">information matrix</span> is <span class="math display">
I(\beta) = \mathcal{I}(\beta) = \frac{1}{\sigma^2}\sum_{i=1}^n \bm{x}_i\bm{x}_i^T = \frac{1}{\sigma^2}\bm{X}^T\bm{X} \implies \text{var}_\beta(\hat{\beta}) = \sigma^2 (\bm{X}^T\bm{X})^{-1}.
</span></p></li>
<li><p>Provided that <span class="math inline">\mathbb{E}_0(Y_i) = \bm{x}_i^T\beta</span>, that is, as long as the <span class="blue">linearity assumption</span> holds under <span class="math inline">f_0</span>, then the OLS estimator is <span class="orange">robust</span> and <span class="blue">unbiased</span> even when <span class="math inline">\epsilon_i</span> are not Gaussian or <span class="orange">not iid</span>.</p></li>
<li><p>If the iid Gaussian model is <span class="orange">misspecified</span>, e.g.&nbsp;because <span class="math inline">\text{var}_0(\epsilon_i) = \sigma^2_i</span> (heteroskedasticity), then the OLS estimator is still a good choice but its variance should be adjusted.</p></li>
</ul>
</section>
<section id="example-linear-models-with-misspecified-variance-ii" class="level2">
<h2 class="anchored" data-anchor-id="example-linear-models-with-misspecified-variance-ii">Example: linear models with misspecified variance II 📖</h2>
<ul>
<li><p>The <span class="blue">variance</span> of <span class="math inline">\hat{\beta}</span> under a general model <span class="math inline">f_0</span> in which the random vector <span class="math inline">(\epsilon_1,\dots,\epsilon_n)</span> has zero mean and <span class="math inline">\text{var}_0(\bm{\epsilon}) = \bm{\Sigma}</span>, is explicity available <span class="math display">
\text{var}_0(\hat{\beta}) =  (\bm{X}^T\bm{X})^{-1} \bm{X}^T \bm{\Sigma}\bm{X} (\bm{X}^T\bm{X})^{-1}.
</span> This coincides with the inverse <span class="blue">Godambe information</span> of the <span class="orange">estimating equation</span> <span class="math inline">\ell^*(\beta)</span>: <span class="math display">
\text{var}_0(\hat{\beta}) = I(\beta)^{-1} \mathbb{E}_0\left\{\frac{1}{\sigma^4}\bm{X}^T(\bm{Y} - \bm{X}\beta)(\bm{Y} - \bm{X}\beta)\bm{X}\right\} I(\beta)^{-1}
</span></p></li>
<li><p>In practice, the matrix <span class="math inline">\bm{\Sigma}</span> is unknown. Thus, we rely on the <span class="blue">sandwich estimator</span>: <span class="math display">
\begin{aligned}
\hat{V}(\hat{\beta})^{-1} &amp;= I(\hat{\beta})^{-1} \left\{\frac{1}{\sigma^4}\sum_{i=1}^n \bm{x}_i(y_i - \bm{x}_i^T\hat{\beta})(y_i - \bm{x}_i^T\hat{\beta})\bm{x}_i^T\right\} I(\hat{\beta})^{-1} \\
&amp;= (\bm{X}^T\bm{X})^{-1} \bm{X}^T \bm{R}^2 \bm{X} (\bm{X}^T\bm{X})^{-1},
\end{aligned}
</span> where <span class="math inline">\bm{R}^2 = \text{diag}(r_1^2,\dots,r_n^2)</span> and <span class="math inline">r_i = y_i - \bm{x}_i^T\hat{\beta}</span> are the <span class="orange">residuals</span>.</p></li>
<li><p>This <span class="orange">robust</span> estimator is known as the <span class="blue">White’s correction</span> after <span class="citation" data-cites="White1980">White (<a href="#ref-White1980" role="doc-biblioref">1980</a>)</span>, who also proved it is consistent if <span class="math inline">\bm{\Sigma} = \text{diag}(\sigma_1^2,\dots,\sigma_n^2)</span> and under technical conditions on <span class="math inline">\bm{X}</span>.</p></li>
</ul>
</section>
<section id="example-linear-models-with-misspecified-variance-iii" class="level2">
<h2 class="anchored" data-anchor-id="example-linear-models-with-misspecified-variance-iii">Example: linear models with misspecified variance III</h2>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid figure-img" width="1800"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>In this example, we fit a linear model (in the parameters!) so that <span class="math inline">\bm{Y} = \bm{X}\beta + \epsilon</span> using <span class="blue">least squares</span>, with <span class="math inline">p = 12</span>. The residuals clearly show <span class="orange">heteroskedasticity</span>.</li>
</ul>
</section>
<section id="example-linear-models-with-misspecified-variance-iv" class="level2">
<h2 class="anchored" data-anchor-id="example-linear-models-with-misspecified-variance-iv">Example: linear models with misspecified variance IV</h2>
<ul>
<li>The <span class="blue">standard</span> estimator is <span class="math inline">\hat{\sigma}^2 (\bm{X}^T\bm{X})^{-1}</span>. Below we show the first <span class="math inline">4 \times 4</span> entries</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\beta_1</span></th>
<th style="text-align: right;"><span class="math inline">\beta_2</span></th>
<th style="text-align: right;"><span class="math inline">\beta_3</span></th>
<th style="text-align: right;"><span class="math inline">\beta_4</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\beta_1</span></td>
<td style="text-align: right;"><strong>113.11</strong></td>
<td style="text-align: right;">-47.95</td>
<td style="text-align: right;">17.70</td>
<td style="text-align: right;">-12.81</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\beta_2</span></td>
<td style="text-align: right;">-47.95</td>
<td style="text-align: right;"><strong>170.53</strong></td>
<td style="text-align: right;">-73.32</td>
<td style="text-align: right;">54.10</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\beta_3</span></td>
<td style="text-align: right;">17.70</td>
<td style="text-align: right;">-73.32</td>
<td style="text-align: right;"><strong>87.43</strong></td>
<td style="text-align: right;">-69.35</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\beta_4</span></td>
<td style="text-align: right;">-12.81</td>
<td style="text-align: right;">54.10</td>
<td style="text-align: right;">-69.35</td>
<td style="text-align: right;"><strong>217.58</strong></td>
</tr>
</tbody>
</table>
<ul>
<li>The <span class="orange">sandwhich (White)</span> estimator is instead <span class="math inline">(\bm{X}^T\bm{X})^{-1} \bm{X}^T \bm{R}^2 \bm{X} (\bm{X}^T\bm{X})^{-1}</span>:</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\beta_1</span></th>
<th style="text-align: right;"><span class="math inline">\beta_2</span></th>
<th style="text-align: right;"><span class="math inline">\beta_3</span></th>
<th style="text-align: right;"><span class="math inline">\beta_4</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\beta_1</span></td>
<td style="text-align: right;"><strong>2.82</strong></td>
<td style="text-align: right;">-9.73</td>
<td style="text-align: right;">9.45</td>
<td style="text-align: right;">-8.80</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\beta_2</span></td>
<td style="text-align: right;">-9.73</td>
<td style="text-align: right;"><strong>42.79</strong></td>
<td style="text-align: right;">-43.71</td>
<td style="text-align: right;">40.57</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\beta_3</span></td>
<td style="text-align: right;">9.45</td>
<td style="text-align: right;">-43.71</td>
<td style="text-align: right;"><strong>72.01</strong></td>
<td style="text-align: right;">-67.22</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\beta_4</span></td>
<td style="text-align: right;">-8.80</td>
<td style="text-align: right;">40.57</td>
<td style="text-align: right;">-67.22</td>
<td style="text-align: right;"><strong>288.72</strong></td>
</tr>
</tbody>
</table>
<ul>
<li>We used the <code>sandwich</code> R package, that implements several sandwich variants.</li>
</ul>
</section>
</section>
<section id="references-and-study-material" class="level1">
<h1>References and study material</h1>
<section id="main-references" class="level2">
<h2 class="anchored" data-anchor-id="main-references">Main references</h2>
<ul>
<li><span class="citation" data-cites="Davison2003">Davison (<a href="#ref-Davison2003" role="doc-biblioref">2003</a>)</span>
<ul>
<li><span class="orange">Chapter 4</span> (<em>Likelihood</em>)</li>
<li><span class="orange">Chapter 7</span> (<em>Estimation and Hypothesis Testing</em>)</li>
<li><span class="orange">Chapter 8</span> (<em>Linear Regression Model</em>)</li>
</ul></li>
<li><span class="citation" data-cites="Casella2002">Casella and Berger (<a href="#ref-Casella2002" role="doc-biblioref">2002</a>)</span>
<ul>
<li><span class="blue">Chapter 7</span> (<em>Point estimation</em>)</li>
<li><span class="blue">Chapter 10</span> (<em>Asymptotic evaluations</em>)</li>
</ul></li>
<li><span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#ref-Pace1997" role="doc-biblioref">1997</a>)</span>
<ul>
<li><span class="grey">Chapter 2</span> (<em>Data and model reduction</em>)</li>
<li><span class="grey">Chapter 3</span> (<em>Survey of basic concepts and techniques</em>)</li>
</ul></li>
</ul>
</section>
<section id="additional-references" class="level2">
<h2 class="anchored" data-anchor-id="additional-references">Additional references</h2>
<ul>
<li><span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#ref-Lehmann1998" role="doc-biblioref">1998</a>)</span>
<ul>
<li><span class="orange">Chapter 1</span> (<em>Preparations</em>)</li>
<li><span class="orange">Chapter 2</span> (<em>Unbiasedness</em>)</li>
<li><span class="orange">Chapter 4</span> (<em>Average risk optimality</em>)</li>
<li><span class="orange">Chapter 5</span> (<em>Minimax and admissibility</em>)</li>
<li><span class="orange">Chapter 5</span> (<em>Minimax and admissibility</em>)</li>
<li><span class="orange">Chapter 6</span> (<em>Asymptotic optimality</em>)</li>
</ul></li>
<li><span class="citation" data-cites="Robert1994">Robert (<a href="#ref-Robert1994" role="doc-biblioref">1994</a>)</span>
<ul>
<li><span class="orange">Chapter 2</span> (<em>Decision-theoretic foundations</em>)</li>
</ul></li>
<li><span class="citation" data-cites="vandervaart1998">van der Vaart (<a href="#ref-vandervaart1998" role="doc-biblioref">1998</a>)</span>
<ul>
<li><span class="blue">Chapter 4</span> (<em>Moment estimators</em>)</li>
<li><span class="blue">Chapter 5</span> (<em>M- and Z-Estimators</em>)</li>
</ul></li>
</ul>
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Agresti2015" class="csl-entry" role="listitem">
Agresti, A. (2015), <em><span class="nocase">Foundations of Linear and Generalized Linear Models</span></em>, Wiley.
</div>
<div id="ref-Casella2002" class="csl-entry" role="listitem">
Casella, G., and Berger, R. L. (2002), <em><span>Statistical Inference</span></em>, Duxbury.
</div>
<div id="ref-Davison2003" class="csl-entry" role="listitem">
Davison, A. C. (2003), <em><span>Statistical Models</span></em>, Cambridge University Press.
</div>
<div id="ref-Diaconis1979" class="csl-entry" role="listitem">
Diaconis, P., and Ylvisaker, D. (1979), <span>“<span class="nocase">Conjugate prior for exponential families</span>,”</span> <em>The Annals of Statistics</em>, 7, 269–292.
</div>
<div id="ref-Efron1975b" class="csl-entry" role="listitem">
Efron, B. (1975), <span>“<span class="nocase">Defining the curvature of a statistical problem (with applications to second order efficiency)</span>,”</span> <em>The Annals of Statistics</em>, 3, 1189–1242.
</div>
<div id="ref-Efron2010" class="csl-entry" role="listitem">
Efron, B. (2010), <em><span class="nocase">Large Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction</span></em>, Institute of mathematical statistics monographs, Leiden: Cambridge University Press.
</div>
<div id="ref-Efron2016" class="csl-entry" role="listitem">
Efron, B., and Hastie, T. (2016), <em><span>Computer Age Statistical Inference</span></em>, Cambridge University Press.
</div>
<div id="ref-Efron1978" class="csl-entry" role="listitem">
Efron, B., and Hinkley, D. V. (1978), <span>“Assessing the accuracy of the maximum likelihood estimator: Observed vs fisher information,”</span> <em>Biometrika</em>, 65, 457–482.
</div>
<div id="ref-Efron1975" class="csl-entry" role="listitem">
Efron, B., and Morris, C. (1975), <span>“<span class="nocase">Data analysis using Stein’s estimator and its generalizations</span>,”</span> <em>Journal of the American Statistical Association</em>, 70, 311–319.
</div>
<div id="ref-Firth1993" class="csl-entry" role="listitem">
Firth, D. (1993), <span>“Bias reduction of maximum likelihood estimates,”</span> <em>Biometrika</em>, 80, 27–38.
</div>
<div id="ref-Godambe1960" class="csl-entry" role="listitem">
Godambe, V. P. (1960), <span>“<span class="nocase">An optimum property of regular maximum likelihood estimation</span>,”</span> <em>Annals of Mathematical Statistics</em>, 31, 1208–1211.
</div>
<div id="ref-Huber1967" class="csl-entry" role="listitem">
Huber, P. J. (1967), <span>“The behavior of maximum likelihood estimates under nonstandard conditions,”</span> in <em>Proceedings of the fifth berkeley symposium on mathematical statistics and probability</em>.
</div>
<div id="ref-James1961" class="csl-entry" role="listitem">
James, W., and Stein, C. (1961), <span>“Estimation with quadratic loss,”</span> in <em>Proc. Fourth berkeley symposium</em>, <span>University of California Press</span>, pp. 361–380.
</div>
<div id="ref-Lehmann1998" class="csl-entry" role="listitem">
Lehmann, E. L., and Casella, G. (1998), <em><span class="nocase">Theory of Point Estimation, Second Edition</span></em>, Springer.
</div>
<div id="ref-Pace1997" class="csl-entry" role="listitem">
Pace, L., and Salvan, A. (1997), <em><span class="nocase">Principles of statistical inference from a Neo-Fisherian perspective</span></em>, Advanced series on statistical science and applied probability, World Scientific.
</div>
<div id="ref-Robert1994" class="csl-entry" role="listitem">
Robert, C. P. (1994), <em><span class="nocase">The Bayesian Choice: from decision-theoretic foundations to computational implementation</span></em>, Springer.
</div>
<div id="ref-Stein1956" class="csl-entry" role="listitem">
Stein, C. (1956), <span>“Inadmissibility of the usual estimator of the mean of a multivariate normal distribution,”</span> in <em><span>Proc. Third Berkeley Symposium</span></em>, <span>University of California Press</span>, pp. 197–206.
</div>
<div id="ref-vandervaart1998" class="csl-entry" role="listitem">
van der Vaart, A. W. (1998), <em><span>Asymptotic Statistics</span></em>, Cambridge University Press.
</div>
<div id="ref-White1980" class="csl-entry" role="listitem">
White, H. (1980), <span>“<span class="nocase">A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity</span>,”</span> <em>Econometrica</em>, 4, 817–838.
</div>
</div>
</section>
</section>


</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<script src="un_A_files/libs/quarto-html/zenscroll-min.js"></script>
</body></html>