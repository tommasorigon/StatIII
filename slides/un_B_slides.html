<!DOCTYPE html>
<html lang="en"><head>
<script src="un_B_files/libs/clipboard/clipboard.min.js"></script>
<script src="un_B_files/libs/quarto-html/tabby.min.js"></script>
<script src="un_B_files/libs/quarto-html/popper.min.js"></script>
<script src="un_B_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="un_B_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="un_B_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="un_B_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <meta name="author" content="Tommaso Rigon">
  <title>Generalized Linear Models</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="un_B_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="un_B_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="un_B_files/libs/revealjs/dist/theme/quarto-52f9c9ea99cef3131c4880afbe91edc6.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">
  <link href="un_B_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="un_B_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="un_B_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="un_B_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Generalized Linear Models</h1>
  <p class="subtitle">Statistics III - CdL SSE</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<span class="orange">Tommaso Rigon</span> 
</div>
        <p class="quarto-title-affiliation">
            <em>Università degli Studi di Milano-Bicocca</em>
          </p>
    </div>
</div>

</section>
<section id="homepage" class="slide level2 center">
<h2><a href="../index.html">Homepage</a></h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><img data-src="img/gaussian.png"></p>
</div><div class="column" style="width:70%;">
<ul>
<li><p>This unit will cover the following <span class="orange">topics</span>:</p>
<ul>
<li>Exponential dispersion families</li>
<li>Likelihood, inference, and testing</li>
<li>Iteratively Re-weighted Least Squares (IRLS)</li>
<li>Deviance, model checking, and residuals</li>
<li>Model selection</li>
</ul></li>
<li><p>GLMs are regression models with a linear predictor, where the response variable follows an <span class="blue">exponential dispersion family</span>.</p></li>
<li><p>The symbol 📖 means that a few extra steps are discussed in the <span class="blue">handwritten notes</span>.</p></li>
</ul>
</div></div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The content of this Unit is covered in <span class="orange">Chapter 2</span> of <span class="citation" data-cites="Salvan2020">Salvan et al. (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span>. Alternatively, see <span class="blue">Chapter 4</span> of <span class="citation" data-cites="Agresti2015">Agresti (<a href="#/references" role="doc-biblioref" onclick="">2015</a>)</span> or <span class="grey">Chapter 6</span> of <span class="citation" data-cites="Azzalini2008">Azzalini (<a href="#/references" role="doc-biblioref" onclick="">2008</a>)</span>.</p>
</div>
</div>
</div>
</section>
<section>
<section id="introduction" class="title-slide slide level1 center">
<h1>Introduction</h1>

</section>
<section id="preliminaries" class="slide level2 center">
<h2>Preliminaries</h2>
<ul>
<li><p>GLMs are a <span class="blue">class</span> of <span class="blue">regression models</span> in which a <span class="orange">response</span> random variable <span class="math inline">Y_i</span> is modeled as a function of a vector of <span class="grey">covariates</span> <span class="math inline">\bm{x}_i \in \mathbb{R}^p</span>.</p></li>
<li><p>The random variables <span class="math inline">Y_i</span> are not restricted to be Gaussian. For example:</p>
<ul>
<li><span class="math inline">Y_i \in \{0,1\}</span>, known as <span class="blue">binary regression</span><br>
</li>
<li><span class="math inline">Y_i \in \{0,1,\dots\}</span>, known as <span class="orange">count regression</span><br>
</li>
<li><span class="math inline">Y_i \in (0,\infty)</span> or <span class="math inline">Y_i \in (-\infty,\infty)</span></li>
</ul></li>
<li><p>Gaussian linear models are a special case of GLMs, arising when <span class="math inline">Y_i \in (-\infty,\infty)</span>.</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>The <span class="orange">response random variables</span> are collected in the random vector <span class="math inline">\bm{Y} = (Y_1,\dots,Y_n)^T</span>, whose <span class="blue">observed realization</span> is <span class="math inline">\bm{y} = (y_1,\dots,y_n)^T</span>.</p></li>
<li><p>The <span class="blue">design matrix</span> <span class="math inline">\bm{X}</span> is an <span class="math inline">n \times p</span> <span class="orange">non-stochastic</span> matrix containing the covariate values. The <span class="math inline">j</span>th variable (column) is denoted by <span class="math inline">\tilde{\bm{x}}_j</span>, while the <span class="math inline">i</span>th observation (row) is <span class="math inline">\bm{x}_i</span>.</p></li>
<li><p>We assume that <span class="math inline">\bm{X}</span> has <span class="orange">full rank</span>, that is, <span class="math inline">\text{rk}(\bm{X}) = p</span> with <span class="math inline">p \le n</span>.</p></li>
</ul>
</div>
</section>
<section id="beetles-data-from-bliss-1935" class="slide level2 center">
<h2><code>Beetles</code> data, from Bliss (1935)</h2>
<ul>
<li>The <code>Beetles</code> dataset originates from Bliss (1935). It records the number of adult flour beetles that died after a 5-hour exposure to gaseous carbon disulphide.</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: right;">n</th>
<th style="text-align: right;">deaths</th>
<th style="text-align: right;">logdose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">59</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">1.6907</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">1.7242</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">1.7552</td>
</tr>
<tr class="even">
<td style="text-align: right;">56</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">1.7842</td>
</tr>
<tr class="odd">
<td style="text-align: right;">63</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">1.8113</td>
</tr>
<tr class="even">
<td style="text-align: right;">59</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">1.8369</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">1.8610</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">1.8839</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li><p>We aim to predict the proportion of <code>deaths</code> as a function of <code>logdose</code>.</p></li>
<li><p>Modeling death proportions directly with <span class="orange">linear models</span> is <span class="orange">inappropriate</span>. A <span class="blue">variable transformation</span> provides a more <span class="blue">principled</span> solution, but it comes with <span class="orange">drawbacks</span>.</p></li>
</ul>
</section>
<section id="beetles-data-a-dose-response-plot" class="slide level2 center">
<h2><code>Beetles</code> data, a dose-response plot</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-3-1.png" class="quarto-figure quarto-figure-center r-stretch" width="1950"><ul>
<li>There is a clear <span class="orange">positive</span> and <span class="blue">non-linear</span> pattern between the <span class="orange">proportion of deaths</span> as a function of the logdose. The response variable take values in <span class="math inline">[0, 1]</span>.</li>
</ul>
</section>
<section id="modelling-the-beetles-data" class="slide level2 center">
<h2>Modelling the <code>Beetles</code> data</h2>
<ul>
<li><p>Let <span class="math inline">Y_i</span> be the number of dead beetles out of <span class="math inline">m_i</span>, and let <span class="math inline">x_i</span> denote the log-dose. By definition, <span class="math inline">S_i \in \{0, 1, \dots, m_i\}</span> for <span class="math inline">i = 1,\dots,8</span>.</p></li>
<li><p>It is natural to model each <span class="math inline">Y_i</span> as <span class="blue">independent binomial</span> random variables, counting the number of deaths out of <span class="math inline">m_i</span> individuals. In other words: <span class="math display">
S_i \overset{\text{ind}}{\sim} \text{Binomial}(m_i, \pi_i), \qquad i = 1,\dots,8,
</span> where <span class="math inline">\pi_i</span> is the <span class="orange">probability</span> of death at a given dose <span class="math inline">x_i</span>. Moreover, we have <span class="math display">
\mathbb{E}\left(\frac{S_i}{m_i}\right)  = \pi_i = \mu_i.
</span></p></li>
<li><p>A modeling approach, called <span class="blue">logistic regression</span>, specifies:<br>
<span class="math display">
g(\pi_i) = \log\left(\frac{\pi_i}{1 - \pi_i}\right) = \beta_1 + \beta_2 x_i \quad \Longrightarrow \quad \pi_i = g^{-1}(\beta_1 + \beta_2 x_i) = \frac{\exp(\beta_1 + \beta_2 x_i)}{1 + \exp(\beta_1 + \beta_2 x_i)}.
</span> for some parameters <span class="math inline">\beta_1, \beta_2 \in \mathbb{R}</span>. <span class="orange">Note</span> that <span class="math inline">\pi_i \in (0, 1)</span> by construction.</p></li>
</ul>
</section>
<section id="beetles-data-fitted-model" class="slide level2 center">
<h2><code>Beetles</code> data, fitted model</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-4-1.png" class="quarto-figure quarto-figure-center r-stretch" width="1950"><ul>
<li>The <span class="orange">maximum likelihood</span> estimates are <span class="math inline">\hat{\beta}_1 = -60.72</span> and <span class="math inline">\hat{\beta}_2 = 34.3</span>. This yields the <span class="blue">predictive curve</span> <span class="math inline">\hat{\pi}(x) = g^{-1}(\hat{\beta}_1 + \hat{\beta}_2 x),</span> which estimates the mean proportion <span class="math inline">\mathbb{E}(S_i / m_i)</span>.</li>
</ul>
</section>
<section id="a-comparison-with-old-tools-i" class="slide level2 center">
<h2>A comparison with old tools I 📖</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y_i = S_i / m_i</span> be the proportion of deaths. A direct application of linear models implies: <span class="math display">
Y_i = \beta_1 + \beta_2 x_i + \epsilon_i.
</span> The coefficients <span class="math inline">\beta_1</span> and <span class="math inline">\beta_2</span> are then estimated using OLS using <span class="math inline">Y_i</span> as response.</p>
</div>
</div>
</div>
<ul>
<li><p>The prediction <span class="math inline">\hat{\beta}_1 + \hat{\beta}_2 x_i</span> is <span class="orange">unrestricted</span>, meaning it could produce values like “1.3” or “-2” as estimated <span class="blue">proportions</span>, which is clearly undesirable.</p></li>
<li><p>The <span class="blue">additive structure</span> <span class="math inline">Y_i = \beta_1 + \beta_2 x_i + \epsilon_i</span> cannot hold with <span class="blue">iid</span> errors <span class="math inline">\epsilon_i</span>, because <span class="math inline">S_i</span>, and thus <span class="math inline">Y_i</span>, are <span class="orange">discrete</span>. As a result, the errors are always <span class="orange">heteroschedastic</span>.</p></li>
<li><p>If <span class="math inline">m_i = 1</span>, i.e.&nbsp;when the data are <span class="orange">binary</span>, all the above issues are <span class="orange">exacerbated</span>.</p></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>This approach is sometimes called the <span class="blue">linear probability model</span>. Before GLMs, it was considered acceptable despite its issues, but by modern standards it should <span class="orange">not be used</span>.</p>
</div>
</div>
</div>
</section>
<section id="a-comparison-with-old-tools-ii" class="slide level2 center">
<h2>A comparison with old tools II 📖</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>We consider the <span class="blue">empirical logit</span> variable transformation of <span class="math inline">S_i = Y_i / m_i</span>, obtaining<br>
<span class="math display">
\text{logit}(\tilde{Y}_i) = \log\left(\frac{S_i + 0.5}{m_i - S_i + 0.5}\right) = \beta_1 + \beta_2 x_i + \epsilon_i, \qquad \tilde{Y}_i = \frac{S_i + 0.5}{m_i +1}.
</span> A correction term is necessary because otherwise <span class="math inline">g(\cdot) = \text{logit}(\cdot)</span> is undefined. The predictions belong to <span class="math inline">(0, 1)</span>, since <span class="math display">
\hat{\pi}_i = g^{-1}[\mathbb{E}\{g(\tilde{Y}_i)\}] = g^{-1}(\hat{\beta}_1 + \hat{\beta}_2 x_i) = \frac{\exp(\hat{\beta}_1 + \hat{\beta}_2 x_i)}{1 + \exp(\hat{\beta}_1 + \hat{\beta}_2 x_i)},</span> in which <span class="math inline">\hat{\beta}_1</span> and <span class="math inline">\hat{\beta}_2</span> are estimated with OLS using <span class="math inline">\text{logit}(\tilde{Z}_i)</span> as response.</p>
</div>
</div>
</div>
<ul>
<li><p>The <span class="blue">interpretation</span> of <span class="math inline">\hat{\beta}</span> is less clear, as they refer to the mean of <span class="math inline">\text{logit}(\tilde{Y}_i)</span> instead of <span class="math inline">\mathbb{E}(Y_i)</span>.</p></li>
<li><p>An arbitrary <span class="orange">boundary correction</span> is needed.</p></li>
<li><p>Inference is problematic and requires further corrections, because of <span class="orange">heteroschedastic</span> errors.</p></li>
<li><p>This approach is <span class="orange">not compatible</span> with the reasonable assumption <span class="math inline">S_i \sim \text{Binomial}(m_i, \pi_i)</span>.</p></li>
</ul>
</section>
<section id="a-comparison-with-old-tools-iii" class="slide level2 center">
<h2>A comparison with old tools III</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-5-1.png" class="quarto-figure quarto-figure-center r-stretch" width="1950"><ul>
<li>The black line is the predicted curve of a <span class="grey">logistic regression GLM</span>. The orange line is the predictived curve of a <span class="orange">linear model</span>. The blue line is the predictive curve of a <span class="blue">linear model</span> after an <span class="blue">empirical logit variable transformation</span>.</li>
</ul>
</section>
<section id="aids-data" class="slide level2 center">
<h2><code>Aids</code> data</h2>
<ul>
<li>Number of AIDS <code>deaths</code> in Australia in a sequence of three-months periods between 1983 and 1986.</li>
</ul>
<div class="smaller">
<div class="cell">
<div class="cell-output-display">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">1983-1</th>
<th style="text-align: right;">1984-1</th>
<th style="text-align: right;">1985-1</th>
<th style="text-align: right;">1986-1</th>
<th style="text-align: right;">1983-2</th>
<th style="text-align: right;">1984-2</th>
<th style="text-align: right;">1985-2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">deaths</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="even">
<td style="text-align: left;">period</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">7</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output-display">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">1986-2</th>
<th style="text-align: right;">1983-3</th>
<th style="text-align: right;">1984-3</th>
<th style="text-align: right;">1985-3</th>
<th style="text-align: right;">1986-3</th>
<th style="text-align: right;">1983-4</th>
<th style="text-align: right;">1984-4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">deaths</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">23</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">37</td>
<td style="text-align: right;">45</td>
</tr>
<tr class="even">
<td style="text-align: left;">period</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">14</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<ul>
<li><p>We are interested in predicting the number of <code>deaths</code> as a function of the <code>period</code> of time.</p></li>
<li><p>The response variable <span class="math inline">Y_i \in \{0, 1, \dots\}</span> is a non-negative <span class="orange">count</span>.</p></li>
</ul>
</section>
<section id="aids-data-scatter-plot" class="slide level2 center">
<h2><code>Aids</code> data, scatter plot</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-7-1.png" class="quarto-figure quarto-figure-center r-stretch" width="1950"><ul>
<li>There is a clear positive association between period and deaths. However, the increase appears to be <span class="orange">faster</span> than <span class="orange">linear</span>. Note that both the mean and the <span class="blue">variability</span> of <span class="math inline">Y_i</span> increase over time.</li>
</ul>
</section>
<section id="modelling-the-aids-data" class="slide level2 center">
<h2>Modelling the <code>Aids</code> data</h2>
<ul>
<li>Let <span class="math inline">Y_i</span> be the number of deaths, and let <span class="math inline">x_i</span> denote the period. By definition, <span class="math inline">Y_i \in \{0, 1, \dots\}</span> are non-negative counts, for <span class="math inline">i = 1,\dots,14</span>.</li>
</ul>
<div class="fragment">
<ul>
<li><p>We model <span class="math inline">Y_i</span> as <span class="blue">independent Poisson</span> random variables, counting the number of deaths: <span class="math display">
Y_i \overset{\text{ind}}{\sim} \text{Poisson}(\mu_i), \qquad i = 1,\dots,14,
</span> where <span class="math inline">\mu_i</span> is the <span class="orange">mean</span> of <span class="math inline">Y_i</span>, namely <span class="math inline">\mathbb{E}(Y_i) = \mu_i</span>.</p></li>
<li><p>A modeling approach, called <span class="blue">Poisson regression</span>, specifies:<br>
<span class="math display">
g(\mu_i) = \log(\mu_i) = \beta_1 + \beta_2 x_i \quad \Longrightarrow \quad \mu_i = g^{-1}(\beta_1 + \beta_2 x_i) = \exp(\beta_1 + \beta_2 x_i),
</span> for some parameters <span class="math inline">\beta_1, \beta_2 \in \mathbb{R}</span>. <span class="orange">Note</span> that <span class="math inline">\mu_i &gt; 0</span> by construction.</p></li>
<li><p>Under this specification, the <span class="blue">variances</span> of the observations are<br>
<span class="math display">
\text{var}(Y_i) = \mu_i = \exp(\beta_1 + \beta_2 x_i),
</span> which increases with <span class="math inline">x</span>, as desired. This implies that <span class="math inline">Y_1,\dots,Y_n</span> are <span class="orange">heteroschedastic</span>, but this is not an issue in GLMs, as this aspect is <span class="blue">automatically accounted</span> for.</p></li>
</ul>
</div>
</section>
<section id="aids-data-fitted-model" class="slide level2 center">
<h2><code>Aids</code> data, fitted model</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-8-1.png" class="quarto-figure quarto-figure-center r-stretch" width="1950"><ul>
<li>The <span class="orange">maximum likelihood</span> estimates are <span class="math inline">\hat{\beta}_1 = 0.30</span> and <span class="math inline">\hat{\beta}_2 = 0.26</span>. This yields the <span class="blue">predictive curve</span> <span class="math inline">\hat{\mu}(x) = \exp(\hat{\beta}_1 + \hat{\beta}_2 x),</span> which estimates the mean <span class="math inline">\mathbb{E}(Y_i)</span>.</li>
</ul>
</section>
<section id="a-comparison-with-old-tools-i-1" class="slide level2 center">
<h2>A comparison with old tools I</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>We consider the <span class="blue">variance-stabilizing</span> transformation <span class="math inline">S_i = \sqrt{Y_i}</span>, obtaining<br>
<span class="math display">
\sqrt{Y_i} = \beta_1 + \beta_2 x_i + \epsilon_i.
</span> The predictions belong to <span class="math inline">(0, \infty)</span>, since <span class="math display">
\hat{\mu}_i = \mathbb{E}(\sqrt{Y_i})^2 = (\hat{\beta}_1 + \hat{\beta}_2 x_i)^2,</span> in which <span class="math inline">\hat{\beta}_1</span> and <span class="math inline">\hat{\beta}_2</span> are estimated with OLS using <span class="math inline">\sqrt{Y_i}</span> as response.</p>
</div>
</div>
</div>
<ul>
<li><p>The <span class="blue">interpretation</span> of <span class="math inline">\hat{\beta}</span> is less clear, as they refer to the mean of <span class="math inline">\sqrt{Y}_i</span> instead of <span class="math inline">\mathbb{E}(Y_i)</span>.</p></li>
<li><p>This approach is <span class="orange">not compatible</span> with the reasonable assumption <span class="math inline">Y_i \sim \text{Poisson}(\mu_i)</span> and it only valid as an <span class="blue">asymptotic approximation</span>.</p></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>To compare such a model with a similar specification, we also fit another Poisson GLM in which <span class="math display">
Y_i \overset{\text{ind}}{\sim} \text{Poisson}(\mu_i), \qquad \sqrt{\mu_i} = \beta_1 + \beta_2 x_i, \qquad i=1,\dots,14.
</span></p>
</div>
</div>
</div>
</section>
<section id="a-comparison-with-old-tools-ii-1" class="slide level2 center">
<h2>A comparison with old tools II</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-9-1.png" class="quarto-figure quarto-figure-center r-stretch" width="1950"><ul>
<li>The black line is the predicted curve of a <span class="grey">Poisson regression GLM</span> with <span class="gray">logarithmic link</span>. The orange line is the predicted curve of a <span class="orange">linear model</span> with a <span class="orange">square-root transformation</span>. The blue line is the predictive curve of a <span class="blue">Poisson regression GLM</span> with <span class="blue">square-root link</span>.</li>
</ul>
</section>
<section id="the-components-of-a-glm" class="slide level2 center">
<h2>The components of a GLM</h2>
<ul>
<li><span class="orange">Random component</span>. This specifies the probability distribution response variable <span class="math inline">Y_i</span>. The observations <span class="math inline">\bm{y} =(y_1,\dots,y_n)</span> on that distribution are treated as <span class="orange">independent</span>.</li>
</ul>
<div class="fragment">
<ul>
<li><span class="blue">Linear predictor</span>. For a parameter vector <span class="math inline">\bm{\beta} = (\beta_1,\dots,\beta_p)^T</span> and an <span class="math inline">n \times p</span> design matrix <span class="math inline">\bm{X}</span>, the linear predictor is <span class="math inline">\bm{\eta} = \bm{X}\beta</span>. We will also write <span class="math display">
\eta_i = \bm{x}_i^T\beta = \beta_1x_{i1} + \cdots + x_{ip}\beta_p, \qquad i=1,\dots,n.
</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li><span class="grey">Link function</span>. This is an invertible and differentiable function <span class="math inline">g(\cdot)</span> applied to each component of the <span class="grey">mean</span> <span class="math inline">\mu_i = \mathbb{E}(Y_i)</span> that relates it to the linear predictor: <span class="math display">
g(\mu_i) = \eta_i = \bm{x}_i^T\beta, \qquad \Longrightarrow \qquad \mu_i = g^{-1}(\eta_i) = g^{-1}(\bm{x}_i^T\beta).
</span></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Note that, in general, we <span class="orange">cannot</span> express the response in an additive way <span class="math inline">Y_i = g^{-1}(\eta_i) + \epsilon_i</span>.</p>
</div>
</div>
</div>
</div>
</section>
<section id="random-component-of-a-glm" class="slide level2 center">
<h2>Random component of a GLM</h2>
<ul>
<li><p>In GLMs the random variables <span class="math inline">Y_i</span> are <span class="orange">independent</span> and they are distributed according to an <span class="blue">exponential dispersion family</span>, whose definition will be provided in a few slides.</p></li>
<li><p>For now, it suffices to say that the distributions most commonly used in Statistics, such as the normal, binomial, gamma, and Poisson, are exponential family distributions.</p></li>
<li><p>Exponential dispersion families are <span class="orange">characterized</span> by their <span class="blue">mean</span> and <span class="blue">variance</span>. Let <span class="math inline">v(\mu) &gt; 0</span> be a function of the mean, called <span class="blue">variance function</span> and let <span class="math inline">a_i(\phi) &gt;0</span> be functions of an additional unknown parameter <span class="math inline">\phi &gt; 0</span> called <span class="orange">dispersion</span>.</p></li>
</ul>
<div class="fragment">
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>In a GLMs the observations are independent draws from a distribution <span class="math inline">\text{ED}(\mu_i, a_i(\phi)v(\mu_i))</span>: <span class="math display">
Y_i \overset{\text{ind}}{\sim} \text{ED}(\mu_i, a_i(\phi)v(\mu_i)), \qquad \mathbb{E}(Y_i) = \mu_i, \qquad g(\mu_i) = \bm{x}_i^T\beta,
</span> with <span class="math inline">\mu_i \in \mathcal{M}</span>. Moreover, the <span class="orange">variance</span> is connected to the <span class="blue">mean</span> via <span class="math inline">v(\mu)</span>: <span class="math display">
\text{var}(Y_i) = a_i(\phi) v(\mu_i),
</span> where <span class="math inline">a_i(\phi) = \phi / \omega_i</span> and <span class="math inline">\omega_i</span> are <span class="blue">known weights</span>. Special cases are <span class="math inline">a_i(\phi) = \phi</span> and <span class="math inline">a_i(\phi) = 1</span>.</p>
</div>
</div>
</div>
</div>
</section>
<section id="notable-examples" class="slide level2 center">
<h2>Notable examples</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>In a <span class="blue">Gaussian linear model</span> we consider the <span class="blue">identity link</span> <span class="math inline">g(\mu) = \mu</span> and let <span class="math display">
Y_i \overset{\text{ind}}{\sim}\text{N}(\mu_i, \sigma^2), \qquad \mu_i = \bm{x}_i^T\beta.
</span> The unknown variance <span class="math inline">\sigma^2 = \phi</span> is called <span class="orange">dispersion</span> in GLMs. The <span class="orange">parameter space</span> is <span class="math inline">\mathcal{M} = \mathbb{R}</span>, whereas <span class="math inline">a_i(\phi) = \phi</span> and the variance function is <span class="blue">constant</span> <span class="math inline">v(\mu) = 1</span> (homoschedasticity).</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>In a <span class="blue">binomial regression model</span> with <span class="blue">logit link</span> <span class="math inline">g(\mu) = \text{logit}(\mu)</span> we let <span class="math inline">Y_i = S_i/m_i</span> and <span class="math display">
S_i \overset{\text{ind}}{\sim}\text{Binomial}(m_i, \pi_i),\qquad \mathbb{E}\left(Y_i\right) = \pi_i = \mu_i, \qquad \text{logit}(\mu_i) =  \bm{x}_i^T\beta.
</span> We have <span class="math inline">a_i(\phi) = 1/m_i</span> and <span class="math inline">v(\mu) = \mu(1-\mu)</span>. There is <span class="orange">no dispersion</span> parameter.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>In <span class="blue">Poisson regression</span> with <span class="blue">logarithmic link</span> <span class="math inline">g(\mu) = \log(\mu)</span> we let <span class="math display">
Y_i \overset{\text{ind}}{\sim}\text{Poisson}(\mu), \qquad \log(\mu_i) =  \bm{x}_i^T\beta.
</span> We have <span class="math inline">a_i(\phi) = 1</span> and <span class="math inline">v(\mu) = \mu</span>. There is <span class="orange">no dispersion</span> parameter.</p>
</div>
</div>
</div>
</section></section>
<section>
<section id="exponential-dispersion-families" class="title-slide slide level1 center">
<h1>Exponential dispersion families</h1>

</section>
<section id="overview" class="slide level2 center">
<h2>Overview</h2>
<ul>
<li>Figure 1 of <span class="citation" data-cites="Efron2023">Efron (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span>. <span class="grey">Three level</span> of statistical modeling.</li>
</ul>

<img data-src="img/EF.png" class="quarto-figure quarto-figure-center r-stretch" style="width:6in"><ul>
<li><p>The <span class="orange">prime role</span> of <span class="orange">exponential families</span> in the theory of statistical inference was first emphasized by <span class="citation" data-cites="Fisher1934">Fisher (<a href="#/references" role="doc-biblioref" onclick="">1934</a>)</span>.</p></li>
<li><p>Most <span class="blue">well-known</span> <span class="blue">distributions</span>—such as Gaussian, Poisson, Binomial, and Gamma—are instances of exponential families.</p></li>
</ul>
</section>
<section id="definition" class="slide level2 center">
<h2>Definition</h2>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The <span class="orange">density</span> of <span class="math inline">Y_i</span> belongs to an <span class="blue">exponential dispersion family</span> if it can be written as <span class="math display">
p(y_i; \theta_i, \phi) = \exp\left\{\frac{\theta_i y_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi)\right\},
</span> where <span class="math inline">y_i \in \mathcal{Y} \subseteq \mathbb{R}</span>, <span class="math inline">\theta_i \in \Theta \subseteq\mathbb{R}</span> and <span class="math inline">a_i(\phi) = \phi / \omega_i</span> where <span class="math inline">\omega_i</span> are <span class="blue">known positive weights</span>. The parameter <span class="math inline">\theta_i</span> is called <span class="orange">natural parameter</span> while <span class="math inline">\phi</span> is called <span class="blue">dispersion</span> parameter.</p>
</div>
</div>
</div>
<ul>
<li><p>By specifying the functions <span class="math inline">a_i(\cdot), b(\cdot)</span> and <span class="math inline">c(\cdot)</span> one obtain a particular <span class="orange">parametric model</span>.</p></li>
<li><p>The support <span class="math inline">\mathcal{Y}</span> of <span class="math inline">Y_i</span> does not depend on the parameters <span class="math inline">\phi</span> or <span class="math inline">\theta_i</span> and <span class="math inline">b(\cdot)</span> can be <span class="blue">differentiated</span> infinitely many times. In particular, this is a <span class="orange">regular statistical model</span>.</p></li>
<li><p>As mentioned, special cases are <span class="math inline">a_i(\phi) = \phi</span> and <span class="math inline">a_i(\phi) = 1</span>. When <span class="math inline">a_i(\phi) = 1</span> and <span class="math inline">c(y_i, \phi) = c(y_i)</span> we obtain <span class="math display">
p(y_i; \theta_i) = \exp\left\{\theta_i y_i - b(\theta_i) + c(y_i)\right\},
</span> which is called <span class="orange">natural exponential family</span> of order 1.</p></li>
</ul>
</section>
<section id="mean-and-variance-i" class="slide level2 center">
<h2>Mean and variance I 📖</h2>
<ul>
<li><p>Let us consider the <span class="orange">log-likelihood</span> contribution of the <span class="math inline">i</span>th observations, which is defined as <span class="math display">
\ell(\theta_i, \phi; y_i) = \log{p(y_i; \theta_i, \phi)} = \frac{\theta_i y_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi).
</span> If you prefer, this is the log-likelihood when the sample size <span class="math inline">n = 1</span> and we only observe <span class="math inline">Y_i</span>.</p></li>
<li><p>The <span class="blue">score</span> and <span class="orange">hessian</span> functions, namely the first and second derivative over <span class="math inline">\theta_i</span> are <span class="math display">
\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; y_i) = \frac{y_i - b'(\theta_i)}{a_i(\phi)}, \qquad \frac{\partial^2}{\partial \theta_i^2}\ell(\theta_i, \phi; y_i) = \frac{-b''(\theta_i)}{a_i(\phi)}.
</span> where <span class="math inline">b'(\cdot)</span> and <span class="math inline">b''(\cdot)</span> denote the <span class="blue">first</span> and <span class="orange">second</span> derivative of <span class="math inline">b(\cdot)</span>.</p></li>
<li><p>Recall the following <span class="grey">Bartlett identities</span>, valid in any <span class="orange">regular</span> statistical model: <span class="math display">
\begin{aligned}
\mathbb{E}\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right) &amp;= 0, \\
\mathbb{E}\left\{\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right)^2\right\} = \text{var}\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right) &amp;= \mathbb{E}\left(-\frac{\partial^2}{\partial \theta_i^2}\ell(\theta_i, \phi; Y_i)\right).
\end{aligned}
</span></p></li>
</ul>
</section>
<section id="mean-and-variance-ii" class="slide level2 center">
<h2>Mean and variance II 📖</h2>
<ul>
<li>Specializing Bartlett identities in <span class="blue">exponential dispersion families</span>, we obtain <span class="math display">
\mathbb{E}\left(\frac{Y_i - b'(\theta_i)}{a_i(\phi)}\right) = 0, \qquad \text{var}\left(\frac{Y_i - b'(\theta_i)}{a_i(\phi)}\right) = \frac{\text{var}(Y_i)}{a_i(\phi)^2} = \frac{b''(\theta_i)}{a_i(\phi)}.
</span> Re-arranging the terms, we finally get the following key result.</li>
</ul>
<div class="callout callout-warning no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y_i</span> be an exponential dispersion family, identified by the functions <span class="math inline">a_i(\cdot), b(\cdot)</span> and <span class="math inline">c(\cdot)</span>, and with natural parameter <span class="math inline">\theta_i</span>. Then the <span class="orange">mean</span> and the <span class="blue">variance</span> of <span class="math inline">Y_i</span> equal <span class="math display">
\mathbb{E}(Y_i) =  b'(\theta_i), \qquad \text{var}(Y_i) = a_i(\phi) b''(\theta_i).
</span></p>
</div>
</div>
</div>
<ul>
<li><p>The mean <span class="math inline">\mu_i = b'(\theta_i)</span> does <span class="orange">not</span> depend on the <span class="orange">dispersion</span> parameter.</p></li>
<li><p>We have <span class="math inline">b''(\cdot) &gt; 0</span> because <span class="math inline">\text{var}(Y_i)</span>, which means that <span class="math inline">b(\cdot)</span> is a <span class="blue">convex function</span>.</p></li>
<li><p>Moreover, the function <span class="math inline">b'(\theta)</span> is <span class="orange">continuous</span> and <span class="orange">monotone increasing</span> and hence <span class="orange">invertible</span>.</p></li>
</ul>

<aside><div>
<p>The function <span class="math inline">b(\cdot)</span> is related to the moment generating function of <span class="math inline">Y_i</span>. Thus, higher order derivatives of <span class="math inline">b(\cdot)</span> allows the calculations of skewness, kurtosis, etc.</p>
</div></aside></section>
<section id="mean-parametrization-variance-function" class="slide level2 center">
<h2>Mean parametrization, variance function</h2>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y_i</span> be an exponential dispersion family, identified by the functions <span class="math inline">a_i(\cdot), b(\cdot)</span> and <span class="math inline">c(\cdot)</span>, and with <span class="orange">natural parameter</span> <span class="math inline">\theta_i</span>, then <span class="math display">
\mu(\theta_i):= \mu_i = \mathbb{E}(Y_i) = b'(\theta_i).
</span> The function <span class="math inline">\mu(\cdot) : \Theta \to\mathcal{M}</span> is <span class="blue">one-to-one</span> and invertible, that is, a <span class="orange">reparametrization</span> of <span class="math inline">\theta_i</span>. We call <span class="math inline">\mu_i</span> the <span class="blue">mean parametrization</span> of an exponential dispersion family.</p>
</div>
</div>
</div>
<div class="fragment">
<ul>
<li><p>The <span class="orange">inverse</span> relationship, re-obtaining <span class="math inline">\theta_i</span> as a function of <span class="math inline">\mu_i</span>, is denoted with <span class="math display">
\theta_i = \theta(\mu_i) = b'^{-1}(\mu_i).
</span></p></li>
<li><p>Using this notation, we can express the variance of <span class="math inline">Y_i</span> as a function of <span class="math inline">\mu_i</span> as follows <span class="math display">
\text{var}(Y_i) = a_i(\phi)b''(\theta_i) =  a_i(\phi)b''(\theta(\mu_i)) = a_i(\phi)v(\mu_i),
</span> where <span class="math inline">v(\mu_i) := b''(\theta(\mu_i))</span> is the <span class="blue">variance function</span>.</p></li>
<li><p>The domain <span class="math inline">\mathcal{M}</span> and the variance function <span class="math inline">v(\mu)</span> <span class="orange">characterize</span> the function <span class="math inline">b(\cdot)</span> and the entire distribution, for any given <span class="math inline">a_i(\phi)</span>. This justifies the <span class="blue">notation</span> <span class="math inline">Y_i \sim \text{ED}(\mu_i, a_i(\phi)v(\mu_i))</span>.</p></li>
</ul>
</div>
</section>
<section id="gaussian-distribution" class="slide level2 center">
<h2>Gaussian distribution 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_i \sim \text{N}(\mu_i, \sigma^2)</span>. The <span class="blue">density</span> function of <span class="math inline">Y_i</span> can be written as <span class="math display">
\begin{aligned}
p(y_i; \mu_i, \sigma^2) &amp;= \frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(y_i - \mu_i)^2\right\}
\\
&amp;=\exp\left\{\frac{y_i \mu_i - \mu_i^2/2}{\sigma^2}- \frac{\log(2\pi\sigma^2)}{2}-\frac{y_i^2}{2\sigma^2}\right\}
\end{aligned}
</span></p></li>
<li><p>Then, we can recognize the following relationships: <span class="math display">
\theta_i = \theta(\mu_i) = \mu_i, \quad a_i(\phi) = \phi = \sigma^2, \quad b(\theta_i) = \frac{\theta_i^2}{2}, \quad c(y_i, \phi) = - \frac{\log(2\pi\phi)}{2}-\frac{y_i^2}{2\phi}.
</span> In the Gaussian case, the <span class="blue">mean parametrization</span> and the <span class="orange">natural parametrization</span> coincide. Moreover, the <span class="orange">dispersion</span> <span class="math inline">\phi</span> coincides with the <span class="blue">variance</span> <span class="math inline">\sigma^2</span>.</p></li>
<li><p>Using the results we previously discussed, we obtain the well-known relationships <span class="math display">
\mathbb{E}(Y_i) = b'(\theta_i) = \theta_i, \qquad \text{var}(Y_i) = a_i(\phi)b''(\theta_i) = \phi.
</span> The <span class="blue">variance function</span> <span class="math inline">v(\mu_i) = 1</span> is <span class="orange">constant</span>. We will write <span class="math inline">Y_i \sim \text{ED}(\mu_i, \phi)</span> with <span class="math inline">\mu_i \in \mathcal{M} = \mathbb{R}</span>.</p></li>
</ul>
</section>
<section id="poisson-distribution" class="slide level2 center">
<h2>Poisson distribution 📖</h2>
<p>Let <span class="math inline">Y_i \sim \text{Poisson}(\mu_i)</span>. The <span class="blue">pdf</span> function of <span class="math inline">Y_i</span> can be written as <span class="math display">
\begin{aligned}
p(y_i; \mu_i) &amp;= \frac{\mu_i^{y_i} e^{-\mu_i}}{y_i!}=\exp\{y_i \log(\mu_i) - \mu_i - \log(y_i!)\} \\
&amp;=\exp\{y_i \theta_i - e^{\theta_i} - \log(y_i!)\}, \qquad y_i = 0, 1, 2,\dots.
\end{aligned}
</span></p>
<ul>
<li><p>Then, we can recognize the following relationships: <span class="math display">
\begin{aligned}
\theta_i &amp;= \theta(\mu_i) = \log(\mu_i), \quad &amp;&amp;a_i(\phi) = 1, \\
b(\theta_i) &amp;= e^{\theta_i}, \quad &amp;&amp;c(y_i, \phi) = c(y_i) = -\log(y_i!).
\end{aligned}
</span> There is <span class="orange">no dispersion</span> parameter since <span class="math inline">a_i(\phi) = 1</span>.</p></li>
<li><p>Using the results we previously discussed, we obtain the well-known relationships <span class="math display">
\begin{aligned}
\mathbb{E}(Y_i) &amp;= b'(\theta_i) = e^{\theta_i} = \mu_i, \\
\text{var}(Y_i) &amp;= a_i(\phi) b''(\theta_i) = e^{\theta_i} = \mu_i.
\end{aligned}
</span> The <span class="blue">variance function</span> <span class="math inline">v(\mu_i) = \mu_i</span> is <span class="orange">linear</span>. We will write <span class="math inline">Y_i \sim \text{ED}(\mu_i, \mu_i)</span> with <span class="math inline">\mu_i \in (0, \infty)</span>.</p></li>
</ul>
</section>
<section id="gamma-distribution-i" class="slide level2 center">
<h2>Gamma distribution I 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_i \sim \text{Gamma}(\alpha, \lambda_i)</span>. The <span class="blue">density</span> function of <span class="math inline">Y_i</span> can be written as <span class="math display">
\begin{aligned}
p(y_i; \alpha, \lambda_i) &amp;= \frac{\lambda_i^\alpha y_i^{\alpha-1}\alpha e^{-\lambda_i y_i}}{\Gamma(\alpha)}
\\
&amp;=\exp\left\{\alpha\log{\lambda_i} - \lambda_i y_i + (\alpha-1)\log{y_i}  - \log{\Gamma}(\alpha)\right\} \\
&amp;=\exp\left\{\alpha\left(\log{\lambda_i} - \frac{\lambda_i}{\alpha} y_i\right) + (\alpha-1)\log{y_i}  - \log{\Gamma}(\alpha)\right\} \\
&amp;=\exp\left\{\frac{\theta_i y_i + \log(-\theta_i)}{\phi} - (1/\phi)\log{\phi}+ (1/\phi - 1)\log{y_i}  - \log{\Gamma}(1/\phi)\right\}, \qquad y &gt; 0,\\
\end{aligned}
</span> having defined the <span class="blue">dispersion</span> <span class="math inline">\phi = 1/\alpha</span> and the <span class="orange">natural parameter</span> <span class="math inline">\theta_i = -\lambda_i/\alpha</span>.</p></li>
<li><p>Then, we can recognize the following relationships: <span class="math display">
\begin{aligned}
\quad a_i(\phi) &amp;= \phi, \qquad b(\theta_i) = - \log(-\theta_i), \\
c(y_i, \phi) &amp;= -  (1/\phi)\log{\phi}+ (1/\phi - 1)\log{y_i}  - \log{\Gamma}(1/\phi).
\end{aligned}
</span></p></li>
</ul>
</section>
<section id="gamma-distribution-ii" class="slide level2 center">
<h2>Gamma distribution II 📖</h2>
<ul>
<li><p>Using the results we previously discussed, we obtain the well-known relationships <span class="math display">
\mathbb{E}(Y_i) = b'(\theta_i) = - \frac{1}{\theta_i} = \frac{\alpha}{\lambda_i} = \mu_i, \qquad \text{var}(Y_i) = a_i(\phi)b''(\theta_i) = \frac{\phi}{\theta_i^2} = \frac{\alpha}{\lambda_i^2}.
</span></p></li>
<li><p>At the same time, we can write the <span class="orange">inverse</span> relationship linking <span class="math inline">\theta_i</span> to the <span class="blue">mean</span> as <span class="math display">
\theta_i = \theta(\mu_i) = - \frac{1}{\mu_i}
</span> from which we finally obtain the following <span class="blue">quadratic</span> variance function <span class="math display">
v(\mu_i) = \mu_i^2.
</span></p></li>
<li><p>We will write <span class="math inline">Y_i \sim \text{ED}(\mu_i, \phi\mu_i^2)</span> with <span class="math inline">\mu_i \in (0, \infty)</span>.</p></li>
</ul>
</section>
<section id="binomial-distribution-i" class="slide level2 center">
<h2>Binomial distribution I 📖</h2>
<ul>
<li>Let <span class="math inline">S_i \sim \text{Binomial}(m_i, \pi_i)</span>, with <span class="math inline">\pi_i \in (0, 1)</span>. The random variable <span class="math inline">Y_i = S_i/m_i</span> has <span class="blue">density</span> <span class="math display">
\begin{aligned}
p(y_i; m_i, \pi_i) &amp;= \binom{m_i}{m_i y_i}\pi_i^{m_i y_i}(1 - \pi_i)^{m_i - m_i y_i}\\
&amp;=\binom{m_i}{m_i y_i}\left(\frac{\pi_i}{1 - \pi_i}\right)^{m_i y_i}(1 - \pi_i)^{m_i}\\
&amp;=\exp\left\{m_iy_i\log\left(\frac{\pi_i}{1 - \pi_i}\right) + m_i\log(1 - \pi_i) + \log\binom{m_i}{m_i y_i}\right\},
\end{aligned}
</span> for <span class="math inline">y_i \in \{0, 1/m_i, 2/m_2, \dots, m_i/m_i\}</span>. This can be written as <span class="math display">
p(y_i; m_i, \pi_i) =\exp\left\{\frac{y_i\theta_i - \log\{1 + \exp(\theta_i)\}}{1/m_i}+ \log\binom{m_i}{m_i y_i}\right\},
</span> where the <span class="orange">natural parameter</span> is <span class="math inline">\theta_i = \text{logit}(\pi_i) = \log\{\pi/(1-\pi_i)\}</span>.</li>
</ul>
</section>
<section id="binomial-distribution-ii" class="slide level2 center">
<h2>Binomial distribution II 📖</h2>
<ul>
<li><p>Note that <span class="math inline">\mathbb{E}(Y_i) = \mathbb{E}(Z_i / m_i) = \pi_i = \mu_i</span>. This means there <span class="orange">no dispersion</span> parameter <span class="math inline">\phi</span> and <span class="math display">
\theta_i = \text{logit}(\mu_i), \quad a_i(\phi) = \frac{1}{m_i}, \quad b(\theta_i) = \log\{1 + \exp(\theta_i)\}, \quad c(y_i) = \log\binom{m_i}{m_i y_i}.
</span></p></li>
<li><p>Using the general formulas therefore we obtain <span class="math display">
\begin{aligned}
\mathbb{E}(Y_i) &amp;= b'(\theta_i) = \frac{\exp(\theta_i)}{1 + \exp(\theta_i)} = \mu_i, \\
\text{var}(Y_i) &amp;= a_i(\phi)b''(\theta_i) = \frac{1}{m_i}\frac{\exp(\theta_i)}{[1 + \exp(\theta_i)]^2} = \frac{\mu_i (1 - \mu_i)}{m_i},
\end{aligned}
</span> from which we obtain that the <span class="blue">variance function</span> is <span class="math inline">v(\mu_i) = \mu_i(1-\mu_i)</span> is quadratic.</p></li>
<li><p>We will write <span class="math inline">Y_i \sim \text{ED}(\mu_i, \mu_i(1-\mu_i))</span> with <span class="math inline">\mu_i \in \mathcal{M} = (0, 1)</span>.</p></li>
</ul>
</section>
<section id="notable-exponential-dispersion-families" class="slide level2 center">
<h2>Notable exponential dispersion families</h2>
<table class="caption-top">
<colgroup>
<col style="width: 25%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th><span class="math inline">\text{N}(\mu_i, \sigma^2)</span></th>
<th><span class="math inline">\text{Gamma}(\alpha, \alpha/\mu_i)</span></th>
<th><span class="math inline">\frac{1}{m_i}\text{Binomial}(m_i, \mu_i)</span></th>
<th><span class="math inline">\text{Poisson}(\mu_i)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="blue">Support</span> <span class="math inline">\mathcal{Y}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">[0, \infty)</span></td>
<td><span class="math inline">\{0, 1/m_i,\dots, 1\}</span></td>
<td><span class="math inline">\mathbb{N}</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\theta_i</span></td>
<td><span class="math inline">\mu_i</span></td>
<td><span class="math inline">- 1/\mu_i</span></td>
<td><span class="math inline">\log\left(\frac{\mu_i}{1 - \mu_i}\right)</span></td>
<td><span class="math inline">\log{\mu_i}</span></td>
</tr>
<tr class="odd">
<td><span class="orange">Parametric space</span> <span class="math inline">\Theta</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">(-\infty, 0)</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
</tr>
<tr class="even">
<td><span class="math inline">b(\theta_i)</span></td>
<td><span class="math inline">\theta_i^2/2</span></td>
<td><span class="math inline">-\log(-\theta_i)</span></td>
<td><span class="math inline">\log\{1 + \exp(\theta_i)\}</span></td>
<td><span class="math inline">\exp(\theta_i)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\phi</span></td>
<td><span class="math inline">\sigma^2</span></td>
<td><span class="math inline">1/\alpha</span></td>
<td><span class="math inline">1</span></td>
<td><span class="math inline">1</span></td>
</tr>
<tr class="even">
<td><span class="math inline">a_i(\phi)</span></td>
<td><span class="math inline">\sigma^2</span></td>
<td><span class="math inline">1/\alpha</span></td>
<td><span class="math inline">1/m_i</span></td>
<td><span class="math inline">1</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\mathcal{M}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">(0, \infty)</span></td>
<td><span class="math inline">(0, 1)</span></td>
<td><span class="math inline">(0, \infty)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">v(\mu_i)</span></td>
<td><span class="math inline">1</span></td>
<td><span class="math inline">\mu_i^2</span></td>
<td><span class="math inline">\mu_i(1-\mu_i)</span></td>
<td><span class="math inline">\mu_i</span></td>
</tr>
</tbody>
</table>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The list of exponential dispersion families does not end here. Other examples are the <span class="blue">inverse Gaussian</span>, the <span class="orange">negative binomial</span> and <span class="grey">hyperbolic secant</span> distributions.</p>
</div>
</div>
</div>
</section>
<section id="link-functions-and-canonical-link" class="slide level2 center">
<h2>Link functions and canonical link</h2>
<ul>
<li><p>To complete the GLM specification, we need to choose a <span class="blue">link function</span> <span class="math inline">g(\cdot)</span> such that: <span class="math display">
g(\mu_i) = \bm{x}_i^T\beta, \qquad \theta_i = \theta(\mu_i) \quad \Longrightarrow \quad \theta_i = \theta(g^{-1}(\bm{x}_i^T\beta)).
</span></p></li>
<li><p>It is fairly natural to consider a <span class="blue">monotone</span> and <span class="orange">differentiable</span> link function <span class="math inline">g(\cdot) : \mathcal{M} \to \mathbb{R}</span> so that the inverse <span class="math inline">g^{-1}(\cdot) : \mathbb{R} \to \mathcal{M}</span>. This ensures that the predictions are well-defined. <span class="math display">
\mathbb{E}(Y_i) = g^{-1}(\bm{x}_i^T\beta) \in \mathcal{M}.
</span></p></li>
<li><p>For example, in <span class="blue">binary regression</span> any continuous <span class="orange">cumulative distribution function</span> for <span class="math inline">g^{-1}(\cdot)</span> leads to a good link function, such as <span class="math inline">g(\cdot) = \Phi(\cdot)</span> (probit) or <span class="math inline">g^{-1}(\eta_i) = e^{\eta_i}/(1 + e^{\eta_i})</span> (logistic).</p></li>
</ul>
<div class="fragment">
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The following link is called <span class="orange">canonical link</span> and it is implied by the distribution: <span class="math display">
g(\mu_i) = \theta(\mu_i) \quad \Longrightarrow \quad \theta_i = \bm{x}_i^T\beta.
</span> Such a choice leads to remarkable simplifications in the likelihood function.</p>
</div>
</div>
</div>
<ul>
<li>The <span class="blue">identity</span> link is canonical for the <span class="blue">Gaussian</span>, the <span class="orange">logarithm</span> is canonical for the <span class="orange">Poisson</span>, the <span class="grey">logit</span> is canonical for the Binomial and the <span class="blue">reciprocal</span> is canonical for the <span class="blue">Gamma</span>.</li>
</ul>
<!-- - In a Gamma GLM, the reciprocal canonical link does not map $\bm{x}_i^T\beta$ into positive values, therefore the logarithmic link is sometimes preferred.  -->
</div>
</section></section>
<section>
<section id="likelihood-inference-and-testing" class="title-slide slide level1 center">
<h1>Likelihood, inference, and testing</h1>

</section>
<section id="likelihood-function" class="slide level2 center">
<h2>Likelihood function</h2>
<ul>
<li><p>Let <span class="math inline">Y_i \overset{\text{ind}}{\sim}\text{ED}(\mu_i, a_i(\phi)v(\mu_i))</span> be the response variable of a GLM, with <span class="math inline">g(\mu_i) = \bm{x}_i^T\beta</span>. The <span class="blue">joint distribution</span> of <span class="math inline">\bm{Y} = (Y_1,\dots,Y_n)</span> is <span class="math display">
p(\bm{y}; \beta, \phi) = \prod_{i=1}^np(y_i; \beta, \phi) = \prod_{i=1}^n \exp\left\{\frac{y_i\theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi)\right\}.
</span> with <span class="math inline">\theta_i = \theta(\mu_i) = \theta(g^{-1}(\bm{x}_i^T\beta))</span>.</p></li>
<li><p>The <span class="orange">log-likelihood</span> function therefore is <span class="math display">
\ell(\beta, \phi) = \sum_{i=1}^n\frac{y_i\theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi).
</span></p></li>
</ul>
<div class="fragment">
<ul>
<li>In general, there is <span class="orange">no sufficient statistic</span> with dimension smaller than <span class="math inline">n</span>.</li>
</ul>
</div>
</section>
<section id="sufficient-statistics-and-canonical-link" class="slide level2 center">
<h2>Sufficient statistics and canonical link 📖</h2>
<ul>
<li><p>Consider the <span class="blue">canonical link</span>, namely <span class="math inline">\theta(\mu_i) = g(\mu_i)</span> so that <span class="math inline">\theta_i = \bm{x}_i^T\beta</span>. Then the log-likelihood function <span class="orange">simplifies</span>: <span class="math display">
\begin{aligned}
\ell(\beta, \phi) &amp;= \sum_{i=1}^n\frac{y_i\bm{x}_i^T\beta - b(\bm{x}_i^T\beta)}{a_i(\phi)} + c(y_i, \phi) \\
&amp;= \left[\beta_1\sum_{i=1}^n\frac{x_{i1}y_i}{a_i(\phi)} + \cdots + \beta_p\sum_{i=1}^n\frac{x_{ip}y_i}{a_i(\phi)} \right] - \sum_{i=1}^n\frac{b(\bm{x}_i^T\beta)}{a_i(\phi)} + c(y_i, \phi).
\end{aligned}
</span> If <span class="math inline">\phi</span> were <span class="blue">known</span> or fixed, say <span class="math inline">a_i(\phi) = \phi</span> with <span class="math inline">\phi</span> known or <span class="math inline">a_i(\phi) = 1/\omega_i</span>, then <span class="math display">
\left(\sum_{i=1}^n\frac{1}{a_i(\phi)}x_{i1}y_i, \dots, \sum_{i=1}^n\frac{1}{a_i(\phi)}x_{ip}y_i \right),
</span> is (minimal) <span class="blue">sufficient</span> of dimension <span class="math inline">p \le n</span> for inference on <span class="math inline">\beta</span>.</p></li>
<li><p>In <span class="blue">logistic regression</span> for binary observations and <span class="orange">Poisson regression</span> with logarithmic link we have <span class="math inline">a_i(\phi) = 1</span>. The <span class="blue">sufficient</span> statistic is <span class="math inline">\bm{X}^T\bm{y} = \sum_{i=1}^n\bm{x}_iy_i = \left(\sum_{i=1}^nx_{i1}y_i, \dots, \sum_{i=1}^nx_{ip}y_i \right).</span></p></li>
</ul>
</section>
<section id="likelihood-equations-i" class="slide level2 center">
<h2>Likelihood equations I 📖</h2>
<ul>
<li>To conduct inference using the <span class="orange">classical theory</span> (as in <em>Statistica II</em>), we need to consider the first and second derivative of the log-likelihood, that is, the <span class="blue">score function</span> <span class="math display">
\frac{\partial}{\partial \beta_r}\ell(\beta; \phi), \qquad r=1,\dots,p,
</span> and the <span class="orange">observed information matrix</span> <span class="math inline">\bm{J}</span>, whose elements are <span class="math display">
j_{rs} = - \frac{\partial}{\partial \beta_r}\frac{\partial}{\partial \beta_s}\ell(\beta; \phi), \qquad r, s=1,\dots,p.
</span></li>
</ul>
<div class="fragment">
<div class="callout callout-warning no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>For now, let us focus on estimating <span class="math inline">\beta</span> and let us assume that <span class="math inline">\phi</span> is a <span class="blue">known parameter</span>, as in binomial and Poisson regression.</p>
<p>This is not a restrictive even when <span class="math inline">\phi</span> is unknown. In fact, we will show that the maximum likelihood estimate <span class="math inline">\hat{\beta}</span> does not depend on <span class="math inline">\phi</span> and that <span class="math inline">\beta</span> and <span class="math inline">\phi</span> are <span class="orange">orthogonal parameters</span>.</p>
</div>
</div>
</div>
</div>
</section>
<section id="likelihood-equations-ii" class="slide level2 center">
<h2>Likelihood equations II 📖</h2>
<ul>
<li><p>Let us begin by noting that <span class="math display">
\frac{\partial}{\partial \beta_r}\ell(\beta; \phi) =  \sum_{i=1}^n\frac{1}{a_i(\phi)} \left(y_i \frac{\partial \theta_i}{\partial \beta_r} - \frac{\partial b(\theta_i)}{\partial \beta_r} \right), \qquad r = 1,\dots,p.
</span> Such an expression can be <span class="blue">simplified</span> because <span class="math display">
\frac{\partial b(\theta_i)}{\partial \beta_r} = b'(\theta_i)\frac{\partial \theta_i}{\partial \beta_r} = \mu_i\frac{\partial \theta_i}{\partial \beta_r},
</span> which implies that the score function will have the <span class="orange">structure</span> <span class="math display">
\frac{\partial}{\partial \beta_r}\ell(\beta; \phi) = \sum_{i=1}^n\frac{1}{a_i(\phi)}(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r}, \qquad r=1,\dots,p.
</span></p></li>
<li><p>Recall that <span class="math inline">a_i(\phi) = \phi/\omega_i</span>, hence the <span class="blue">maximum likelihood estimator</span> is obtained by solving: <span class="math display">
\textcolor{red}{\cancel{\frac{1}{\phi}}}\sum_{i=1}^n\omega_i(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r} = 0, \qquad r=1,\dots,p.
</span></p></li>
</ul>
</section>
<section id="likelihood-equations-iii" class="slide level2 center">
<h2>Likelihood equations III 📖</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">f(x)</span> be a function with <span class="orange">inverse</span> <span class="math inline">g(x) = f^{-1}(x)</span> and <span class="blue">first derivative</span> <span class="math inline">f'(x)</span>. Then <span class="math display">
\frac{\partial g}{\partial{x}} = [f^{-1}]'(x) = \frac{1}{f'(f^{-1}(x))}.
</span></p>
</div>
</div>
</div>
<ul>
<li>Recall that <span class="math inline">g(\mu_i) = \bm{x}_i^T\beta = \eta_i</span> and that <span class="math inline">\theta_i = \theta(\mu_i)</span> is the inverse of <span class="math inline">\mu(\theta_i)</span>. As an <span class="blue">application</span> of the above <span class="blue">lemma</span>: <span class="math display">
\frac{\partial \theta_i}{\partial \mu_i} = \theta'(\mu_i) = \frac{1}{\mu'(\theta(\mu_i))}= \frac{1}{b''(\theta(\mu_i))} = \frac{1}{v(\mu_i)},
</span> Moreover, since we <span class="math inline">\mu_i = g^{-1}(\eta_i)</span> we obtain <span class="math display">
\frac{\partial \mu_i}{\partial \eta_i} = \frac{1}{g'(g^{-1}(\eta_i))} = \frac{1}{g'(\mu_i)}.
</span></li>
<li>Summing up, the <span class="orange">chain rule of derivation</span> for <span class="blue">composite functions</span> implies: <span class="math display">
\frac{\partial \theta_i}{\partial \beta_r} = \frac{\partial \theta_i}{\partial \mu_i} \frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \eta_i}{\partial\beta_r} = \frac{1}{v(\mu_i)}\frac{1}{g'(\mu_i)}x_{ir}, \qquad r=1,\dots,p.
</span></li>
</ul>
</section>
<section id="likelihood-equations-iv" class="slide level2 center">
<h2>Likelihood equations IV 📖</h2>
<ul>
<li>Combining all the above equations, we obtain an explicit formula for the score function <span class="math display">
\frac{\partial}{\partial \beta_r}\ell(\beta; \phi)  = \frac{1}{\phi}\sum_{i=1}^n \omega_i \frac{(y_i - \mu_i)}{v(\mu_i)}\frac{x_{ir}}{g'(\mu_i)} = \sum_{i=1}^n \frac{(y_i - \mu_i)}{\text{var}(Y_i)}\frac{x_{ir}}{g'(\mu_i)}, \qquad r=1,\dots,p.
</span></li>
</ul>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The <span class="blue">maximum likelihood estimator</span> is the solution of <span class="orange">likelihood equations</span>: <span class="math display">
\sum_{i=1}^n \omega_i \frac{(y_i - \mu_i)}{v(\mu_i)}\frac{x_{ir}}{g'(\mu_i)} = 0, \qquad r=1,\dots,p,
</span> which do not depend on <span class="math inline">\phi</span>. In <span class="blue">matrix notation</span> <span class="math display">
\bm{D}^T \bm{V}^{-1}(\bm{y} - \bm{\mu}) = \bm{0},
</span> where <span class="math inline">\bm{V} = \text{diag}(v(\mu_1)/\omega_1,\dots,v(\mu_n)/\omega_n)</span> and <span class="math inline">\bm{D}</span> is an <span class="math inline">n \times p</span> matrix whose elements are <span class="math display">
d_{ir} = \frac{\partial \mu_i}{\partial \beta_r} =\frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \eta_i}{\partial\beta_r} =\frac{1}{g'(\mu_i)}x_{ir}, \qquad i=1,\dots,n, \quad r=1,\dots,p.
</span></p>
</div>
</div>
</div>
</section>
<section id="canonical-link-simplifications" class="slide level2 center">
<h2>Canonical link: simplifications</h2>
<ul>
<li>When using the <span class="orange">canonical link</span> <span class="math inline">\theta(\mu_i) = g(\mu_i)</span> there are major simplification, because <span class="math display">
\frac{\partial \theta_i}{\partial \mu_i}= \frac{1}{v(\mu_i)} = g'(\mu_i) \quad \Longrightarrow\quad v(\mu_i)g'(\mu_i) = 1.
</span> In other terms, we obtain the simple equality: <span class="math display">
\frac{\partial \theta_i}{\partial \beta_r} = x_{ir}, \qquad r=1,\dots,p,
</span> which is not surprising since the canonical link implies <span class="math inline">\theta_i = x_{i1}\beta_1 + \cdots + x_{ip}\beta_p</span>.</li>
</ul>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The <span class="orange">likelihood equations</span> under the <span class="blue">canonical link</span> are <span class="math display">
\sum_{i=1}^n \omega_i (y_i - \mu_i)x_{ir} = 0, \qquad r=1,\dots,p.
</span> Let <span class="math inline">\bm{\Omega} = \text{diag}(\omega_1,\dots,\omega_n)</span>, then in <span class="blue">matrix notation</span> <span class="math inline">\bm{X}^T\bm{\Omega}(\bm{y} - \bm{\mu}) = \bm{0}</span>. This simplifies even further when the weights are constant, i.e.&nbsp;<span class="math inline">\bm{\Omega} = I_n</span>, obtaining <span class="math inline">\bm{X}^T(\bm{y} - \bm{\mu}) = \bm{0}</span>.</p>
</div>
</div>
</div>
</section>
<section id="examples" class="slide level2 center">
<h2>Examples</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y_i \sim \text{ED}(\mu_i, \phi)</span> with <span class="math inline">g(\mu_i) = \mu_i</span>, namely the <span class="orange">Gaussian</span> linear model with the <span class="grey">identity</span> (canonical) link. The likelihood equations are <span class="math display">
\bm{X}^T(\bm{y} - \bm{X}\beta) = \bm{0},
</span> which are also called <span class="orange">normal equations</span>. Their solution over <span class="math inline">\beta</span> is the OLS <span class="math inline">\hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}</span>.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y_i \sim \text{ED}(\mu_i, \phi/\omega_i)</span> with <span class="math inline">g(\mu_i) = \mu_i</span>, namely the <span class="orange">Gaussian</span> linear model with the <span class="grey">identity</span> (canonical) link and <span class="blue">heteroschedastic errors</span>. The likelihood equations are <span class="math display">
\bm{X}^T\bm{\Omega}(\bm{y} - \bm{X}\beta) = \bm{0},
</span> Their solution over <span class="math inline">\beta</span> is the weighted least square estimator <span class="math inline">\hat{\beta}_\text{wls} = (\bm{X}^T\bm{\Omega}\bm{X})^{-1}\bm{X}^T\bm{\Omega}\bm{y}</span>.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y_i \sim \text{ED}(\mu_i, \mu_i)</span> with <span class="math inline">g(\mu_i) = \log{\mu_i}</span>, namely a <span class="orange">Poisson</span> regression model with the <span class="grey">logarithmic</span> (canonical) link. The likelihood equations can be solved <span class="orange">numerically</span> <span class="math display">
\bm{X}^T(\bm{y} - \bm{\mu}) = \bm{0}, \qquad \bm{\mu} = (e^{\bm{x}_1^T\beta}, \dots,  e^{\bm{x}_n^T\beta}).
</span></p>
</div>
</div>
</div>
</section>
<section id="example-aids-data" class="slide level2 center">
<h2>Example: <code>Aids</code> data</h2>
<ul>
<li><p>In the <code>Aids</code> data we specified a <span class="blue">Poisson</span> regression model with <span class="math inline">\mathbb{E}(Y_i) = \exp(\beta_1 + \beta_2 x_i)</span>.</p></li>
<li><p>The maximum likelihood estimate <span class="math inline">(\hat{\beta}_1, \hat{\beta}_2)</span> <span class="orange">solve</span> simultaneously: <span class="math display">
\sum_{i=1}^n y_i =  \sum_{i=1}^n \exp(\beta_1 + \beta_2x_i), \quad
\text{and }\quad \sum_{i=1}^n x_i y_i =  \sum_{i=1}^n x_i\exp(\beta_1 + \beta_2 x_i).
</span></p></li>
<li><p>This system does <span class="orange">not</span> always admits a <span class="orange">solution</span>. This happens, for example, in the <span class="blue">extreme case</span> <span class="math inline">\sum_{i=1}y_i = 0</span>, occurring when all counts equal zero.</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>Using the <code>Aids</code> data <span class="math inline">\sum_{i=1}^ny_i = 217</span> and <span class="math inline">\sum_{i=1}^nx_i y_i = 2387</span> and via <span class="orange">numerical methods</span> we obtain <span class="math inline">\hat{\beta}_1 = 0.304</span> and <span class="math inline">\hat{\beta}_2 = 0.259</span>.</p></li>
<li><p>The estimated mean values are <span class="math inline">\hat{\mu}_i = \mathbb{E}(Y_i) = \exp(0.304 + 0.259 x_i)</span> and in particular the mean for the <span class="blue">next period</span> is <span class="math display">
\hat{\mu}_{i+1} = \exp(0.304 + 0.259 (x_i +1)) = \exp(0.259) \hat{\mu}_i = 1.296 \hat{\mu}_i.
</span> In other words, the estimated increase of Aids deaths is about the <span class="math inline">30\%</span> every trimester.</p></li>
</ul>
</div>
</section>
<section id="observed-and-expected-information-i" class="slide level2 center">
<h2>Observed and expected information I</h2>
<ul>
<li><p>We suppose for now that <span class="math inline">\phi</span> is a <span class="orange">known parameter</span>, say <span class="math inline">\phi = 1</span>.</p></li>
<li><p>Let <span class="math inline">\bm{I} = \mathbb{E}(\bm{J})</span> be the <span class="math inline">p \times p</span> <span class="blue">Fisher information matrix</span> associated to <span class="math inline">\beta</span>, whose elements are <span class="math display">
i_{rs} = \mathbb{E}(j_{rs}) = \mathbb{E}\left(- \frac{\partial}{\partial \beta_r}\frac{\partial}{\partial \beta_s}\ell(\beta; \phi)\right), \qquad r,s = 1,\dots,p.
</span></p></li>
</ul>
</section>
<section id="observed-and-expected-information-ii" class="slide level2 center">
<h2>Observed and expected information II</h2>
</section>
<section id="orthogonality-of-beta-and-psi" class="slide level2 center">
<h2>Orthogonality of <span class="math inline">\beta</span> and <span class="math inline">\psi</span></h2>
<ul>
<li>Let us now consider the case in which <span class="math inline">\phi</span> is <span class="blue">unknown</span> so that <span class="math inline">a_i(\phi) = \phi/\omega_i</span>. We obtain: <span class="math display">
j_{r \phi} = - \frac{\partial}{\partial \beta_r}\frac{\partial}{\partial \phi}\ell(\beta; \phi) = \frac{1}{\phi^2}\sum_{i=1}^n \omega_i(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r}, \qquad r = 1,\dots,p.
</span> whose <span class="orange">expected value</span> is <span class="math inline">i_{r\phi} = \mathbb{E}(j_{r\phi}) = 0</span> since <span class="math inline">\mathbb{E}(Y_i) = \mu_i</span>.</li>
</ul>
<div class="fragment">
<ul>
<li>This means the Fisher information matrix accounting for <span class="math inline">\phi</span> takes the form: <span class="math display">
\begin{bmatrix}
\bm{I} &amp; \bm{0} \\
\bm{0} &amp; i_{\phi \phi}
\end{bmatrix} \qquad\Longrightarrow\qquad  \begin{bmatrix}
\bm{I} &amp; \bm{0} \\
\bm{0} &amp; i_{\phi \phi}
\end{bmatrix}^{-1} = \begin{bmatrix}
\bm{I}^{-1} &amp; \bm{0} \\
\bm{0} &amp; 1 /i_{\phi \phi}
\end{bmatrix}
</span> where <span class="math inline">[\bm{I}]_{rs} = i_{rs}</span> are the elements associated to <span class="math inline">\beta</span> as before.</li>
</ul>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The parameters <span class="math inline">\beta</span> and <span class="math inline">\phi</span> are <span class="orange">orthogonal</span> and their maximum likelihood estimates are <span class="blue">asymptotically independent</span>.</p>
<p>Moreover, the matrices <span class="math inline">\bm{I}</span> and <span class="math inline">\bm{I}^{-1}</span> are sufficient for inference on <span class="math inline">\beta</span> and there is no need to compute <span class="math inline">i_{\phi \phi}</span>. Note that the maximum likelihood <span class="math inline">\hat{\beta}</span> can also be computed without knowing <span class="math inline">\phi</span>.</p>
</div>
</div>
</div>
<!-- - In a GLM with $p = 2$ the Fisher information matrix, evaluated on a given $\beta,\phi$, looks like: -->
<!-- $$ -->
<!-- \begin{bmatrix} -->
<!-- i_{11} & i_{12} &  i_{1\phi} \\ -->
<!-- i_{21} & i_{22} &  i_{2\phi} \\ -->
<!-- i_{\phi 1} & i_{\phi2 } &  i_{\phi\phi} \\ -->
<!-- \end{bmatrix}= \begin{bmatrix} -->
<!-- 10.39 & -1.22 & 0 \\  -->
<!-- -1.22 & 10.39 & 0 \\  -->
<!-- 0 & 0 & 10 \\  -->
<!--   \end{bmatrix} \Longrightarrow \begin{bmatrix} -->
<!-- i_{11} & i_{12} &  i_{1\phi} \\ -->
<!-- i_{21} & i_{22} &  i_{2\phi} \\ -->
<!-- i_{\phi 1} & i_{\phi2 } &  i_{\phi\phi} \\ -->
<!-- \end{bmatrix}^{-1}= \begin{bmatrix} -->
<!-- 0.098 & 0.011 & 0 \\  -->
<!-- 0.011 & 0.098 & 0 \\  -->
<!-- 0 & 0 & 0.1 \\  -->
<!--   \end{bmatrix} -->
<!-- $$ -->
<!-- ```{r} -->
<!-- set.seed(1234) -->
<!-- XtX <- crossprod(matrix(rnorm(20), 10, 2)) -->
<!-- XtX <- rbind(cbind(XtX, 0),0) -->
<!-- XtX[3, 3] <- 10 -->
<!-- # xtable::xtable(XtX, digits = 2) -->
<!-- # xtable::xtable(solve(XtX), digits = 3) -->
<!-- ``` -->
</div>
</section>
<section id="asymptotic-distribution-of-hatbeta" class="slide level2 center">
<h2>Asymptotic distribution of <span class="math inline">\hat{\beta}</span></h2>
</section>
<section id="wald-confidence-intervals-for-beta" class="slide level2 center">
<h2>Wald confidence intervals for <span class="math inline">\beta</span></h2>
</section>
<section id="delta-methods-and-fitted-values" class="slide level2 center">
<h2>Delta methods and fitted values</h2>
</section></section>
<section id="irls-algorithm" class="title-slide slide level1 center">
<h1>IRLS algorithm</h1>

</section>

<section id="deviance-model-checking-residuals" class="title-slide slide level1 center">
<h1>Deviance, model checking, residuals</h1>

</section>

<section>
<section id="model-selection" class="title-slide slide level1 center">
<h1>Model selection</h1>

</section>
<section id="references" class="slide level2 unnumbered smaller scrollable">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Agresti2015" class="csl-entry" role="listitem">
Agresti, A. (2015), <em><span class="nocase">Foundations of Linear and Generalized Linear Models</span></em>, Wiley.
</div>
<div id="ref-Azzalini2008" class="csl-entry" role="listitem">
Azzalini, A. (2008), <em>Inferenza statistica</em>, Springer Verlag.
</div>
<div id="ref-Efron2023" class="csl-entry" role="listitem">
Efron, B. (2023), <em><span class="nocase">Exponential Families in Theory and Practice</span></em>, Cambridge University Press.
</div>
<div id="ref-Fisher1934" class="csl-entry" role="listitem">
Fisher, R. A. (1934), <span>“<span class="nocase">Two new properties of mathematical likelihood</span>,”</span> <em>Proceedings of the Royal Society of London. Series A</em>, 144, 285–307.
</div>
<div id="ref-Salvan2020" class="csl-entry" role="listitem">
Salvan, A., Sartori, N., and Pace, L. (2020), <em>Modelli lineari generalizzati</em>, Springer.
</div>
</div>

</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="img/logoB.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://tommasorigon.github.io/StatIII">Home page</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="un_B_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="un_B_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="un_B_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>