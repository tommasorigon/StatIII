<!DOCTYPE html>
<html lang="en"><head>
<script src="un_B_files/libs/clipboard/clipboard.min.js"></script>
<script src="un_B_files/libs/quarto-html/tabby.min.js"></script>
<script src="un_B_files/libs/quarto-html/popper.min.js"></script>
<script src="un_B_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="un_B_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="un_B_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="un_B_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="un_B_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.56">

  <meta name="author" content="Tommaso Rigon">
  <title>Exponential families</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="un_B_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="un_B_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="un_B_files/libs/revealjs/dist/theme/quarto.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">
  <link href="un_B_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="un_B_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="un_B_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="un_B_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Exponential families</h1>
  <p class="subtitle">Statistical Inference - PhD EcoStatData</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<span class="orange">Tommaso Rigon</span> 
</div>
        <p class="quarto-title-affiliation">
            <em>Università degli Studi di Milano-Bicocca</em>
          </p>
    </div>
</div>

</section>
<section id="homepage" class="slide level2 center">
<h2><a href="../index.html">Homepage</a></h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><img data-src="img/gaussian.png"></p>
<!-- *"Pluralitas non est ponenda sine necessitate."* -->
<!-- [William of Ockham]{.grey} -->
</div><div class="column" style="width:70%;">
<ul>
<li><p>This unit will cover the following <span class="orange">topics</span>:</p>
<ul>
<li>One-parameter and multiparameter exponential families</li>
<li>Likelihood, inference, sufficiency and completeness</li>
</ul></li>
<li><p>The <span class="orange">prime role</span> of <span class="orange">exponential families</span> in the theory of statistical inference was first emphasized by <span class="citation" data-cites="Fisher1934">Fisher (<a href="#/references" role="doc-biblioref" onclick="">1934</a>)</span>.</p></li>
<li><p>Most <span class="blue">well-known</span> <span class="blue">distributions</span>—such as Gaussian, Poisson, Binomial, and Gamma—are instances of exponential families.</p></li>
<li><p>Exponential families are the distributions typically considered when presenting the usual “regularity conditions”.</p></li>
</ul>
</div><ul>
<li>With a few minor exceptions, this presentation will closely follow Chapters 5 and 6 of <span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#/references" role="doc-biblioref" onclick="">1997</a>)</span>.</li>
</ul>
</div>
</section>
<section id="overview" class="slide level2 center">
<h2>Overview</h2>

<img data-src="img/EF.png" class="quarto-figure quarto-figure-center r-stretch" style="width:6in"><ul>
<li>Figure 1 of <span class="citation" data-cites="Efron2023">Efron (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span>. Three level of statistical modeling.</li>
</ul>
</section>
<section>
<section id="one-parameter-exponential-families" class="title-slide slide level1 center">
<h1>One-parameter exponential families</h1>

</section>
<section id="exponential-tilting" class="slide level2 center">
<h2>Exponential tilting</h2>
<ul>
<li><p>Let <span class="math inline">Y</span> be a <span class="orange">non-degenerate</span> random variable with <span class="blue">support</span> <span class="math inline">\mathcal{Y} \subseteq \mathbb{R}</span> and <span class="orange">density</span> <span class="math inline">f_0(y)</span> with respect to a dominating measure <span class="math inline">\nu(\mathrm{d}y)</span>.</p></li>
<li><p>We aim at building a <span class="blue">parametric family</span> <span class="math inline">\mathcal{F} = \{f(;\theta) : \theta \in \Theta \subseteq \mathbb{R} \}</span> with common support <span class="math inline">\mathcal{Y}</span> such that <span class="math inline">f_0</span> is a special case, namely <span class="math inline">f_0 \in \mathcal{F}</span>.</p></li>
<li><p>A strategy for doing this is called <span class="orange">exponential tilting</span>, namely we could set <span class="math display">
f(y; \theta) \propto e^{\theta y}f_0(y).
</span> Thus, if <span class="math inline">f(y;\theta)</span> is generated via exponential tilting, then <span class="math inline">f(y; 0) = e^0 f_0(y) = f_0(y)</span>.</p></li>
<li><p>Let us define the mapping <span class="math inline">M_0:\mathbb{R}\rightarrow (0,\infty]</span> <span class="math display">
M_0(\theta):=\int_\mathcal{Y}e^{\theta y}f_0(y)\nu(\mathrm{d}y), \qquad \theta \in \mathbb{R}.
</span> If <span class="math inline">M_0(\theta)</span> is <span class="orange">finite</span> in a neighborhood of the origin, it is the <span class="blue">moment generating function</span> of <span class="math inline">Y</span>.</p></li>
<li><p>Moreover, we define the set <span class="math inline">\tilde{\Theta} \subseteq \mathbb{R}</span> as the set of all <span class="math inline">\theta</span> such that <span class="math inline">M_0(\theta)</span> is finite, i.e. <span class="math display">
\tilde{\Theta} = \{\theta \in \mathbb{R} : M_0(\theta) &lt; \infty\}.
</span></p></li>
</ul>
</section>
<section id="natural-exponential-family-of-order-one" class="slide level2 center">
<h2>Natural exponential family of order one</h2>
<ul>
<li>The mapping <span class="math inline">K(\theta) = K_0(\theta) = \log{M_0(\theta)}</span> is the <span class="blue">cumulant generating function</span> of <span class="math inline">f_0</span>. It is <span class="orange">finite</span> if and only if <span class="math inline">M_0(\theta)</span> is finite.</li>
</ul>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The parametric family generated via <span class="orange">exponential tilting</span> of <span class="math inline">f_0</span> <span class="math display">
\mathcal{F}_{\text{ne}}^1 = \left\{f(y;\theta) = \frac{e^{\theta y}f_0(y)}{M_0(\theta)} = f_0(y)\exp\{\theta y - K(\theta)\}, \quad y \in \mathcal{Y}, \theta \in \tilde{\Theta} \right\},
</span> is called a <span class="orange">natural exponential family</span> of order one, and <span class="math inline">\tilde{\Theta} = \{\theta \in \mathbb{R} : K(\theta) &lt; \infty\}</span> is the <span class="blue">natural parameter space</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>The natural parameter space <span class="math inline">\tilde{\Theta}</span> is the <span class="orange">widest possible</span> and must be an <span class="orange">interval</span>; see exercises. The family <span class="math inline">\mathcal{F}_{\text{ne}}^1</span> is said to be <span class="blue">full</span>, whereas a subfamily of <span class="math inline">\mathcal{F}_{\text{ne}}^1</span> with <span class="math inline">\Theta \subseteq \tilde{\Theta}</span> is <span class="blue">non-full</span>.</p></li>
<li><p>By definition, all the densities <span class="math inline">f(y;\theta) \in \mathcal{F}_{\text{ne}}^1</span> have the <span class="blue">same support</span>.</p></li>
</ul>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>A natural exponential family of order one, <span class="math inline">\mathcal{F}_{\text{ne}}^1</span>, is said to be <span class="orange">regular</span> if <span class="math inline">\tilde{\Theta}</span> is open.</p>
</div>
</div>
</div>
</section>
<section id="moment-generating-function" class="slide level2 center">
<h2>Moment generating function</h2>
<ul>
<li>In regular problems, the functions <span class="math inline">M_0(\theta)</span> and <span class="math inline">K_0(\theta)</span> associated to a r.v. <span class="math inline">Y</span> with density <span class="math inline">f_0</span> are <span class="orange">finite</span> in a neighbor of the origin. A <span class="blue">sufficient</span> condition is that <span class="math inline">\tilde{\Theta}</span> is an <span class="blue">open set</span> (regular <span class="math inline">\mathcal{F}_\text{en}^1</span>).</li>
</ul>
<div class="callout callout-warning no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Suppose <span class="math inline">M_0(t) &lt; \infty</span> for any <span class="math inline">|t| &lt; t_0</span> and for some <span class="math inline">t_0 &gt; 0</span>. Then a standard result of probability theory (e.g. <span class="citation" data-cites="Billingsley1995">Billingsley (<a href="#/references" role="doc-biblioref" onclick="">1995</a>)</span>, Section 21) implies:</p>
<ul>
<li><p>The random variable <span class="math inline">Y</span> has <span class="blue">finite moments</span> of all orders, i.e.&nbsp;<span class="math inline">\mu_k = \mathbb{E}(Y^k) &lt; \infty</span> for all <span class="math inline">k \geq 1</span>.</p></li>
<li><p>The moments <span class="math inline">(\mu_k)_{k \ge 1}</span> and <span class="blue">moment generating function</span> <span class="math inline">M_0(t)</span> <span class="orange">uniquely characterize</span> the <span class="orange">law</span> of <span class="math inline">Y</span> and <span class="math inline">f_0</span>. Moreover, <span class="math inline">M_0(t)</span> admits a <span class="grey">Taylor expansion</span> around the origin: <span class="math display">
M_0(t) = 1 +  \mu_1 t + \mu_2 \frac{t^2}{2!} + \mu_3 \frac{t^3}{3!} + \cdots = \sum_{k=0}^\infty \frac{t^k}{k!}\mu_k, \qquad |t| &lt; t_0.
</span></p></li>
<li><p>The moments <span class="math inline">\mu_k</span> equal the <span class="math inline">k</span>th derivative of <span class="math inline">M_0(t)</span> evaluated at the origin: <span class="math display">
\mu_k = \mathbb{E}_\theta(Y^k) = \frac{\partial^k}{\partial t^k} M_0(t) \Big|_{t = 0}, \qquad k \ge 1.
</span></p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="cumulant-generating-function" class="slide level2 center">
<h2>Cumulant generating function</h2>
<div class="callout callout-warning no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Suppose <span class="math inline">K_0(t) = \log{M_0(t)} &lt; \infty</span> for any <span class="math inline">|t| &lt; t_0</span> and for some <span class="math inline">t_0 &gt; 0</span>. Then:</p>
<ul>
<li><p><span class="math inline">K_0</span> <span class="orange">uniquely characterizes</span> the law of <span class="math inline">Y</span> and it admits a <span class="grey">Taylor expansion</span> <span class="math display">
K_0(t) = \kappa_1 t + \kappa_2 \frac{t^2}{2!} + \kappa_3 \frac{t^3}{3!} + \cdots = \sum_{k=1}^\infty \frac{t^k}{k!} \kappa_k, \qquad |t| &lt; t_0,
</span> where the coefficients <span class="math inline">(\kappa_k)_{k \ge 1}</span> are the <span class="blue">cumulants</span> of <span class="math inline">Y</span>.</p></li>
<li><p>The cumulants <span class="math inline">\kappa_k</span> equal the <span class="math inline">k</span>th derivative of <span class="math inline">K_0(t)</span> evaluated at the origin <span class="math display">
\kappa_k = \frac{\partial^k}{\partial t^k} K_0(t) \Big|_{t = 0}, \qquad k \ge 1.
</span> Moreover, it can be shown the following moment relationships hold: <span class="math display">
\kappa_1 = \mathbb{E}_\theta(Y), \quad \kappa_2 = \text{var}_\theta(Y), \quad \kappa_3 = \mathbb{E}_\theta\{(Y - \mu_1)^3\}, \quad \kappa_4 = \mathbb{E}_\theta\{(Y - \mu_1)^4\} - 3\text{var}_\theta(Y)^2.
</span></p></li>
</ul>
</div>
</div>
</div>

<aside><div>
<p>Refer to <span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#/references" role="doc-biblioref" onclick="">1997</a>)</span>, Section 3.2.5 for detailed derivations. Standardized cumulants <span class="math inline">\kappa_3/\kappa_2^{3/2}</span> and <span class="math inline">\kappa_4/\kappa_2^2</span> are the <span class="blue">skewness</span> and the (excess of) <span class="blue">kurtosis</span> of <span class="math inline">Y</span>.</p>
</div></aside></section>
<section id="example-uniform-distribution" class="slide level2 center">
<h2>Example: uniform distribution 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y \sim \text{Unif}(0,1)</span> so that <span class="math inline">f_0(y) = 1</span> for <span class="math inline">y \in [0,1]</span>. The <span class="orange">exponential tilting</span> of <span class="math inline">f_0</span> gives <span class="math display">
f(y; \theta) \propto e^{\theta y}f_0(y) = e^{\theta y}, \qquad y \in [0,1], \quad \theta \in \mathbb{R}.
</span></p></li>
<li><p>The normalizing constant, that is, the <span class="blue">moment generating function</span>, is <span class="math display">
M_0(\theta)= \mathbb{E}(e^{\theta Y})  = \int_0^1 e^{\theta y} \mathrm{d}y = \frac{e^\theta}{\theta}\Big|_0^1 = \frac{e^\theta - 1}{\theta}, \qquad \theta \neq 0.
</span> with <span class="math inline">M_0(0) = 1</span>. Note that <span class="math inline">M_0</span> is continuous since <span class="math inline">\lim_{\theta \to 0}(e^\theta - 1)/\theta = 1</span>.</p></li>
<li><p>Consequently, we have <span class="math inline">M_0(\theta) &lt; \infty</span> for all <span class="math inline">\theta \in \mathbb{R}</span> and the <span class="orange">natural parameter space</span> is <span class="math inline">\tilde{\Theta} = \mathbb{R}</span>, which is an <span class="blue">open set</span>. The resulting density is <span class="math display">
f(y; \theta) = \frac{\theta e^{\theta y}}{e^{\theta -1}} = \exp\{\theta y - K(\theta)\}, \qquad y \in [0, 1],
</span> where <span class="math inline">K(\theta) = \log\{(e^\theta - 1)/\theta\}</span>.</p></li>
<li><p>It <a href="https://math.stackexchange.com/questions/1008707/moment-generating-function-of-bounded-variables">holds in general</a> that <span class="math inline">\tilde{\Theta} = \mathbb{R}</span> whenever <span class="math inline">f_0</span> has <span class="blue">bounded support</span>; thus, the family is <span class="orange">regular</span>.</p></li>
</ul>
</section>
<section id="example-poisson-distribution" class="slide level2 center">
<h2>Example: Poisson distribution 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y \sim \text{Poisson}(1)</span> so that <span class="math inline">f_0(y) = e^{-1}/y!</span> for <span class="math inline">y \in \mathbb{N}</span>. The <span class="orange">exponential tilting</span> of <span class="math inline">f_0</span> gives <span class="math display">
f(y; \theta) \propto e^{\theta y}f_0(y) = \frac{e^{\theta y}e^{-1}}{y!}, \qquad y \in \mathbb{N}, \quad \theta \in \mathbb{R}.
</span></p></li>
<li><p>The normalizing constant, that is, the <span class="blue">moment generating function</span>, is <span class="math display">
M_0(\theta)= \mathbb{E}(e^{\theta Y})  = e^{-1}\sum_{k=0}^\infty \frac{e^{\theta k}}{k!} = \exp\{e^\theta - 1\}, \qquad \theta \in \mathbb{R}.
</span></p></li>
<li><p>Consequently, we have <span class="math inline">M_0(\theta) &lt; \infty</span> for all <span class="math inline">\theta \in \mathbb{R}</span> and the <span class="orange">natural parameter space</span> is <span class="math inline">\tilde{\Theta} = \mathbb{R}</span>, which is an <span class="blue">open set</span>. The resulting density is <span class="math display">
f(y; \theta) = \frac{e^{\theta y} e^{-1}}{y!}\frac{e^{-e^\theta}}{e^{-1}} = \frac{e^{-1}}{y!}\exp\{\theta y - (e^\theta - 1)\} = \frac{\lambda^y e^{\lambda}}{y!}, \qquad y \in \mathbb{N},
</span> so that <span class="math inline">K(\theta) = e^\theta - 1</span> and having defined <span class="math inline">\lambda  = e^\theta</span>.</p></li>
<li><p>In other words, the tilted density is again a Poisson distribution with mean <span class="math inline">e^\theta</span>.</p></li>
</ul>
</section>
<section id="example-exponential-family-generated-by-a-gaussian" class="slide level2 center">
<h2>Example: exponential family generated by a Gaussian 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y \sim \text{N}(0,1)</span> so that <span class="math inline">f_0(y) = 1/(\sqrt{2\pi})e^{-y^2/2}</span> for <span class="math inline">y \in \mathbb{R}</span>. The <span class="orange">exponential tilting</span> of <span class="math inline">f_0</span> gives <span class="math display">
f(y; \theta) \propto e^{\theta y}f_0(y) = \frac{1}{\sqrt{2\pi}}e^{\theta y -y^2/2}, \qquad y,\theta \in \mathbb{R}.
</span></p></li>
<li><p>The normalizing constant, that is, the <span class="blue">moment generating function</span>, is <span class="math display">
M_0(\theta)= \mathbb{E}(e^{\theta Y})  = \frac{1}{\sqrt{2\pi}}\int_\mathbb{R}e^{\theta y -y^2/2}\mathrm{d}y = e^{\theta^2/2}, \qquad \theta \in \mathbb{R}.
</span></p></li>
<li><p>Consequently, we have <span class="math inline">M_0(\theta) &lt; \infty</span> for all <span class="math inline">\theta \in \mathbb{R}</span> and the <span class="orange">natural parameter space</span> is <span class="math inline">\tilde{\Theta} = \mathbb{R}</span>, which is an <span class="blue">open set</span>. The resulting density is <span class="math display">
f(y; \theta) = \frac{1}{\sqrt{2\pi}}e^{\theta y}e^{-y^2/2}e^{-\theta^2/2} = \frac{e^{-y^2/2}}{\sqrt{2\pi}}\exp\{\theta y - \theta^2/2\} = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(y - \theta)^2}, \qquad y \in \mathbb{R},
</span> so that <span class="math inline">K(\theta) = \theta^2/2</span>.</p></li>
<li><p>In other words, the tilted density is again a Gaussian distribution with mean <span class="math inline">\theta</span>.</p></li>
</ul>
</section>
<section id="closure-under-exponential-tilting" class="slide level2 center">
<h2>Closure under exponential tilting 📖</h2>
<ul>
<li><p>Let <span class="math inline">\mathcal{F}_{\text{ne}}^1</span> be an exponential family with <span class="blue">parameter</span> <span class="math inline">\psi</span> and <span class="orange">natural parameter space</span> <span class="math inline">\tilde{\Psi}</span>, with density <span class="math inline">f(y; \psi) = f_0(y)\exp\{\psi y - K(\psi)\}</span>. The <span class="orange">exponential tilting</span> of <span class="math inline">f(y; \psi)</span> gives <span class="math display">
f(y; \theta, \psi) \propto e^{\theta y} f(y; \psi) \propto f_0(y) \exp\{(\theta + \psi)y\},
</span> and the <span class="blue">normalizing constant</span> of <span class="math inline">f_0(y) \exp\{(\theta + \psi)y\}</span> is therefore<br>
<span class="math display">
\int_\mathcal{Y} f_0(y) \exp\{(\theta + \psi)y\} \, \nu(\mathrm{d}y) = M_0(\theta + \psi).
</span></p></li>
<li><p>Thus, for any <span class="math inline">\theta</span> and <span class="math inline">\psi</span> such that <span class="math inline">M_0(\theta + \psi) &lt; \infty</span>, the corresponding density is <span class="math display">
f(y; \theta, \psi) = f_0(y) \exp\{(\theta + \psi)y - K(\theta + \psi)\},
</span> which is again a <span class="blue">member</span> of the <span class="orange">exponential family</span> <span class="math inline">\mathcal{F}_{\text{ne}}^1</span>, with updated parameter <span class="math inline">\theta + \psi</span>.</p></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Exponential families are <span class="blue">closed</span> under <span class="orange">exponential tilting</span>, and <span class="math inline">\mathcal{F}_{\text{ne}}^1</span> can be thought of as being generated by any of its members.</p>
</div>
</div>
</div>
</section>
<section id="moments-and-cumulants" class="slide level2 center">
<h2>Moments and cumulants</h2>
<ul>
<li><p>The functions <span class="math inline">M_0(\theta)</span> and <span class="math inline">K(\theta) = K_0(\theta)</span> of a <span class="math inline">\mathcal{F}_\text{en}^1</span>, refer to the <span class="orange">baseline</span> density <span class="math inline">f_0(y)</span>. Indeed, for any fixed <span class="math inline">\theta</span>, the <span class="blue">moment generating function</span> of <span class="math inline">f(y; \theta) \in \mathcal{F}_\text{en}^1</span> is <span class="math display">
M_\theta(t) := \int_\mathcal{Y} e^{ty} f(y; \theta)\, \nu(\mathrm{d}y)
= \frac{1}{M_0(\theta)} \int_\mathcal{Y} e^{(t + \theta)y} f_0(y)\, \nu(\mathrm{d}y)
= \frac{M_0(t + \theta)}{M_0(\theta)}, \quad t + \theta \in \tilde{\Theta}.
</span></p></li>
<li><p>Consequently, the <span class="orange">cumulant generating function</span> of <span class="math inline">f(y; \theta)</span> relates to <span class="math inline">K_0</span> as follows: <span class="math display">
K_\theta(t) = \log M_\theta(t) = K_0(t + \theta) - K_0(\theta), \quad t + \theta \in \tilde{\Theta}.
</span></p></li>
</ul>

<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>If <span class="math inline">\mathcal{F}_\text{en}^1</span> is a <span class="orange">regular</span> family, then <span class="math inline">\tilde{\Theta}</span> is an <span class="blue">open set</span>, and <span class="math inline">\tilde{\Theta} = \text{int}\:\tilde{\Theta}</span>, meaning that <span class="math inline">\theta</span> is always an <span class="blue">inner point</span> of <span class="math inline">\tilde{\Theta}</span>. Therefore, there exists a <span class="math inline">t_0</span> such that <span class="math inline">t + \theta \in \tilde{\Theta}</span> for all <span class="math inline">|t| &lt; t_0</span> implying that both <span class="math inline">M_\theta</span> and <span class="math inline">K_\theta</span> are <span class="orange">well-defined</span>.</p>
<p>If <span class="math inline">\mathcal{F}_\text{en}^1</span> is <span class="orange">not</span> regular, then for <span class="math inline">M_\theta(t)</span> and <span class="math inline">K_\theta(t)</span> to be well-defined, we require that <span class="math inline">\theta</span> is <span class="orange">not</span> a <span class="blue">boundary point</span>; that is, <span class="math inline">\theta \in \text{int}\:\tilde{\Theta}</span>, meaning it belongs to the interior of <span class="math inline">\tilde{\Theta}</span>.</p>
</div>
</div>
</div>
<aside><div>
<p>Textbooks sometimes suppress additive constants in defining <span class="math inline">K_0(\theta)</span>, e.g.&nbsp;using <span class="math inline">e^\theta</span> instead of <span class="math inline">e^\theta-1</span>. This is inconsequential (constants cancel in <span class="math inline">K_\theta(t)</span>) but somewhat misleading.</p>
</div></aside></section>
<section id="mean-value-mapping-i" class="slide level2 center">
<h2>Mean value mapping I</h2>
<ul>
<li><span class="blue">Moments</span> and <span class="orange">cumulants</span> exist for every <span class="math inline">\theta \in \text{int}\:\tilde{\Theta}</span>. In particular, the cumulants are <span class="math display">
\kappa_k = \frac{\partial^k}{\partial t^k} K_\theta(t) \Big|_{t = 0} = \frac{\partial^k}{\partial t^k} \left[ K(t + \theta) - K(\theta) \right] \Big|_{t = 0} = \frac{\partial^k}{\partial \theta^k} K(\theta), \qquad k \ge 1.
</span></li>
</ul>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y \sim f(y; \theta)</span>, with <span class="math inline">f(y; \theta) \in \mathcal{F}_\text{en}^1</span>. The first two moments of <span class="math inline">Y</span> are obtained as: <span class="math display">
\mu(\theta) := \mathbb{E}_\theta(Y) = \frac{\partial}{\partial \theta} K(\theta), \qquad \text{var}_\theta(Y) = \frac{\partial}{\partial \theta} \mu(\theta) = \frac{\partial^2}{\partial \theta^2} K(\theta),
</span> We call <span class="math inline">\mu : \text{int}\:\tilde{\Theta} \to \mathbb{R}</span> the <span class="orange">mean value mapping</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>If <span class="math inline">f_0</span> is non-degenerate, then <span class="math inline">\text{var}_\theta(Y) &gt; 0</span>, implying that <span class="math inline">K(\theta)</span> is a <span class="blue">convex function</span>, and <span class="math inline">\mu(\theta)</span> is a <span class="orange">smooth</span> and <span class="orange">monotone increasing</span>, namely is a <span class="blue">one-to-one</span> map.</p></li>
<li><p>Thus, if <span class="math inline">\mathcal{F}_\text{en}^1</span> is a <span class="orange">regular</span> exponential family, then <span class="math inline">\tilde{\Theta} = \text{int}\:\tilde{\Theta}</span> and <span class="math inline">\mu(\theta)</span> is a <span class="blue">reparametrization</span>.</p></li>
</ul>
</section>
<section id="mean-value-mapping-ii" class="slide level2 center">
<h2>Mean value mapping II</h2>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The mean value mapping has range <span class="math inline">\mathcal{M} = \text{Range}(\mu) = \{\mu(\theta) : \theta \in \text{int}\:\tilde{\Theta}\}</span>. The set <span class="math inline">\mathcal{M} \subseteq\mathbb{R}</span> is called <span class="blue">mean space</span> or <span class="orange">expectation space</span>.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">C = C(\mathcal{Y})</span> be the <span class="blue">closed convex hull</span> of the sample space <span class="math inline">\mathcal{Y}</span>, which is the <span class="blue">smallest closed convex set</span> <span class="math inline">C \subseteq \mathbb{R}</span> <span class="orange">containing</span> <span class="math inline">\mathcal{Y}</span>, namely: <span class="math display">
C(\mathcal{Y}) = \{ y \in \mathbb{R} : y = \lambda y_1 + (1 - \lambda)y_2, \quad 0 \le \lambda \le 1, \quad y_1,y_2 \in \mathcal{Y}\}.
</span></p>
</div>
</div>
</div>
<ul>
<li><p>Hence, if <span class="math inline">\mathcal{Y} = \{0, 1, \dots, N\}</span>, then <span class="math inline">C = [0,N]</span>. If <span class="math inline">\mathcal{Y} = \mathbb{N}</span>, then <span class="math inline">C = \mathbb{R}^+</span>. If <span class="math inline">\mathcal{Y} = \mathbb{R}</span>, then <span class="math inline">C = \mathbb{R}</span>.</p></li>
<li><p>Because of the properties of expectations, <span class="math inline">\mu(\theta) \in \text{int}\:C(\mathcal{Y})</span> for all <span class="math inline">\theta \in \text{int}\:\tilde{\Theta}</span>, namely <span class="math display">
\mathcal{M} \subseteq \text{int}\:C(\mathcal{Y}).
</span> Indeed, <span class="math inline">\text{int}\:C(\mathcal{Y})</span> is an <span class="orange">open interval</span> whose extremes are the <span class="blue">infimum</span> and <span class="blue">supremum</span> of <span class="math inline">\mathcal{Y}</span>.</p></li>
</ul>

<aside><div>
<p>Both definitions naturally generalize to the multivariate case when <span class="math inline">C, \mathcal{Y} \subseteq \mathbb{R}^p</span>, for <span class="math inline">p &gt; 1</span>.</p>
</div></aside></section>
<section id="mean-value-mapping-iii" class="slide level2 center">
<h2>Mean value mapping III 📖</h2>
<ul>
<li><p>In a regular exponential family, the mean value mapping <span class="math inline">\mu(\theta)</span> is a <span class="orange">reparametrization</span>, meaning that for each <span class="math inline">\theta \in \tilde{\Theta}</span>, there exists a <span class="blue">unique</span> mean <span class="math inline">\mu \in \mathcal{M}</span> such that <span class="math inline">\mu = \mu(\theta)</span>.</p></li>
<li><p>Moreover, in regular families, a much stronger result holds: for each value of <span class="math inline">y \in \text{int}\:C(\mathcal{Y})</span>, there exists a <span class="orange">unique</span> <span class="math inline">\theta \in \tilde{\Theta}</span> such that <span class="math inline">\mu(\theta) = y</span>.</p></li>
</ul>
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem (<span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#/references" role="doc-biblioref" onclick="">1997</a>)</span>, Theorem 5.1)</strong></p>
</div>
<div class="callout-content">
<p>If <span class="math inline">\mathcal{F}_\text{en}^1</span> is regular, then <span class="math inline">\Theta = \text{int}\:\tilde{\Theta} = \tilde{\Theta}</span> and <span class="math inline">\mathcal{M} = \text{int}\:C.</span></p>
</div>
</div>
</div>
<ul>
<li><p>This establishes a <span class="blue">duality</span> between the expectation space <span class="math inline">\mathcal{M}</span> and the sample space. Any value in <span class="math inline">\text{int}\:C</span> can be “reached”, that is, there exists a distribution <span class="math inline">f(y; \theta)</span> with that mean.</p></li>
<li><p>This correspondence is crucial in maximum likelihood estimation and inference.</p></li>
</ul>

<aside><div>
<p>This theorem can actually be strengthened: a necessary and sufficient condition for <span class="math inline">\mathcal{M} = \text{int}\:C</span> is that the family <span class="math inline">\mathcal{F}_\text{en}^1</span> is <span class="orange">steep</span> (a regular family is also steep); see <span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#/references" role="doc-biblioref" onclick="">1997</a>)</span>.</p>
</div></aside></section>
<section id="a-non-regular-and-non-steep-exponential-family" class="slide level2 center">
<h2>A non regular and non steep exponential family</h2>
<ul>
<li>Let us a consider an exponential family <span class="math inline">\mathcal{F}_\text{en}^1</span> generated by the density <span class="math display">
f_0(y) = c \frac{e^{-|y|}}{1 + y^4}, \qquad y \in \mathbb{R}.
</span> for some normalizing constant <span class="math inline">c &gt; 0</span>. The <span class="orange">exponential tilting</span> of <span class="math inline">f_0</span> gives <span class="math display">
f(y; \theta) \propto e^{\theta y}f_0(y) \propto \frac{e^{-|y| + \theta y}}{1 + y^4}, \qquad y \in \mathbb{R}, \quad \theta \in \tilde{\Theta}.
</span></li>
<li>The function <span class="math inline">M_0(\theta)</span> is unavailable in closed form, however <span class="math inline">\tilde{\Theta} = [-1,1]</span> since <span class="math display">
M_0(\theta) &lt; \infty, \qquad \theta \in  [-1, 1].
</span></li>
<li>Since <span class="math inline">\tilde{\Theta}</span> is a <span class="blue">closed set</span>, the exponential family is <span class="orange">not regular</span> (and is not steep either). In fact, one can show that <span class="math inline">\lim_{\theta \to 1} \mu(\theta) = a &lt; \infty</span>, implying that <span class="math display">
\mathcal{M} = (-a, a), \qquad \text{ whereas } \qquad \text{int}\:C = \mathbb{R}.
</span></li>
<li>In other words, there are no values of <span class="math inline">\theta</span> such that <span class="math inline">\mu(\theta) = y</span> for any <span class="math inline">y &gt; a</span>, which implies, for instance, that the method of moments will encounter difficulties in estimating <span class="math inline">\theta</span>.</li>
</ul>
</section>
<section id="variance-function-i" class="slide level2 center">
<h2>Variance function I 📖</h2>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y \sim f(y; \theta)</span>, with <span class="math inline">f(y; \theta) \in \mathcal{F}_\text{en}^1</span> and let <span class="math inline">\theta(\mu)</span> be the <span class="orange">inverse map</span> of <span class="math inline">\mu(\theta)</span>. The variance of <span class="math inline">Y</span> can be expressed as a function of <span class="math inline">\mu</span>: <span class="math display">
V(\mu) := \text{var}_{\theta(\mu)}(Y) = \frac{\partial^2}{\partial \theta^2} K(\theta) \Big|_{\theta = \theta(\mu)}.
</span> The function <span class="math inline">V : \mathcal{M} \to \mathbb{R}^+</span> is called the <span class="orange">variance function</span> of the exponential family <span class="math inline">\mathcal{F}_\text{en}^1</span>.</p>
</div>
</div>
</div>
<ul>
<li>The importance of the variance function <span class="math inline">V(\mu)</span> is related to the following <span class="blue">characterization</span> result due to <span class="citation" data-cites="Morris1982">Morris (<a href="#/references" role="doc-biblioref" onclick="">1982</a>)</span>.</li>
</ul>
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem (<span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#/references" role="doc-biblioref" onclick="">1997</a>)</span>, Theorem 5.2)</strong></p>
</div>
<div class="callout-content">
<p>If <span class="math inline">Y</span> has a density that belongs to a <span class="math inline">\mathcal{F}_\text{en}^1</span>, then the <span class="orange">pair</span> <span class="math inline">(\mathcal{M}, V(\mu))</span> <span class="blue">uniquely</span> determine the natural parameter space <span class="math inline">\tilde{\Theta}</span> and the cumulant generating function <span class="math inline">K(\theta)</span>, and hence also <span class="math inline">f(y;\theta)</span>.</p>
</div>
</div>
</div>
</section>
<section id="variance-function-ii" class="slide level2 center">
<h2>Variance function II 📖</h2>
<ul>
<li><p>The characterization theorem of <span class="citation" data-cites="Morris1982">Morris (<a href="#/references" role="doc-biblioref" onclick="">1982</a>)</span> is <span class="blue">constructive</span> in nature, as its <span class="orange">proof</span> provides a practical way of determining <span class="math inline">K(\theta)</span> from <span class="math inline">(\mathcal{M}, V(\mu))</span>. In particular, the function <span class="math inline">K(\cdot)</span> must satisfy <span class="math display">
K\left(\int_{\mu_0}^\mu \frac{1}{V(m)}\mathrm{d}m\right) = \int_{\mu_0}^\mu \frac{m}{V(m)}\mathrm{d}m,
</span> where <span class="math inline">\mu_0</span> is an arbitrary point in <span class="math inline">\mathcal{M}</span>.</p></li>
<li><p>For example, let <span class="math inline">\mathcal{M} = (0, \infty)</span> and <span class="math inline">V(\mu) = \mu^2</span>. Then, choosing <span class="math inline">\mu_0=1</span> gives <span class="math display">
K\left(1 - \frac{1}{\mu}\right) = \log\mu,
</span> and therefore <span class="math inline">\theta(\mu) = 1 - 1/\mu</span>, giving <span class="math inline">\tilde{\Theta} = (-\infty, 1)</span> and <span class="math inline">\mu(\theta) = (1 - \theta)^{-1}</span>. Hence we obtain <span class="math inline">K(\theta) = -\log(1 - \theta)</span>, which corresponds to the exponential density <span class="math inline">f_0(y) = e^{-y}</span>, for <span class="math inline">y &gt; 0</span>.</p></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>In order to identify <span class="math inline">\mathcal{F}_\text{en}^1</span> <span class="orange">both</span> <span class="math inline">\mathcal{M}</span> and <span class="math inline">V(\mu)</span> must be known.</p>
</div>
</div>
</div>
</section>
<section id="well-known-exponential-families" class="slide level2 center">
<h2>Well-known exponential families</h2>
<table class="caption-top">
<colgroup>
<col style="width: 23%">
<col style="width: 9%">
<col style="width: 8%">
<col style="width: 33%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Notation</th>
<th><span class="math inline">\text{N}(\psi, 1)</span></th>
<th><span class="math inline">\text{Poisson}(\psi)</span></th>
<th><span class="math inline">\text{Bin}(N, \psi)</span></th>
<th><span class="math inline">\text{Gamma}(\nu,\psi), \nu &gt; 0</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\mathcal{Y}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">\mathbb{N}</span></td>
<td><span class="math inline">\{0, 1, \dots, N\}</span></td>
<td><span class="math inline">(0, \infty)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="orange">Natural param.</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\theta(\psi)</span></td>
<td><span class="math inline">\psi</span></td>
<td><span class="math inline">\log{\psi}</span></td>
<td><span class="math inline">\log\{\psi/(1 - \psi)\}</span></td>
<td><span class="math inline">-\psi</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">f_0(y)</span></td>
<td><span class="math inline">(\sqrt{2\pi})^{-1}e^{-\frac{1}{2}y^2}</span></td>
<td><span class="math inline">e^{-1}/ y!</span></td>
<td><span class="math inline">\binom{N}{y}\left(\frac{1}{2}\right)^N</span></td>
<td><span class="math inline">y^{\nu - 1}e^{-y}/\Gamma(\nu)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">K(\theta)</span></td>
<td><span class="math inline">\theta^2/2</span></td>
<td><span class="math inline">e^\theta-1</span></td>
<td><span class="math inline">N \log(1 + e^\theta) - N\log{2}</span></td>
<td><span class="math inline">-\nu \log(1-\theta)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\tilde{\Theta}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">(-\infty, 0)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="blue">Mean param.</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\mu(\theta)</span></td>
<td><span class="math inline">\theta</span></td>
<td><span class="math inline">e^\theta</span></td>
<td><span class="math inline">N e^\theta/(1 + e^{\theta})</span></td>
<td><span class="math inline">-\nu/\theta</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\mathcal{M}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">(0, \infty)</span></td>
<td><span class="math inline">(0, N)</span></td>
<td><span class="math inline">(0, \infty)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">V(\mu)</span></td>
<td><span class="math inline">1</span></td>
<td><span class="math inline">\mu</span></td>
<td><span class="math inline">\mu(1 - \mu/ N)</span></td>
<td><span class="math inline">\mu^2/\nu</span></td>
</tr>
</tbody>
</table>
</section>
<section id="quadratic-variance-functions" class="slide level2 center">
<h2>Quadratic variance functions</h2>
<ul>
<li><p>There is more in <span class="citation" data-cites="Morris1982">Morris (<a href="#/references" role="doc-biblioref" onclick="">1982</a>)</span>’s paper. Specifically, he focused on a subclass of <span class="orange">quadratic</span> variance functions, which can be written as <span class="math display">
V(\mu) = a + b\mu + c\mu^2,
</span> for some known constants <span class="math inline">a</span>, <span class="math inline">b</span>, and <span class="math inline">c</span>.</p></li>
<li><p><span class="citation" data-cites="Morris1982">Morris (<a href="#/references" role="doc-biblioref" onclick="">1982</a>)</span> showed that, up to transformations such as convolution, there exist only <span class="blue">six families</span> within <span class="math inline">\mathcal{F}_\text{en}^1</span> that possess a <span class="orange">quadratic variance</span> function. These are: (i) the normal, (ii) the Poisson, (iii) the gamma, (iv) the binomial, (v) the negative binomial, and (vi) a sixth family.</p></li>
<li><p>The sixth (less known) distribution is called the <span class="blue">generalized hyperbolic secant</span>, and it has density <span class="math display">
f(y; \theta) = \frac{\exp\left\{\theta y - \log\cos{\theta}\right\}}{2\cosh(\pi y/2)}, \qquad y \in \mathbb{R}, \quad \theta \in (-\pi/2, \pi/2),
</span> with <span class="orange">mean</span> function <span class="math inline">\mu(\theta) = \tan{\theta}</span> and <span class="blue">variance</span> function <span class="math inline">V(\mu) = \csc^2(\theta) = 1 + \mu^2</span>, and <span class="math inline">\mathcal{M} = \mathbb{R}</span>. It is also a <span class="orange">regular</span> exponential family.</p></li>
</ul>
</section>
<section id="a-general-definition-of-exponential-families-i" class="slide level2 center">
<h2>A general definition of exponential families I</h2>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">h(y) &gt; 0</span>, <span class="math inline">s(y)</span>, be real-valued functions not depending on <span class="math inline">\psi</span> and let <span class="math inline">\theta(\psi), G(\psi)</span> be real-valued functions not depending on <span class="math inline">y</span>. The parametric family <span class="math display">
\mathcal{F}_{\text{e}}^1 = \left\{f(y;\psi) = h(y)\exp\{\theta(\psi) s(y) - G(\psi)\}, \quad y \in \mathcal{Y}\subseteq \mathbb{R}, \: \psi \in \Psi \right\},
</span> is called a <span class="orange">exponential family</span> of order one, where the normalizing constant is <span class="math display">
\exp{G(\psi)} = \int_\mathcal{Y} h(y) \exp\{\theta(\psi) s(y)\} \nu(\mathrm{d}y).
</span> The family is <span class="blue">full</span> if the parameter space <span class="math inline">\Psi</span> is the widest possible <span class="math inline">\tilde{\Psi} = \{\psi \subseteq\mathbb{R}: G(\psi) &lt; \infty\}</span>.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Suppose <span class="math inline">f(y; \psi) \in \mathcal{F}_\text{e}^1</span>. Then, the function <span class="math inline">\theta(\psi)</span> must be a <span class="blue">one-to-one</span> mapping, that is, a <span class="orange">reparametrization</span>, otherwise, the model would <span class="orange">not</span> be <span class="orange">identifiable</span>. Hence, we can write: <span class="math display">
f(y; \psi) = h(y)\exp\{\theta(\psi) s(y) - \tilde{G}(\theta(\psi))\},
</span> for some function <span class="math inline">\tilde{G}(\cdot)</span> such that <span class="math inline">G(\psi) = \tilde{G}(\theta(\psi))</span>.</p>
</div>
</div>
</div>
</section>
<section id="a-general-definition-of-exponential-families-ii" class="slide level2 center">
<h2>A general definition of exponential families II</h2>
<ul>
<li><p>When <span class="math inline">s(y)</span> is an arbitrary function of <span class="math inline">y</span>, then <span class="math inline">\mathcal{F}_\text{e}^1</span> is <span class="blue">broader</span> than <span class="math inline">\mathcal{F}_\text{en}^1</span>.</p></li>
<li><p>Without loss of generality, we can focus on the natural parametrization <span class="math inline">\theta \in \Theta</span> and a density baseline <span class="math inline">h(y) = f_0(y)</span>, meaning that <span class="math inline">f(y;\theta) \in \mathcal{F}_\text{e}^1</span> can be written as <span class="math display">
f(y; \theta) = f_0(y)\exp\{\theta s(y) - K(\theta)\},
</span> because the general case would be a <span class="orange">reparametrization</span> of this one.</p></li>
<li><p>Let <span class="math inline">Y \sim f(y; \theta)</span>, with <span class="math inline">f(y; \theta) \in \mathcal{F}_\text{e}^1</span>. Then, the random variable <span class="math inline">S = s(Y)</span> has density <span class="math display">
f_S(s; \psi) = \tilde{f}_0(s)\exp\{\theta s - K(\theta)\},
</span> for some baseline density <span class="math inline">\tilde{f}_0(s)</span>, namely <span class="math inline">f_S(s; \psi) \in \mathcal{F}_\text{en}^1</span>. If in addition <span class="math inline">s(y)</span> is a <span class="orange">one-to-one</span> invertible mapping, this means <span class="math inline">Y = s^{-1}(S)</span> is just a transformation of an <span class="math inline">\mathcal{F}_\text{en}^1</span>.</p></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>A full exponential family <span class="math inline">\mathcal{F}_\text{e}^1</span> is, technically, a broader definition, but in practice it leads to a <span class="orange">reparametrization</span> of a natural exponential family <span class="math inline">\mathcal{F}_\text{en}^1</span> in a <span class="blue">transformed space</span> <span class="math inline">s(Y)</span>.</p>
</div>
</div>
</div>
<!-- ## Independent sampling -->
<!-- - Let $Y_1,\dots,Y_n$ be iid random variables with density $f(y; \theta)$, where $f(y; \theta) \in \mathcal{F}_\text{e}^1$ is a full exponential family. , assume the density can be written as -->
<!-- $$ -->
<!-- f(y; \theta) = h(y)\exp\{\theta s(y) - G(\theta)\}, \qquad \theta \in \tilde{\Theta}. -->
<!-- $$ -->
<!-- - The [likelihood]{.blue} function is -->
<!-- $$ -->
<!-- L(\theta; \bm{y}) = \prod_{i=1}^n \exp\left\{\theta s(y_i) - G(\theta)\right\} = \exp\left\{\theta \sum_{i=1}^n s(y_i) - n G(\theta)\right\}, -->
<!-- $$ -->
<!-- from which we see that $s = \sum_{i=1}^n s(y_i)$ is the [minimal sufficient statistic]{.orange} for $\theta$. -->
<!-- - Inference can therefore be based on the random variable $S = \sum_{i=1}^n s(Y_i)$, whose distribution is -->
<!-- $$ -->
<!-- f_S(s; \theta) = \tilde{f}_0(s)\exp\{\theta s - K(\theta)\}, -->
<!-- $$ -->
<!-- with $K(\theta) = n G(\theta)$ and for some density $\tilde{f}_0(s)$. That is, the distribution of the minimal sufficient statistic $f_S(s; \theta)$ is itself a [natural exponential family]{.blue} of order one. -->
</section></section>
<section>
<section id="multiparameter-exponential-families" class="title-slide slide level1 center">
<h1>Multiparameter exponential families</h1>

</section>
<section id="natural-exponential-families-of-order-p" class="slide level2 center">
<h2>Natural exponential families of order <span class="math inline">p</span></h2>
<ul>
<li><p>Let <span class="math inline">Y</span> be a <span class="orange">non-degenerate</span> random variable with <span class="blue">support</span> <span class="math inline">\mathcal{Y} \subseteq \mathbb{R}^p</span> and <span class="orange">density</span> <span class="math inline">f_0(y)</span> with respect to a dominating measure <span class="math inline">\nu(\mathrm{d}y)</span>.</p></li>
<li><p>Let us define the mapping <span class="math inline">M_0:\mathbb{R}^p\rightarrow (0,\infty]</span> <span class="math display">
M_0(\theta):=\int_\mathcal{Y}e^{\theta^T y}f_0(y)\nu(\mathrm{d}y), \qquad \theta \in \mathbb{R}^p.
</span></p></li>
</ul>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The parametric family generated via <span class="orange">exponential tilting</span> of a density <span class="math inline">f_0</span> <span class="math display">
\mathcal{F}_{\text{ne}}^p = \left\{f(y;\theta) = \frac{e^{\theta^T y}f_0(y)}{M_0(\theta)} = f_0(y)\exp\{\theta^T y - K(\theta)\}, \quad y \in \mathcal{Y}\subseteq \mathbb{R}^p, \:\theta \in \tilde{\Theta} \right\},
</span> is called a <span class="orange">natural exponential family</span> of order one, <span class="math inline">K(\theta) = \log M_0(\theta)</span> and <span class="math inline">\tilde{\Theta} = \{\theta \in \mathbb{R}^p : K(\theta) &lt; \infty\}</span> is the <span class="blue">natural parameter space</span>.</p>
</div>
</div>
</div>
<ul>
<li>The family <span class="math inline">\mathcal{F}_{\text{ne}}^p</span> is said to be <span class="blue">full</span>, whereas a subfamily of <span class="math inline">\mathcal{F}_{\text{ne}}^p</span> with <span class="math inline">\Theta \subseteq \tilde{\Theta}</span> is <span class="blue">non-full</span>. Moreover, the family <span class="math inline">\mathcal{F}_{\text{ne}}^p</span> is said to be <span class="orange">regular</span> if <span class="math inline">\tilde{\Theta}</span> is an open set.</li>
</ul>
</section>
<section id="example-multinomial-distribution-i" class="slide level2 center">
<h2>Example: multinomial distribution I 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y = (Y_1,\dots,Y_{p-1}) \sim \text{Multinom}(N; 1/p,\dots,1/p)</span> be a <span class="orange">multinomial</span> random vector with <span class="blue">uniform probabilities</span>, so that its density <span class="math inline">f_0</span> is<br>
<span class="math display">
f_0(y) = \frac{N!}{y_1!\cdots y_p!}\left(\frac{1}{p}\right)^N, \qquad y = (y_1,\dots,y_{p-1}) \in \mathcal{Y} \subseteq \mathbb{R}^{p-1},
</span> where <span class="math inline">\mathcal{Y} = \{(y_1,\dots,y_{p-1}) \in \{0,\dots,N\}^{p-1} : \sum_{j=1}^{p-1} y_j \le N\}</span>, having set <span class="math inline">y_p := N - \sum_{j=1}^{p-1} y_j</span>.</p></li>
<li><p>The <span class="orange">exponential tilting</span> of <span class="math inline">f_0</span> yields <span class="math display">
f(y; \theta) \propto f_0(y) e^{\theta^T y} = \frac{N!}{y_1!\cdots y_p!}\left(\frac{1}{p}\right)^N e^{\theta_1 y_1 + \cdots + \theta_{p-1} y_{p-1}}, \qquad y \in \mathcal{Y}, \;\theta \in \mathbb{R}^{p-1}.
</span></p></li>
<li><p>As a consequence of the <span class="orange">multinomial theorem</span>, the normalizing constant, that is, the <span class="blue">moment generating function</span>, is<br>
<span class="math display">
M_0(\theta) = \mathbb{E}\left(e^{\theta^T Y}\right) = \left(\frac{1}{p}\right)^N(1 + e^{\theta_1} + \cdots + e^{\theta_{p-1}})^N.
</span> Thus <span class="math inline">M_0(\theta) &lt; \infty</span> for all <span class="math inline">\theta \in \mathbb{R}^{p-1}</span> and the <span class="orange">natural parameter space</span> is the <span class="blue">open set</span> <span class="math inline">\tilde{\Theta} = \mathbb{R}^{p-1}</span>.</p></li>
</ul>
</section>
<section id="example-multinomial-distribution-ii" class="slide level2 center">
<h2>Example: multinomial distribution II 📖</h2>
<ul>
<li><p>The resulting <span class="orange">tilted</span> density is <span class="math display">
f(y; \theta) = f_0(y)e^{\theta^Ty - K(\theta)} = \frac{N!}{y_1!\cdots y_p!}\frac{e^{\theta_1 y_1 + \cdots + \theta_{p-1}y_{p-1}}}{(1 + e^{\theta_1} + \cdots + e^{\theta_{p-1}})^N},
</span> where <span class="math inline">K(\theta) = \log{M_0(\theta)} = N\log(1 + e^{\theta_1} + \cdots + e^{\theta_{p-1}}) - N\log{p}</span>.</p></li>
<li><p>In other words, the tilted density is again a <span class="blue">multinomial</span> distribution with parameters <span class="math inline">N</span> and <span class="orange">probabilities</span> <span class="math inline">\pi_j = e^{\theta_j} / (1 + e^{\theta_1} + \cdots + e^{\theta_{p-1}})</span>. In fact, we can write: <span class="math display">
\begin{aligned}
f(y; \theta) &amp;= \frac{N!}{y_1!\cdots y_p!}\frac{e^{\theta_1 y_1}  \cdots  e^{\theta_p y_p}}{(\sum_{j=1}^p e^{\theta_j})^{y_1} \cdots (\sum_{j=1}^p e^{\theta_j})^{y_p}} = \frac{N!}{y_1!\cdots y_p!} \prod_{j=1}^p\left(\frac{e^{\theta_j}}{\sum_{k=1}^p e^{\theta_k}}\right)^{y_j} \\
&amp;= \frac{N!}{y_1!\cdots y_p!} \prod_{j=1}^p\pi_j^{y_j}.
\end{aligned}
</span> where we defined <span class="math inline">\theta_p := 0</span>, so that <span class="math inline">\sum_{j=1}^pe^{\theta_j} = 1 + e^{\theta_1} + \cdots + e^{\theta_{p-1}}</span>, recalling that <span class="math inline">\sum_{j=1}^py_j = N</span>.</p></li>
<li><p>The tilted density belongs to a regular <span class="blue">natural exponential family</span> <span class="math inline">\mathcal{F}_\text{en}^{p-1}</span> of <span class="orange">order</span> <span class="math inline">p-1</span>.</p></li>
</ul>
</section>
<section id="example-independent-exponential-families" class="slide level2 center">
<h2>Example: independent exponential families 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y = (Y_1,\dots,Y_p)</span> be a random vector of <span class="blue">independent</span> random variables, each belonging to a <span class="orange">full natural exponential family</span> <span class="math inline">\mathcal{F}_\text{en}^1</span> of order 1, with density <span class="math display">
f(y_j; \theta_j) = f_j(y_j)\exp\{\theta_j y_j - K_j(\theta_j)\}, \qquad \theta_j \in \tilde{\Theta}_j.
</span></p></li>
<li><p>Let <span class="math inline">\theta = (\theta_1,\dots,\theta_p)</span>. Because of independence, the <span class="blue">joint distribution</span> of <span class="math inline">Y</span> is <span class="math display">
\begin{aligned}
f(y;\theta) &amp;= \prod_{j=1}^p f(y_j;\theta_j) = \prod_{j=1}^p f_j(y_j)\exp\{\theta_j y_j - K_j(\theta_j)\} \\
&amp;= \left[\prod_{j=1}^p f_j(y_j)\right] \exp\left\{\sum_{j=1}^p \theta_j y_j - \sum_{j=1}^p K_j(\theta_j)\right\} \\
&amp;= f_0(y) \exp\{\theta^T y - K(\theta)\},
\end{aligned}
</span> where <span class="math inline">f_0(y) = \prod_{j=1}^p f_j(y_j)</span>, <span class="math inline">K(\theta) = \sum_{j=1}^p K_j(\theta_j)</span>, and the <span class="orange">natural parameter space</span> is <span class="math display">
\tilde{\Theta} = \tilde{\Theta}_1 \times \cdots \times \tilde{\Theta}_p.
</span></p></li>
<li><p>Thus, <span class="math inline">f(y;\theta)</span> is an <span class="math inline">\mathcal{F}_\text{en}^p</span>, in which <span class="math inline">K(\theta)</span> is a <span class="blue">separable</span> function.</p></li>
</ul>
</section>
<section id="mean-value-mapping-and-other-properties" class="slide level2 center">
<h2>Mean value mapping and other properties</h2>
<!-- - Properties of moments and cumulants in the one-parameter case carry over to the multi-parameter setting, with the obvious adjustments. -->
<ul>
<li><p>Let <span class="math inline">Y \sim f(y; \theta)</span>, with <span class="math inline">f(y; \theta) \in \mathcal{F}_\text{en}^p</span>. The cumulant generating function is<br>
<span class="math display">
K_\theta(t) = \log M_\theta(t) = K_0(t + \theta) - K_0(\theta), \qquad t + \theta \in \tilde{\Theta}.
</span> In particular, the first two moments of <span class="math inline">Y</span> are obtained as:<br>
<span class="math display">
\mu(\theta) := \mathbb{E}_\theta(Y) = \frac{\partial}{\partial \theta} K(\theta), \qquad \text{var}_\theta(Y) = \frac{\partial}{\partial \theta^\top} \mu(\theta) = \frac{\partial^2}{\partial \theta \partial \theta^\top} K(\theta),
</span></p></li>
<li><p>If <span class="math inline">f_0</span> is non-degenerate, then the <span class="blue">covariance matrix</span> <span class="math inline">\text{var}_\theta(Y)</span> is <span class="orange">positive definite</span>, implying that <span class="math inline">K(\theta)</span> is a <span class="blue">convex function</span>, and <span class="math inline">\mu(\theta)</span> is a <span class="orange">smooth</span> <span class="blue">one-to-one</span> map.</p></li>
<li><p>The definitions of mean value mapping <span class="math inline">\mu(\theta)</span>, its range <span class="math inline">\mathcal{M}</span>, the convex hull <span class="math inline">C(\mathcal{Y})</span> of the sample space, and the variance function <span class="math inline">V(\mu)</span> also naturally extend to the multi-parameter setting.</p></li>
<li><p>Refer to <span class="citation" data-cites="Jorgensen1987">Jorgensen (<a href="#/references" role="doc-biblioref" onclick="">1987</a>)</span> for an extension of the results of <span class="citation" data-cites="Morris1982">Morris (<a href="#/references" role="doc-biblioref" onclick="">1982</a>)</span> about <span class="math inline">V(\mu)</span>.</p></li>
</ul>
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem (<span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#/references" role="doc-biblioref" onclick="">1997</a>)</span>, Theorem 5.3)</strong></p>
</div>
<div class="callout-content">
<p>If <span class="math inline">\mathcal{F}_\text{en}^p</span> is regular, then <span class="math inline">\mathcal{M} = \text{int}\:C.</span></p>
</div>
</div>
</div>
</section>
<section id="independence-of-the-components" class="slide level2 center">
<h2>Independence of the components</h2>
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem (<span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#/references" role="doc-biblioref" onclick="">1997</a>)</span>, Theorem 5.4)</strong></p>
</div>
<div class="callout-content">
<p>If the natural observations of an <span class="math inline">\mathcal{F}_\text{en}^p</span> are <span class="blue">independent</span> for some <span class="math inline">\theta_0 \in \tilde{\Theta}</span>, then this is also true for every <span class="math inline">\theta \in \tilde{\Theta}</span>.</p>
</div>
</div>
</div>
<ul>
<li>This theorem essentially establishes that if the baseline density <span class="math inline">f_0(\cdot)</span> has independent components, then the exponential tilting preserves independence.</li>
</ul>
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem (<span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#/references" role="doc-biblioref" onclick="">1997</a>)</span>, Theorem 5.5)</strong></p>
</div>
<div class="callout-content">
<p>If, for every <span class="math inline">\theta \in \tilde{\Theta}</span>, the natural observations of a <span class="blue">regular</span> <span class="math inline">\mathcal{F}_\text{en}^p</span> are <span class="blue">uncorrelated</span>, then they are also <span class="orange">independent</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>This generalizes a well-known fact about multivariate Gaussians, which are in fact an <span class="math inline">\mathcal{F}_\text{en}^p</span>.</p></li>
<li><p>In practice, if the Hessian matrix of <span class="math inline">K(\theta)</span> is <span class="blue">diagonal</span>, then the natural observations are <span class="orange">independent</span>. This occurs whenever <span class="math inline">K(\theta)</span> is <span class="orange">separable</span>.</p></li>
</ul>
</section>
<section id="marginal-and-conditional-distributions" class="slide level2 center">
<h2>Marginal and conditional distributions</h2>
<ul>
<li><p>Consider a <span class="math inline">\mathcal{F}_\text{en}^p</span> family, so that <span class="math inline">f(y; \theta) = f_0(y) \exp\{\theta^T y - K(\theta)\}</span>.</p></li>
<li><p>Let <span class="math inline">y = (t, u)</span> be a <span class="blue">partition</span> of the natural observations <span class="math inline">y</span>, where <span class="math inline">t</span> has <span class="math inline">k</span> components and <span class="math inline">u</span> has <span class="math inline">p-k</span> components. Let us partition <span class="math inline">\theta</span> accordingly, so that <span class="math inline">\theta = (\tau, \zeta)</span> and <span class="math display">
f(y; \tau, \zeta) = f_0(y) \exp\{\tau^T t + \zeta^T u - K(\tau, \zeta)\}, \qquad (\tau, \zeta) \in \tilde{\Theta}.
</span></p></li>
</ul>
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem (<span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#/references" role="doc-biblioref" onclick="">1997</a>)</span>, Theorem 5.6)</strong></p>
</div>
<div class="callout-content">
<ol type="i">
<li><p>The family of <span class="blue">marginal</span> distributions of <span class="math inline">U</span> is an <span class="math inline">\mathcal{F}_\text{en}^{p-k}</span> for every fixed value of <span class="math inline">\tau</span> and <span class="math display">
f_U(u; \tau, \zeta) = h_\tau(u) \exp\{\zeta^T u - K_\tau(\zeta)\}.
</span></p></li>
<li><p>The family of <span class="orange">conditional</span> distributions of <span class="math inline">T</span> given <span class="math inline">U = u</span> is an <span class="math inline">\mathcal{F}_\text{en}^k</span> and the conditional densities do not depend on <span class="math inline">\zeta</span>, that is <span class="math display">
f_{T \mid U = u}(t; u, \tau) = h_u(t) \exp\{\tau^T t - K_u(\tau)\}, \quad \exp{K_u(\tau)} = \mathbb{E}_0\left(e^{\tau^T T} \mid U = u\right).
</span></p></li>
</ol>
</div>
</div>
</div>
</section>
<section id="conditional-likelihoods" class="slide level2 center">
<h2>Conditional likelihoods</h2>
<ul>
<li><p>The former result on marginal and conditional laws is not just an elegant probabilistic fact. Indeed, it has meaningful inferential applications.</p></li>
<li><p>Often, we can split the parameter vector <span class="math inline">\theta</span> into a <span class="blue">parameter of interest</span> <span class="math inline">\tau</span> and a <span class="orange">nuisance parameter</span> <span class="math inline">\zeta</span>. We are not interested in learning <span class="math inline">\zeta</span>.</p></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The main idea relies on noticing that <span class="math inline">f_{T \mid U = u}(t; u, \tau) = h_u(t) \exp\{\tau^T t - K_u(\tau)\}</span> does not involve <span class="math inline">\zeta</span> and therefore we could define a <span class="orange">conditional likelihood</span> based on <span class="math inline">f_{T \mid U = u}</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>A practical drawback of this approach is that the conditional cumulant generating function <span class="math inline">K_u(\tau)</span> is not always available in closed form, albeit with notable exceptions.</p></li>
<li><p>The approach is valid, in the sense that a likelihood based on <span class="math inline">f_{T \mid U = u}</span> is a <span class="blue">genuine likelihood</span>. On the other hand, note that the full likelihood would be based on <span class="math display">
f(y; \tau, \zeta) = f_U(u; \tau, \zeta) \, f_{T \mid U = u}(t; u, \tau),
</span> and thus the conditional likelihood is <span class="orange">discarding information</span>, that is, it neglects <span class="math inline">f_U(u; \tau, \zeta)</span>.</p></li>
</ul>
</section>
<section id="a-general-definition-of-exponential-families-i-1" class="slide level2 center">
<h2>A general definition of exponential families I</h2>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">s_1(y), \dots, s_p(y)</span> and <span class="math inline">h(y) &gt; 0</span> be real-valued functions not depending on the parameter <span class="math inline">\psi</span>, and let <span class="math inline">\theta_1(\psi), \dots, \theta_p(\psi)</span>, <span class="math inline">G(\psi)</span> be real-valued functions not depending on <span class="math inline">y</span>. The family <span class="math display">
\mathcal{F}_{\text{e}}^p = \left\{ f(y; \psi) = h(y) \exp\{\theta(\psi)^T s(y) - G(\psi)\}, \quad y \in \mathcal{Y} \subseteq \mathbb{R}^p, \: \psi \in \Psi \subseteq \mathbb{R}^q \right\},
</span> is called an <span class="orange">exponential family</span> of order <span class="math inline">p</span>, where the normalizing constant is <span class="math display">
\exp{G(\psi)} = \int_{\mathcal{Y}} h(y) \exp\{\theta(\psi)^T s(y)\} \nu(\mathrm{d}y).
</span> The notation <span class="math inline">\mathcal{F}_\text{e}^p</span> is understood to indicate a <span class="blue">minimal representation</span>, i.e., such that there is <span class="orange">no linear dependence</span> between <span class="math inline">1, s_1(y), \dots, s_p(y)</span> or, equivalently, between <span class="math inline">1, \theta_1(\psi), \dots, \theta_p(\psi)</span>.</p>
</div>
</div>
</div>
<ul>
<li>If <span class="math inline">q &gt; p</span>, then <span class="math inline">\psi</span> is <span class="orange">not identifiable</span> and this possibility should be discarded.</li>
<li>If <span class="math inline">q = p</span>, then <span class="math inline">\theta(\psi)</span> must be a one-to-one mapping, i.e., a <span class="blue">reparametrization</span>, otherwise the model is again <span class="orange">not identifiable</span>.</li>
<li>If <span class="math inline">q &lt; p</span>, we have a <span class="math inline">(p,q)</span>-<span class="blue">curved exponential family</span>, which corresponds to a <span class="orange">restriction</span> of the natural parameter space.</li>
</ul>
</section>
<section id="curved-exponential-families" class="slide level2 center">
<h2>Curved exponential families</h2>

<img data-src="img/efron2.png" class="quarto-figure quarto-figure-center r-stretch" style="width:6in"><ul>
<li>Figure 4.1 of <span class="citation" data-cites="Efron2023">Efron (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span>, Chapter 4. Three levels of statistical modeling, now with a fourth level added representing curved exponential families.</li>
</ul>
</section>
<section id="a-general-definition-of-exponential-families-ii-1" class="slide level2 center">
<h2>A general definition of exponential families II</h2>
<ul>
<li><p>We refer to <span class="citation" data-cites="Efron2023">Efron (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span>, Chapter 4, for a detailed discussion on curved exponential families. From now on, we will focus on the <span class="math inline">p = q</span> case.</p></li>
<li><p>Without loss of generality, we can focus on the <span class="blue">natural parametrization</span> <span class="math inline">\theta \in \Theta \subseteq \mathbb{R}^p</span> and baseline density <span class="math inline">h(y) = f_0(y)</span>, meaning that <span class="math inline">f(y;\theta) \in \mathcal{F}_\text{e}^p</span> can be written as <span class="math display">
f(y; \theta) = f_0(y)\exp\{\theta^T s(y) - K(\theta)\},
</span> because the general case would be a <span class="orange">reparametrization</span> of this one.</p></li>
<li><p>Let <span class="math inline">Y \sim f(y; \theta)</span>, with <span class="math inline">f(y; \theta) \in \mathcal{F}_\text{e}^p</span>. Then, the <span class="blue">random vector</span> <span class="math inline">S = s(Y) = (s_1(Y), \dots, s_p(Y))</span> has density <span class="math display">
f_S(s; \theta) = \tilde{f}_0(s)\exp\{\theta^T s - K(\theta)\},
</span> for some baseline density <span class="math inline">\tilde{f}_0(s)</span>, namely <span class="math inline">f_S(s; \theta) \in \mathcal{F}_\text{en}^p</span>. If in addition <span class="math inline">s(y)</span> is a <span class="orange">one-to-one</span> invertible mapping, this means <span class="math inline">Y = s^{-1}(S)</span> is just a transformation of an <span class="math inline">\mathcal{F}_\text{en}^p</span>.</p></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>As in the single parameter case, a full exponential family <span class="math inline">\mathcal{F}_\text{e}^p</span> with <span class="math inline">p = q</span> in practice leads to a <span class="orange">reparametrization</span> of a natural exponential family <span class="math inline">\mathcal{F}_\text{en}^p</span> in a <span class="blue">transformed space</span> <span class="math inline">s(Y)</span>.</p>
</div>
</div>
</div>
</section>
<section id="example-gamma-distribution" class="slide level2 center">
<h2>Example: gamma distribution 📖</h2>
<ul>
<li><p>The family <span class="math inline">\text{Gamma}(\nu, \lambda)</span> with <span class="math inline">\nu,\lambda &gt; 0</span> is an <span class="math inline">\mathcal{F}_\text{e}^2</span>. In fact, its <span class="blue">density</span> is <span class="math display">
\begin{aligned}
f(y; \nu, \lambda) &amp;= \frac{\lambda^\nu}{\Gamma(\nu)}y^{\nu -1}e^{-\lambda y} = \frac{1}{y}\exp\{\nu\log{y} - \lambda y - \log\Gamma(\nu) + \nu\log{\lambda}\} \\
&amp;= h(y)\exp\{\theta(\psi)^T s(y) - G(\psi)\}.
\end{aligned}
</span> where <span class="math inline">h(y) = y^{-1}</span>, the <span class="blue">sufficient statistic</span> <span class="math inline">s(y) = (s_1(y), s_2(y)) = (\log{y}, y)</span>, whereas the <span class="orange">natural parameters</span> and the cumulant generating function are <span class="math display">
\theta(\psi) = (\theta_1(\psi), \theta_2(\psi)) = (\nu, -\lambda), \qquad G(\psi) = \log{\Gamma(\nu)} - \nu\log{\lambda},
</span> having set <span class="math inline">\psi = (\nu, \lambda)</span>.</p></li>
<li><p>As previously shown, this implies that the family <span class="math display">
f(s; \theta) = \tilde{h}(s)\exp\{\theta^Ts - \log{\Gamma(\theta_1)} + \theta_1\log(-\theta_2)\}, \qquad \theta \in \tilde{\Theta},
</span> is a regular <span class="orange">natural exponential family</span> of order 2, with some function <span class="math inline">\tilde{h}(s)</span>.</p></li>
</ul>
</section>
<section id="example-von-mises-distribution-i" class="slide level2 center">
<h2>Example: von Mises distribution I</h2>
<ul>
<li><p>Let <span class="math inline">Y</span> be a random variable describing an <span class="blue">angle</span>, so that <span class="math inline">\mathcal{Y} = (0, 2\pi)</span>, and let us consider the <span class="orange">uniform density</span> on the <span class="blue">circle</span>, namely <span class="math display">
f_0(y) = \frac{1}{2\pi}, \qquad y \in (0, 2\pi).
</span></p></li>
<li><p>We define a tilted density <span class="math inline">f(y; \theta) \in \mathcal{F}_\text{e}^2</span> by considering <span class="math inline">s(y) = (\cos{y}, \sin{y})</span>, i.e., the <span class="orange">cartesian coordinates</span> of <span class="math inline">y</span>. This choice of <span class="math inline">s(y)</span> ensures the appealing property <span class="math inline">f(y;\theta) = f(y + 2k\pi;\theta)</span>.</p></li>
<li><p>More precisely, let <span class="math inline">\theta = (\theta_1,\theta_2)</span> and define the parametric family of densities <span class="math display">
f(y; \theta) = f_0(y)\exp\{\theta^Ts(y) - K(\theta)\}, \qquad \theta \in \tilde{\Theta},
</span> where <span class="math inline">h(y) = 1/2\pi</span>. The normalizing constant has a “closed form” <span class="math display">
\exp{K(\theta)} = \frac{1}{2\pi}\int_0^{2\pi}\exp\{\theta_1\cos(y) + \theta_2\sin(y)\}\mathrm{d}y = \mathcal{A}_0(||\theta||_2),
</span> where <span class="math inline">\mathcal{A}_\nu(\cdot)</span> is known as the <span class="orange">modified Bessel function</span> of the first kind and order <span class="math inline">\nu</span>.</p></li>
<li><p>It is easy to check that <span class="math inline">K(\theta) &lt; \infty</span> for all values of <span class="math inline">\theta \in \mathbb{R}^2</span>; therefore, <span class="math inline">\tilde{\Theta} = \mathbb{R}^2</span>. This completes the definition of what is known as the <span class="blue">von Mises</span> distribution.</p></li>
</ul>
</section>
<section id="example-von-mises-distribution-ii" class="slide level2 center">
<h2>Example: von Mises distribution II</h2>
<ul>
<li><p>Instead of the <span class="orange">natural parametrization</span>, it is often convenient to consider a <span class="blue">reparametrization</span> <span class="math inline">\psi =(\tau, \gamma)</span>, defined through the one-to-one mapping <span class="math display">
\theta(\psi) = (\tau\cos{\gamma}, \tau\sin{\gamma}), \qquad \psi \in \tilde{\Psi} = (0, \infty) \times (0, 2\pi).
</span></p></li>
<li><p>Using this parametrization, thanks to well-known <span class="blue">trigonometric</span> relationships, we obtain the more familiar formulation of the von Mises distribution, which is <span class="math display">
f(y; \psi) = h(y)\exp\{\theta(\psi)s(y) - G(\psi)\} = \frac{1}{2\pi \mathcal{A}_0(\tau)}e^{\tau\cos(y - \gamma)}, \qquad y \in (0, 2\pi),
</span> so that <span class="math inline">\gamma \in (0,2\pi)</span> can be interpreted as the <span class="orange">location</span> and <span class="math inline">\tau &gt; 0</span> as the <span class="blue">precision</span>.</p></li>
<li><p>We also note that the distribution of <span class="math inline">s(Y)</span> is a <span class="blue">regular</span> <span class="orange">natural exponential family</span> of order 2, with density <span class="math display">
f_S(s; \theta) = \frac{1}{2\pi}\exp\{\theta^Ts - \log\mathcal{A}_0(||\theta||_2)\}, \qquad s \in \mathcal{S} = \{(s_1,s_2) \in \mathbb{R}^2 : s_1^2 + s_2^2 = 1\},
</span> clarifying that <span class="math inline">S = s(Y)</span> is a random vector taking values on a <span class="orange">circle</span> with unit radius.</p></li>
</ul>
</section>
<section id="example-wind-direction-in-venice-i" class="slide level2 center">
<h2>Example: wind direction in Venice I</h2>
<ul>
<li><p>The von Mises distribution is sometimes regarded as the “Gaussian distribution for <span class="blue">circular data</span>”. To provide a concrete example, let us consider the <span class="orange">wind directions</span> measured from the <a href="https://www.comune.venezia.it/it/content/17-san-giorgio">San Giorgio meteorological station</a>, in Venice.</p></li>
<li><p>Measurements are recorded every <span class="orange">5 minutes</span>, from 14-04-2025 to 18-04-2025, for a total of <span class="math inline">n = 1153</span>. The variable <code>wind_dir</code> is recorded in <span class="blue">degrees</span>, i.e., between 0 and 360.</p></li>
</ul>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 10 × 3
   date                wind_dir `Wind speed`
   &lt;dttm&gt;                 &lt;dbl&gt;        &lt;dbl&gt;
 1 2025-04-14 00:00:00      148          4.6
 2 2025-04-14 00:05:00      148          4.4
 3 2025-04-14 00:10:00      152          4.1
 4 2025-04-14 00:15:00      150          4.1
 5 2025-04-14 00:20:00      150          4  
 6 2025-04-14 00:25:00      148          3.8
 7 2025-04-14 00:30:00      151          3.3
 8 2025-04-14 00:35:00      145          3  
 9 2025-04-14 00:40:00      148          3.5
10 2025-04-14 00:45:00      150          2.9</code></pre>
</div>
</div>

<aside><div>
<p>The dataset is available <a href="../data/Stazione_SanGiorgio.csv">here</a>. The original source is the <a href="https://www.comune.venezia.it/it/content/dati-e-statistiche-">webpage of Venice municipality</a>.</p>
</div></aside></section>
<section id="example-wind-direction-in-venice-ii" class="slide level2 center">
<h2>Example: wind direction in Venice II</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-4-1.png" class="quarto-figure quarto-figure-center r-stretch" width="1200"><ul>
<li>This is a somewhat <span class="orange">misleading</span> graphical representation of <span class="blue">wind directions</span> evolving over time. Indeed, the “spikes” are not real: the angles 1 and 359 are, in fact, very close.</li>
</ul>
</section>
<section id="example-wind-direction-in-venice-iii" class="slide level2 center">
<h2>Example: wind direction in Venice III</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-5-1.png" width="2000" class="r-stretch"><ul>
<li>A better graphical representation of <span class="blue">wind directions</span> and <span class="orange">wind speed</span>, using Cartesian coordinates. From this wind rose, it is clear the winds were coming mostly from the east.</li>
</ul>
</section></section>
<section>
<section id="inference" class="title-slide slide level1 center">
<h1>Inference</h1>

</section>
<section id="independent-sampling-sufficiency-and-completeness" class="slide level2 center">
<h2>Independent sampling, sufficiency and completeness</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be iid random vectors with density <span class="math inline">f(y; \theta)</span>, where <span class="math inline">f(y; \theta) \in \mathcal{F}_\text{e}^p</span> and, without loss of generality, we let <span class="math inline">f(y; \theta) = f_0(y)\exp\{\theta^T s(y) - K(\theta)\}</span>. The <span class="blue">likelihood</span> function is <span class="math display">
L(\theta; \bm{y}) = \prod_{i=1}^n \exp\left\{\theta^T s(y_i) - K(\theta)\right\} = \exp\left\{\theta^T \sum_{i=1}^n s(y_i) - n K(\theta)\right\},
</span> from which we see that <span class="math inline">s = \sum_{i=1}^n s(y_i) = \left(\sum_{i=1}^n s_1(y_i), \dots, \sum_{i=1}^n s_p(y_i)\right)</span> is the <span class="orange">minimal sufficient statistic</span> as long as <span class="math inline">n \ge p</span>, which has <span class="blue">fixed dimension</span> <span class="math inline">p</span> whatever the sample size.</p></li>
<li><p>Inference can therefore be based on the random vector <span class="math inline">S = \sum_{i=1}^n s(Y_i)</span>, whose distribution is <span class="math display">
f_S(s; \theta) = \tilde{f}_0(s)\exp\{\theta^T s - \tilde{K}(\theta)\},
</span> with <span class="math inline">\tilde{K}(\theta) = n K(\theta)</span> and for some density <span class="math inline">\tilde{f}_0(s)</span>. In other words, <span class="math inline">f_S(s; \theta) \in \mathcal{F}_\text{en}^p</span>.</p></li>
</ul>
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem (<span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#/references" role="doc-biblioref" onclick="">1997</a>)</span>, Theorem 5.7)</strong></p>
</div>
<div class="callout-content">
<p>A sufficient statistic <span class="math inline">S</span> with distribution <span class="math inline">\mathcal{F}_\text{en}^p</span> is <span class="blue">complete</span>, provided that <span class="math inline">\text{int}\:\tilde{\Theta} \neq \emptyset</span>.</p>
</div>
</div>
</div>
</section>
<section id="sufficiency-and-completeness" class="slide level2 center">
<h2>Sufficiency and completeness</h2>
<ul>
<li><p>Thus, the log-likelihood function, after a reduction via <span class="blue">sufficiency</span>, is <span class="math display">
\ell(\theta) = \ell(\theta; s) = \theta^T s - n K(\theta), \qquad \theta \in \tilde{\Theta},
</span> with <span class="math inline">S = \sum_{i=1}^n s(Y_i)</span> being distributed as a <span class="math inline">\mathcal{F}_\text{en}^p</span> with cumulant generating function <span class="math inline">n K(\theta)</span>, whereas each <span class="math inline">s(Y_i)</span> is distributed as a <span class="math inline">\mathcal{F}_\text{en}^p</span> with cumulant generating function <span class="math inline">K(\theta)</span>.</p></li>
<li><p>The <span class="orange">completeness</span> of <span class="math inline">S</span> in exponential families is a classical result that enables the usage of the Rao-Blackwell-Lehmann-Scheffé theorem for finding the UMVUE.</p></li>
<li><p>Moreover, the existence of a <span class="blue">minimal sufficient</span> statistic that performs a <span class="orange">non-trivial</span> <span class="blue">dimensionality reduction</span>, from <span class="math inline">n</span> to <span class="math inline">p</span> and with <span class="math inline">p \le n</span>, is a major simplification.</p></li>
<li><p>This only occurs in exponential families, except for non-regular cases.</p></li>
</ul>
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem (Koopman-Pitman-Darmois, <span class="citation" data-cites="Robert1994">Robert (<a href="#/references" role="doc-biblioref" onclick="">1994</a>)</span>, Theorem 3.3.3)</strong></p>
</div>
<div class="callout-content">
<p>Under iid sampling, if a parametric family whose <span class="orange">support</span> does <span class="orange">not depend</span> on the <span class="orange">parameter</span> is such that there exists a <span class="blue">sufficient statistic</span> of <span class="blue">constant dimension</span> <span class="math inline">p</span>, then the family is <span class="math inline">\mathcal{F}_\text{e}^p</span>.</p>
</div>
</div>
</div>
</section>
<section id="likelihood-quantities" class="slide level2 center">
<h2>Likelihood quantities</h2>
<ul>
<li><p>After a sufficiency reduction, we get <span class="math inline">\ell(\theta) = \theta^T s - n K(\theta)</span>. Thus, the <span class="blue">score function</span> is <span class="math display">
\ell^*(\theta) = s - n \frac{\partial}{\partial \theta}K(\theta) = s - n \mu(\theta),
</span> where <span class="math inline">\mu(\theta) = \mathbb{E}_\theta(s(Y_1))</span> is the <span class="orange">mean value mapping</span> of each <span class="math inline">s(Y_i)</span> and <span class="math inline">n \mu(\theta) = \mathbb{E}(S)</span>.</p></li>
<li><p>By direct calculation, we show that the <span class="blue">first Bartlett identity</span> holds, namely <span class="math display">
\mathbb{E}_\theta(\ell^*(\theta; S)) = \mathbb{E}_\theta(S) - n\mu(\theta) = n\mu(\theta) - n\mu(\theta)= \bm{0}.
</span> The <span class="orange">Fisher information</span> is straightforward to compute, being equal to <span class="math display">
I(\theta) = \mathbb{E}_\theta(\ell^*(\theta)\ell^*(\theta)^T) = \mathbb{E}_\theta\{(S - n\mu(\theta))(S - n\mu(\theta))^T\} = \text{var}_\theta(S) = n \, \text{var}_\theta(s(Y_1)).
</span></p></li>
<li><p>Moreover, the <span class="blue">observed information</span> is <span class="math display">
\mathcal{I}(\theta) = -\frac{\partial^2}{\partial \theta \partial \theta^T}\ell(\theta) = \frac{\partial^2}{\partial \theta \partial \theta^T}\tilde{K}(\theta)= n\frac{\partial^2}{\partial \theta \partial \theta^T}K(\theta)  = n \, \text{var}_\theta(s(Y_1)),
</span> which proves the <span class="blue">second Bartlett identity</span> as an implication of the <span class="orange">remarkable</span> identity <span class="math inline">\mathcal{I}(\theta) = I(\theta)</span>, stronger than the usual <span class="math inline">I(\theta) = \mathbb{E}_\theta(\mathcal{I}(\theta))</span>. In fact, <span class="math inline">\mathcal{I}(\theta)</span> is <span class="blue">non-stochastic</span>.</p></li>
</ul>
</section>
<section id="existence-of-the-maximum-likelihood" class="slide level2 center">
<h2>Existence of the maximum likelihood</h2>
<ul>
<li>The maximum likelihood estimate <span class="math inline">\hat{\theta}</span>, if it exists, is the <span class="orange">unique</span> solution of the <span class="blue">score equation</span> <span class="math display">
s - n \mu(\theta) = \bm{0}, \qquad \text{so that} \qquad \hat{\theta} = \mu^{-1}\left(\frac{s}{n}\right) = \mu^{-1}\left(\frac{1}{n}\sum_{i=1}^ns(y_i)\right).
</span> It is unique because <span class="math inline">\ell(\theta)</span> is <span class="orange">concave</span> in <span class="math inline">\theta</span>, namely its second derivative is <span class="math display">
\frac{\partial^2}{\partial \theta \partial \theta^T}\ell(\theta) = -\text{var}_\theta(S) &lt; 0, \qquad \theta \in \tilde{\Theta}.
</span></li>
</ul>
<div class="callout callout-warning no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Theorem (<span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#/references" role="doc-biblioref" onclick="">1997</a>)</span>, Theorem 5.8)</strong></p>
</div>
<div class="callout-content">
<p>If <span class="math inline">\mathcal{F}_\text{en}^p</span> is regular, then the maximum likelihood estimate <span class="math inline">\hat{\theta}</span> exists and is the unique solution of <span class="math inline">\ell^*(\theta) = \bm{0}</span> if and only if <span class="math inline">s \in \text{int}\:C(\mathcal{S})</span>, where <span class="math inline">C(\mathcal{S})</span> is the closed convex hull of the support of <span class="math inline">\mathcal{S}</span>.</p>
</div>
</div>
</div>
<ul>
<li>As a corollary, if <span class="math inline">\mathcal{F}_\text{en}^p</span> is regular, the MLE exists and is unique <span class="blue">with probability one</span> if and only if the boundary of <span class="math inline">C = C(\mathcal{S})</span> has probability <span class="math inline">0</span>. This is often violated when <span class="math inline">S</span> is <span class="orange">discrete</span>.</li>
</ul>
</section>
<section id="likelihood-quantities-mean-parametrization" class="slide level2 center">
<h2>Likelihood quantities: mean parametrization 📖</h2>
<ul>
<li><p>Let us consider the <span class="blue">mean parametrization</span> <span class="math inline">\mu = \mu(\theta) = \mathbb{E}_\theta(s(Y_1))</span>, whose inverse is <span class="math inline">\theta = \theta(\mu)</span>. The log-likelihood is: <span class="math display">
\ell(\mu) = \ell(\theta(\mu)) = \theta(\mu)^T s - n K(\theta(\mu)), \qquad \mu \in \mathcal{M}.
</span></p></li>
<li><p>Hence, using the <span class="orange">chain rule</span> of differentiation, we obtain the <span class="blue">score</span> <span class="math display">
\ell^*(\mu) = \left(\frac{\partial}{\partial \mu}\theta(\mu)\right)(s - n \mu) = \text{var}_{\mu}(s(Y_1))^{-1}(s - n \mu),
</span> where the last step follows from the properties of the derivatives of inverse functions.</p></li>
<li><p>Thus, the <span class="blue">observed information</span> matrix for the mean parametrization is <span class="math display">
\mathcal{I}_\mu(\mu) = -\frac{\partial^2}{\partial \mu \partial \mu^T}\ell(\mu) = -\left(\frac{\partial^2}{\partial \mu \partial \mu^T}\theta(\mu)\right)(s - n \mu) + n \, \text{var}_{\mu}(s(Y_1))^{-1},
</span> whereas the <span class="orange">Fisher information</span> matrix for <span class="math inline">\mu</span> is <span class="math display">
I_\mu(\mu) = \mathbb{E}_\mu(\mathcal{I}_\mu(\mu)) = n \, \text{var}_{\mu}(s(Y_1))^{-1} = n \, V(\mu)^{-1}.
</span></p></li>
</ul>
</section>
<section id="maximum-likelihood-mean-parametrization" class="slide level2 center">
<h2>Maximum likelihood: mean parametrization</h2>
<ul>
<li><p>Thus, the maximum likelihood estimate of the <span class="orange">mean parametrization</span> <span class="math inline">\hat{\mu} = \mu(\hat{\theta})</span> is <span class="math display">
\hat{\mu} = \frac{s}{n} = \frac{1}{n}\sum_{i=1}^ns(y_i).
</span> This means <span class="math inline">\hat{\mu}</span> is both the <span class="orange">maximum likelihood</span> and the <span class="blue">method of moments</span> estimate of <span class="math inline">\mu</span>.</p></li>
<li><p>It is also an <span class="blue">unbiased estimator</span>, because by definition <span class="math display">
\mathbb{E}(\hat{\mu}) = \frac{1}{n}\sum_{i=1}^n\mathbb{E}_\mu(s(Y_i)) = \mathbb{E}_\theta(s(Y_1)) = \mu.
</span></p></li>
<li><p>Furthermore <span class="math inline">\hat{\mu}</span> is the <span class="orange">UMVUE</span> of <span class="math inline">\mu</span>. Indeed, we could first notice that <span class="math inline">\hat{\mu}</span> is a function of <span class="math inline">S</span>, which is a <span class="orange">complete</span> sufficient statistic Alternatively, we could note that the variance of <span class="math inline">\hat{\mu}</span> is <span class="math display">
\text{var}_\mu(\hat{\mu}) = \frac{1}{n}\text{var}_\mu(s(Y_1))= \frac{1}{n}V(\mu) = \mathcal{I}_\mu(\mu)^{-1},
</span> which corresponds to the <span class="blue">Cramer-Rao</span> lower bound.</p></li>
</ul>
<!-- ::: callout-tip -->
<!-- The maximum likelihood estimate for $\hat{\theta}$ is typically [biased]{.blue}; therefore, it cannot be the [UMVUE]{.orange}, even though it depends on the complete sufficient statistic $S$. -->
<!-- ::: -->
</section>
<section id="example-binomial-distribution" class="slide level2 center">
<h2>Example: binomial distribution 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be iid Bernoulli random variables with mean <span class="math inline">\mu \in (0,1)</span>, that is <span class="math inline">\text{pr}(Y_i = 1) = \mu</span>. Then, the log-likelihood function is <span class="math display">
\ell(\mu) = \sum_{i=1}^n[y_i\log{\mu} + (1 - y_i)\log{(1 - \mu)}] = s \log{\mu} + (n - s)\log{(1 - \mu)},
</span> with <span class="math inline">S = \sum_{i=1}^nY_i</span> being the <span class="orange">minimal sufficient</span> statistic and the <span class="blue">natural parametrization</span> is <span class="math inline">\theta(\mu) = \log{\mu/(1-\mu)}</span>. Note that <span class="math inline">S \sim \text{Binom}(n, \mu)</span>.</p></li>
<li><p>The <span class="blue">variance function</span> is <span class="math inline">V(\mu) = \text{var}_\mu(Y_i)= \mu(1-\mu)</span>, so that the <span class="blue">score function</span> becomes <span class="math display">
\ell^*(\mu) = \frac{s}{\mu} - \frac{n - s}{1 - \mu} = \frac{1}{V(\mu)}(s - n\mu),
</span> leading to the well-known UMVUE maximum likelihood estimator <span class="math inline">\hat{\mu} = s/n</span>.</p></li>
<li><p>Finally, the <span class="orange">observed information</span> and the <span class="blue">Fisher information</span> equal, respectively <span class="math display">
\mathcal{I}_\mu(\mu) = \frac{s}{\mu^2} - \frac{n - s}{(1 - \mu)^2}, \qquad I_\mu(\mu) = \mathbb{E}_\mu(\mathcal{I}_\mu(\mu)) = \frac{n}{\mu(1 - \mu)} = \frac{n}{V(\mu)}.
</span></p></li>
</ul>
</section>
<section id="example-von-mises-distribution-iii" class="slide level2 center">
<h2>Example: von Mises distribution III 📖</h2>
<ul>
<li><p>Let <span class="math inline">Y_1,\dots,Y_n</span> be iid random variables from a <span class="blue">Von-Mises</span> distribution with density <span class="math inline">f(y; \psi) = (2\pi\mathcal{A}_0(\tau))^{-1}\exp\{\tau\cos(y - \gamma)\}</span>, with <span class="math inline">y \in (0, 2\pi)</span>, therefore the <span class="orange">log-likelihood</span> is <span class="math display">
\ell(\psi) = \tau\sum_{i=1}^n\cos(y_i - \gamma) - n\log{\mathcal{A}_0(\tau)}.
</span></p></li>
<li><p>The Jacobian of the log-likelihood is <span class="math display">
\frac{\partial}{\partial \gamma} \ell(\psi) = \tau\sum_{i=1}^n\sin(y_i - \gamma), \quad\frac{\partial}{\partial \tau}\ell(\psi) = \sum_{i=1}^n\cos{(y_i - \gamma)} - n\frac{\mathcal{A_1(\tau)}}{\mathcal{A}_0(\tau)}.
</span></p></li>
<li><p>Thus, the <span class="blue">maximum likelihood estimate</span> <span class="math inline">(\hat{\gamma},\hat{\tau})</span> is the solution of the following equations <span class="math display">
\tan(\hat{\gamma}) = \frac{\sum_{i=1}^n\sin{y_i}}{\sum_{i=1}^n\cos{y_i}}, \qquad \frac{1}{n}\sum_{i=1}^n\cos{(y_i - \hat{\gamma})} = \frac{\mathcal{A_1(\hat{\tau})}}{\mathcal{A}_0(\hat{\tau})}.
</span> The estimate for <span class="math inline">\tau</span> can be obtained <span class="orange">numerically</span> e.g.&nbsp;using the <code>circular::A1inv</code> function.</p></li>
</ul>
</section>
<section id="example-wind-direction-in-venice-iv" class="slide level2 center">
<h2>Example: wind direction in Venice IV</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-7-1.png" width="2000" class="r-stretch"><ul>
<li>The <span class="orange">estimated</span> values are <span class="math inline">\hat{\gamma} = 1.375</span> (corresponding to about 79 degrees) and <span class="math inline">\hat{\tau} = 2.51</span>.</li>
</ul>
</section>
<section id="example-wind-direction-in-venice-v" class="slide level2 center">
<h2>Example: wind direction in Venice V</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-8-1.png" width="2000" class="r-stretch"></section>
<section id="asymptotic-theory-remarks" class="slide level2 center">
<h2>Asymptotic theory: remarks</h2>
<ul>
<li><p>Let us consider an iid sample from a model such that the minimal sufficient statistic belongs to a <span class="orange">regular exponential family</span> <span class="math inline">\mathcal{F}_\text{en}^p</span>, with natural parameter <span class="math inline">\theta \in \tilde{\Theta}</span>.</p></li>
<li><p>It is straightforward to verify that the <span class="blue">regularity conditions</span> <span class="blue">A1–A6</span> from <a href="un_A.html">Unit A</a> are <span class="blue">all satisfied</span>. Thus, Theorem 5.1 of <span class="citation" data-cites="Lehmann1998">Lehmann and Casella (<a href="#/references" role="doc-biblioref" onclick="">1998</a>)</span> applies directly.</p></li>
<li><p>We also proved that, if the score function has a root, then the maximum likelihood estimate <span class="math inline">\hat{\theta}</span> exists and is the <span class="blue">unique solution</span> of <span class="math inline">\ell^*(\theta) = \mathbf{0}</span>, where <span class="math inline">\ell^*(\theta) = s - n \mu(\theta)</span>.</p></li>
<li><p>The maximum likelihood estimate may fail to exist if <span class="math inline">s</span> lies on the boundary of <span class="math inline">C(\mathcal{S})</span>. However, as <span class="math inline">n \to \infty</span>, the probability that <span class="math inline">s</span> lies on the boundary of <span class="math inline">C(\mathcal{S})</span> tends to zero.</p></li>
<li><p>Indeed, by the law of large numbers, <span class="math inline">S/n</span> converges almost surely to <span class="math inline">\mu(\theta) \in \mathcal{M} = \text{int}\:\mathcal{C}(S)</span>, implying that a unique root of the score function eventually exists with probability one.</p></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>If the observations are iid from a <span class="orange">regular</span> exponential family, the maximum likelihood estimator <span class="math inline">\hat{\theta}</span> is consistent and asymptotically normal for <span class="math inline">\theta</span>. By the <span class="blue">continuous mapping theorem</span>, this implies that <span class="math inline">\hat{\mu} = \mu(\hat{\theta})</span>, or any other <span class="orange">smooth reparametrization</span>, is a consistent estimator of <span class="math inline">\mu</span>.</p>
</div>
</div>
</div>
</section>
<section id="wald-inequality-a-direct-proof" class="slide level2 center">
<h2>Wald inequality: a direct proof 📖</h2>
<ul>
<li><p>Let us recall that <span class="blue">Wald inequality</span> states that <span class="math display">
\mathbb{E}_{\theta_0}\left(\ell(\theta; \bm{Y})\right) &lt; \mathbb{E}_{\theta_0}\left(\ell(\theta_0; \bm{Y})\right), \qquad \theta \neq \theta_0,
</span> and the proof relies on the Kullback-Leibler divergence.</p></li>
<li><p>Let us focus on the <span class="orange">univariate</span> case <span class="math inline">\Theta \subseteq \mathbb{R}</span>. It is instructive to provide a <span class="orange">direct proof</span> for exponential families, recalling that <span class="math inline">\ell(\theta_0\bm{Y}) = \theta S - n K(\theta)</span>.</p></li>
<li><p>In the first place, note that <span class="math display">
\mathbb{E}_{\theta_0}(\ell(\theta; \bm{Y})) = n \left[\theta \mu(\theta_0) - K(\theta)\right],
</span> implying that Wald inequality holds true if and only if <span class="math display">
\mu(\theta_0)\left(\theta_0 - \theta\right) &gt; K(\theta_0) - K(\theta), \qquad \theta \neq \theta_0.
</span></p></li>
<li><p>This is indeed the case, the above being a <a href="https://en.wikipedia.org/wiki/Convex_function">characterization</a> of <span class="orange">convexity</span> for <span class="math inline">K(\cdot)</span>, which we previously show having <span class="math inline">\partial^2 /\partial \theta^2 K(\theta) &gt; 0</span> for all <span class="math inline">\theta \in \tilde{\Theta}</span>. Moreover, recall that <span class="math inline">\mu(\theta) = \partial /\partial \theta K(\theta)</span>.</p></li>
</ul>
<!-- ## Consistency and normality: a direct proof -->
<!-- ## Firth corrections -->
</section></section>
<section>
<section id="references-and-study-material" class="title-slide slide level1 center">
<h1>References and study material</h1>

</section>
<section id="main-references" class="slide level2 center">
<h2>Main references</h2>
<ul>
<li><span class="citation" data-cites="Pace1997">Pace and Salvan (<a href="#/references" role="doc-biblioref" onclick="">1997</a>)</span>
<ul>
<li><span class="orange">Chapter 5</span> (<em>Exponential families</em>)</li>
<li><span class="orange">Chapter 6</span> (<em>Exponential dispersion families</em>)</li>
</ul></li>
<li><span class="citation" data-cites="Davison2003">Davison (<a href="#/references" role="doc-biblioref" onclick="">2003</a>)</span>
<ul>
<li><span class="blue">Chapter 5</span> (<em>Models</em>)</li>
</ul></li>
<li><span class="citation" data-cites="Efron2016">Efron and Hastie (<a href="#/references" role="doc-biblioref" onclick="">2016</a>)</span>
<ul>
<li><span class="grey">Chapter 5</span> (<em>Parametric models and exponential families</em>)</li>
</ul></li>
<li><span class="citation" data-cites="Efron2023">Efron (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span>
<ul>
<li><span class="orange">Chapter 1</span> (<em>One-parameter exponential families</em>)</li>
<li><span class="orange">Chapter 2</span> (<em>Multiparameter exponential families</em>)</li>
</ul></li>
</ul>
</section>
<section id="morris1982" class="slide level2 center">
<h2><span class="citation" data-cites="Morris1982">Morris (<a href="#/references" role="doc-biblioref" onclick="">1982</a>)</span></h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="img/Morris.png"></p>
</div><div class="column" style="width:60%;">
<ul>
<li><p><span class="citation" data-cites="Morris1982">Morris (<a href="#/references" role="doc-biblioref" onclick="">1982</a>, AoS)</span> is a <span class="blue">seminal</span> paper in the field of <span class="orange">exponential families</span>.</p></li>
<li><p>It is a must-read, as it encompasses and overviews many of the results discussed in this unit.</p></li>
<li><p>It also shows that exponential families with quadratic variance are <span class="blue">infinitely divisible</span>, provided that <span class="math inline">c \ge 0</span>.</p></li>
<li><p>The paper covers several <span class="orange">advanced topics</span>, including:</p>
<ul>
<li>orthogonal polynomials;</li>
<li>limiting results;</li>
<li>large deviations;</li>
<li>…and more.</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="jorgensen1987" class="slide level2 center">
<h2><span class="citation" data-cites="Jorgensen1987">Jorgensen (<a href="#/references" role="doc-biblioref" onclick="">1987</a>)</span></h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="img/Jorgensen.png"></p>
</div><div class="column" style="width:60%;">
<ul>
<li><p><span class="citation" data-cites="Jorgensen1987">Jorgensen (<a href="#/references" role="doc-biblioref" onclick="">1987</a>, JRSSB)</span> is another <span class="blue">seminal</span> paper in the field of <span class="orange">exponential dispersion families</span>.</p></li>
<li><p>It studies a multivariate extension of exponential dispersion models of <span class="citation" data-cites="Nelder1972">Nelder and Wedderburn (<a href="#/references" role="doc-biblioref" onclick="">1972</a>)</span>.</p></li>
<li><p>It characterizes the entire class in terms of variance function, extending <span class="citation" data-cites="Morris1982">Morris (<a href="#/references" role="doc-biblioref" onclick="">1982</a>)</span>.</p></li>
<li><p>It also describes a notion of asymptotic normality called <span class="blue">small sample asymptotics</span>.</p></li>
<li><p>It is a <span class="orange">read</span> paper and among the discussants we find, J.A. Nelder, A.C. Davison, C.N. Morris.</p></li>
</ul>
</div></div>
</section>
<section id="diaconis1979" class="slide level2 center">
<h2><span class="citation" data-cites="Diaconis1979">Diaconis and Ylvisaker (<a href="#/references" role="doc-biblioref" onclick="">1979</a>)</span></h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="img/Diaconis.png"></p>
</div><div class="column" style="width:60%;">
<ul>
<li><p>Bayesian statistics also greatly benefits from the use of exponential families.</p></li>
<li><p><span class="citation" data-cites="Diaconis1979">Diaconis and Ylvisaker (<a href="#/references" role="doc-biblioref" onclick="">1979</a>, AoS)</span> is a <span class="blue">seminal paper</span> on the topic of <span class="orange">conjugate priors</span>.</p></li>
<li><p>Broadly speaking, conjugate priors always exist for exponential families.</p></li>
<li><p>These are known as the Diaconis–Ylvisaker conjugate priors.</p></li>
<li><p>Classical priors such as beta–Bernoulli and Poisson–gamma are special cases.</p></li>
<li><p>The posterior expectation under the mean parametrization is a <span class="blue">linear combination</span> of the data and the prior mean.</p></li>
</ul>
</div></div>
</section>
<section id="consonni1992" class="slide level2 center">
<h2><span class="citation" data-cites="Consonni1992">Consonni and Veronese (<a href="#/references" role="doc-biblioref" onclick="">1992</a>)</span></h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="img/Consonni.png"></p>
</div><div class="column" style="width:60%;">
<ul>
<li><p><span class="citation" data-cites="Consonni1992">Consonni and Veronese (<a href="#/references" role="doc-biblioref" onclick="">1992</a>, JASA)</span> is another <span class="blue">Bayesian</span> contribution which refines the results of <span class="citation" data-cites="Diaconis1979">Diaconis and Ylvisaker (<a href="#/references" role="doc-biblioref" onclick="">1979</a>)</span>.</p></li>
<li><p>It investigates when a <span class="orange">conjugate prior</span> specified on the mean parameter <span class="math inline">\mu</span> of a natural exponential family leads to a linear posterior expectation of <span class="math inline">\mu</span>.</p></li>
<li><p>The main result shows that this <span class="orange">posterior linearity</span> holds if and only if the <span class="orange">variance function</span> is <span class="blue">quadratic</span>.</p></li>
<li><p>The paper also explores the <span class="blue">monotonicity</span> of the <span class="orange">posterior variance</span> of <span class="math inline">\mu</span> with respect to both the sample size and the prior sample size.</p></li>
</ul>
</div></div>
</section>
<section id="references" class="slide level2 unnumbered smaller scrollable">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Billingsley1995" class="csl-entry" role="listitem">
Billingsley, Patrick. 1995. <em><span>Probability And Measure</span></em>. Wiley.
</div>
<div id="ref-Consonni1992" class="csl-entry" role="listitem">
Consonni, Guido, and Piero Veronese. 1992. <span>“Conjugate Priors for Exponential Families Having Quadratic Functions.”</span> <em>Journal of the American Statistical Association</em> 87 (420): 1123–27.
</div>
<div id="ref-Davison2003" class="csl-entry" role="listitem">
Davison, A. C. 2003. <em><span>Statistical Models</span></em>. Cambridge University Press.
</div>
<div id="ref-Diaconis1979" class="csl-entry" role="listitem">
Diaconis, Persi, and Donald Ylvisaker. 1979. <span>“<span class="nocase">Conjugate prior for exponential families</span>.”</span> <em>The Annals of Statistics</em> 7 (2): 269–92.
</div>
<div id="ref-Efron2023" class="csl-entry" role="listitem">
Efron, Bradley. 2023. <em><span class="nocase">Exponential Families in Theory and Practice</span></em>. Cambridge University Press.
</div>
<div id="ref-Efron2016" class="csl-entry" role="listitem">
Efron, Bradley, and Trevor Hastie. 2016. <em><span>Computer Age Statistical Inference</span></em>. Cambridge University Press.
</div>
<div id="ref-Fisher1934" class="csl-entry" role="listitem">
Fisher, R. A. 1934. <span>“<span class="nocase">Two new properties of mathematical likelihood</span>.”</span> <em>Proceedings of the Royal Society of London. Series A</em> 144 (852): 285–307.
</div>
<div id="ref-Jorgensen1987" class="csl-entry" role="listitem">
Jorgensen, Bert. 1987. <span>“<span class="nocase">Exponential dispersion model</span>.”</span> <em>Journal of the Royal Statistical Society. Series B: Methodological</em> 49 (2): 127–62.
</div>
<div id="ref-Lehmann1998" class="csl-entry" role="listitem">
Lehmann, E. L., and G. Casella. 1998. <em><span class="nocase">Theory of Point Estimation, Second Edition</span></em>. Springer.
</div>
<div id="ref-Morris1982" class="csl-entry" role="listitem">
Morris, Carl N. 1982. <span>“Natural Exponential Families with Quadratic Variance Functions.”</span> <em>Annals of Statistics</em> 10 (1): 65–80.
</div>
<div id="ref-Nelder1972" class="csl-entry" role="listitem">
Nelder, J. A., and R. W. M. Wedderburn. 1972. <span>“<span class="nocase">Generalized linear models</span>.”</span> <em>Journal of the Royal Statistical Society. Series A: Statistics in Society</em> 135 (3): 370–84.
</div>
<div id="ref-Pace1997" class="csl-entry" role="listitem">
Pace, Luigi, and Alessandra Salvan. 1997. <em><span class="nocase">Principles of statistical inference from a Neo-Fisherian perspective</span></em>. Vol. 4. Advanced Series on Statistical Science and Applied Probability. World Scientific.
</div>
<div id="ref-Robert1994" class="csl-entry" role="listitem">
Robert, Christian P. 1994. <em><span class="nocase">The Bayesian Choice: from decision-theoretic foundations to computational implementation</span></em>. Springer.
</div>
</div>
<div class="quarto-auto-generated-content">
<p><img src="img/logoB.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://tommasorigon.github.io/InferentialStat">Home page</a></p>
</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="un_B_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="un_B_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="un_B_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>