<!DOCTYPE html>
<html lang="en"><head>
<script src="un_B_files/libs/clipboard/clipboard.min.js"></script>
<script src="un_B_files/libs/quarto-html/tabby.min.js"></script>
<script src="un_B_files/libs/quarto-html/popper.min.js"></script>
<script src="un_B_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="un_B_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="un_B_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="un_B_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <meta name="author" content="Tommaso Rigon">
  <title>Generalized Linear Models</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="un_B_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="un_B_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="un_B_files/libs/revealjs/dist/theme/quarto-59f62b11fa3217433aa3d23adb7b12fb.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">
  <link href="un_B_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="un_B_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="un_B_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="un_B_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Generalized Linear Models</h1>
  <p class="subtitle">Statistics III - CdL SSE</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<span class="orange">Tommaso Rigon</span> 
</div>
        <p class="quarto-title-affiliation">
            <em>Universit√† degli Studi di Milano-Bicocca</em>
          </p>
    </div>
</div>

</section>
<section id="homepage" class="slide level2 center">
<h2><a href="../index.html">Homepage</a></h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><img data-src="img/gaussian.png"></p>
</div><div class="column" style="width:70%;">
<ul>
<li><p>This unit will cover the following <span class="orange">topics</span>:</p>
<ul>
<li>Exponential dispersion families</li>
<li>Likelihood, inference, and testing</li>
<li>Iteratively Re-weighted Least Squares (IRLS)</li>
<li>Deviance, model checking, and residuals</li>
<li>Model selection</li>
</ul></li>
<li><p>GLMs are regression models with a linear predictor, where the response variable follows an <span class="blue">exponential dispersion family</span>.</p></li>
<li><p>The symbol üìñ means that a few extra steps are discussed in the <span class="blue">handwritten notes</span>.</p></li>
</ul>
</div></div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The content of this Unit is covered in <span class="orange">Chapter 2</span> of <span class="citation" data-cites="Salvan2020">Salvan et al. (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span>. Alternatively, see <span class="blue">Chapter 4</span> of <span class="citation" data-cites="Agresti2015">Agresti (<a href="#/references" role="doc-biblioref" onclick="">2015</a>)</span> or <span class="grey">Chapter 6</span> of <span class="citation" data-cites="Azzalini2008">Azzalini (<a href="#/references" role="doc-biblioref" onclick="">2008</a>)</span>.</p>
</div>
</div>
</div>
</section>
<section>
<section id="introduction" class="title-slide slide level1 center">
<h1>Introduction</h1>

</section>
<section id="preliminaries" class="slide level2 center">
<h2>Preliminaries</h2>
<ul>
<li><p>GLMs are a <span class="blue">class</span> of <span class="blue">regression models</span> in which a <span class="orange">response</span> random variable <span class="math inline">Y_i</span> is modeled as a function of a vector of <span class="grey">covariates</span> <span class="math inline">\bm{x}_i \in \mathbb{R}^p</span>.</p></li>
<li><p>The random variables <span class="math inline">Y_i</span> are not restricted to be Gaussian. For example:</p>
<ul>
<li><span class="math inline">Y_i \in \{0,1\}</span>, known as <span class="blue">binary regression</span><br>
</li>
<li><span class="math inline">Y_i \in \{0,1,\dots\}</span>, known as <span class="orange">count regression</span><br>
</li>
<li><span class="math inline">Y_i \in (0,\infty)</span> or <span class="math inline">Y_i \in (-\infty,\infty)</span></li>
</ul></li>
<li><p>Gaussian linear models are a special case of GLMs, arising when <span class="math inline">Y_i \in (-\infty,\infty)</span>.</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>The <span class="orange">response random variables</span> are collected in the random vector <span class="math inline">\bm{Y} = (Y_1,\dots,Y_n)^T</span>, whose <span class="blue">observed realization</span> is <span class="math inline">\bm{y} = (y_1,\dots,y_n)^T</span>.</p></li>
<li><p>The <span class="blue">design matrix</span> <span class="math inline">\bm{X}</span> is an <span class="math inline">n \times p</span> <span class="orange">non-stochastic</span> matrix containing the covariate values. The <span class="math inline">j</span>th variable (column) is denoted by <span class="math inline">\tilde{\bm{x}}_j</span>, while the <span class="math inline">i</span>th observation (row) is <span class="math inline">\bm{x}_i</span>.</p></li>
<li><p>We assume that <span class="math inline">\bm{X}</span> has <span class="orange">full rank</span>, that is, <span class="math inline">\text{rk}(\bm{X}) = p</span> with <span class="math inline">p \le n</span>.</p></li>
</ul>
</div>
</section>
<section id="beetles-data-from-bliss-1935" class="slide level2 center">
<h2><code>Beetles</code> data, from Bliss (1935)</h2>
<ul>
<li>The <code>Beetles</code> dataset originates from Bliss (1935). It records the number of adult flour beetles that died after a 5-hour exposure to gaseous carbon disulphide.</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: right;">n</th>
<th style="text-align: right;">deaths</th>
<th style="text-align: right;">logdose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">59</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">1.6907</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">1.7242</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">1.7552</td>
</tr>
<tr class="even">
<td style="text-align: right;">56</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">1.7842</td>
</tr>
<tr class="odd">
<td style="text-align: right;">63</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">1.8113</td>
</tr>
<tr class="even">
<td style="text-align: right;">59</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">1.8369</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">1.8610</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">1.8839</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li><p>We aim to predict the proportion of <code>deaths</code> as a function of <code>logdose</code>.</p></li>
<li><p>Modeling death proportions directly with <span class="orange">linear models</span> is <span class="orange">inappropriate</span>. A <span class="blue">variable transformation</span> provides a more <span class="blue">principled</span> solution, but it comes with <span class="orange">drawbacks</span>.</p></li>
</ul>
</section>
<section id="beetles-data-a-dose-response-plot" class="slide level2 center">
<h2><code>Beetles</code> data, a dose-response plot</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-3-1.png" class="quarto-figure quarto-figure-center r-stretch" width="1950"><ul>
<li>There is a clear <span class="orange">positive</span> and <span class="blue">non-linear</span> pattern between the <span class="orange">proportion of deaths</span> as a function of the logdose. The response variable take values in <span class="math inline">[0, 1]</span>.</li>
</ul>
</section>
<section id="modelling-the-beetles-data" class="slide level2 center">
<h2>Modelling the <code>Beetles</code> data</h2>
<ul>
<li><p>Let <span class="math inline">Y_i</span> be the number of dead beetles out of <span class="math inline">m_i</span>, and let <span class="math inline">x_i</span> denote the log-dose. By definition, <span class="math inline">S_i \in \{0, 1, \dots, m_i\}</span> for <span class="math inline">i = 1,\dots,8</span>.</p></li>
<li><p>It is natural to model each <span class="math inline">Y_i</span> as <span class="blue">independent binomial</span> random variables, counting the number of deaths out of <span class="math inline">m_i</span> individuals. In other words: <span class="math display">
S_i \overset{\text{ind}}{\sim} \text{Binomial}(m_i, \pi_i), \qquad i = 1,\dots,8,
</span> where <span class="math inline">\pi_i</span> is the <span class="orange">probability</span> of death at a given dose <span class="math inline">x_i</span>. Moreover, we have <span class="math display">
\mathbb{E}\left(\frac{S_i}{m_i}\right)  = \pi_i = \mu_i.
</span></p></li>
<li><p>A modeling approach, called <span class="blue">logistic regression</span>, specifies:<br>
<span class="math display">
g(\pi_i) = \log\left(\frac{\pi_i}{1 - \pi_i}\right) = \beta_1 + \beta_2 x_i \quad \Longrightarrow \quad \pi_i = g^{-1}(\beta_1 + \beta_2 x_i) = \frac{\exp(\beta_1 + \beta_2 x_i)}{1 + \exp(\beta_1 + \beta_2 x_i)}.
</span> for some parameters <span class="math inline">\beta_1, \beta_2 \in \mathbb{R}</span>. <span class="orange">Note</span> that <span class="math inline">\pi_i \in (0, 1)</span> by construction.</p></li>
</ul>
</section>
<section id="beetles-data-fitted-model" class="slide level2 center">
<h2><code>Beetles</code> data, fitted model</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-4-1.png" class="quarto-figure quarto-figure-center r-stretch" width="1950"><ul>
<li>The <span class="orange">maximum likelihood</span> estimates are <span class="math inline">\hat{\beta}_1 = -60.72</span> and <span class="math inline">\hat{\beta}_2 = 34.3</span>. This yields the <span class="blue">predictive curve</span> <span class="math inline">\hat{\pi}(x) = g^{-1}(\hat{\beta}_1 + \hat{\beta}_2 x),</span> which estimates the mean proportion <span class="math inline">\mathbb{E}(S_i / m_i)</span>.</li>
</ul>
</section>
<section id="a-comparison-with-old-tools-i" class="slide level2 center">
<h2>A comparison with old tools I üìñ</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y_i = S_i / m_i</span> be the proportion of deaths. A direct application of linear models implies: <span class="math display">
Y_i = \beta_1 + \beta_2 x_i + \epsilon_i.
</span> The coefficients <span class="math inline">\beta_1</span> and <span class="math inline">\beta_2</span> are then estimated using OLS using <span class="math inline">Y_i</span> as response.</p>
</div>
</div>
</div>
<ul>
<li><p>The prediction <span class="math inline">\hat{\beta}_1 + \hat{\beta}_2 x_i</span> is <span class="orange">unrestricted</span>, meaning it could produce values like ‚Äú1.3‚Äù or ‚Äú-2‚Äù as estimated <span class="blue">proportions</span>, which is clearly undesirable.</p></li>
<li><p>The <span class="blue">additive structure</span> <span class="math inline">Y_i = \beta_1 + \beta_2 x_i + \epsilon_i</span> cannot hold with <span class="blue">iid</span> errors <span class="math inline">\epsilon_i</span>, because <span class="math inline">S_i</span>, and thus <span class="math inline">Y_i</span>, are <span class="orange">discrete</span>. As a result, the errors are always <span class="orange">heteroschedastic</span>.</p></li>
<li><p>If <span class="math inline">m_i = 1</span>, i.e.&nbsp;when the data are <span class="orange">binary</span>, all the above issues are <span class="orange">exacerbated</span>.</p></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>This approach is sometimes called the <span class="blue">linear probability model</span>. Before GLMs, it was considered acceptable despite its issues, but by modern standards it should <span class="orange">not be used</span>.</p>
</div>
</div>
</div>
</section>
<section id="a-comparison-with-old-tools-ii" class="slide level2 center">
<h2>A comparison with old tools II üìñ</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>We consider the <span class="blue">empirical logit</span> variable transformation of <span class="math inline">S_i = Y_i / m_i</span>, obtaining<br>
<span class="math display">
\text{logit}(\tilde{Y}_i) = \log\left(\frac{S_i + 0.5}{m_i - S_i + 0.5}\right) = \beta_1 + \beta_2 x_i + \epsilon_i, \qquad \tilde{Y}_i = \frac{S_i + 0.5}{m_i +1}.
</span> A correction term is necessary because otherwise <span class="math inline">g(\cdot) = \text{logit}(\cdot)</span> is undefined. The predictions belong to <span class="math inline">(0, 1)</span>, since <span class="math display">
\hat{\pi}_i = g^{-1}[\mathbb{E}\{g(\tilde{Y}_i)\}] = g^{-1}(\hat{\beta}_1 + \hat{\beta}_2 x_i) = \frac{\exp(\hat{\beta}_1 + \hat{\beta}_2 x_i)}{1 + \exp(\hat{\beta}_1 + \hat{\beta}_2 x_i)},</span> in which <span class="math inline">\hat{\beta}_1</span> and <span class="math inline">\hat{\beta}_2</span> are estimated with OLS using <span class="math inline">\text{logit}(\tilde{Z}_i)</span> as response.</p>
</div>
</div>
</div>
<ul>
<li><p>The <span class="blue">interpretation</span> of <span class="math inline">\hat{\beta}</span> is less clear, as they refer to the mean of <span class="math inline">\text{logit}(\tilde{Y}_i)</span> instead of <span class="math inline">\mathbb{E}(Y_i)</span>.</p></li>
<li><p>An arbitrary <span class="orange">boundary correction</span> is needed.</p></li>
<li><p>Inference is problematic and requires further corrections, because of <span class="orange">heteroschedastic</span> errors.</p></li>
<li><p>This approach is <span class="orange">not compatible</span> with the reasonable assumption <span class="math inline">S_i \sim \text{Binomial}(m_i, \pi_i)</span>.</p></li>
</ul>
</section>
<section id="a-comparison-with-old-tools-iii" class="slide level2 center">
<h2>A comparison with old tools III</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-5-1.png" class="quarto-figure quarto-figure-center r-stretch" width="1950"><ul>
<li>The black line is the predicted curve of a <span class="grey">logistic regression GLM</span>. The orange line is the predictived curve of a <span class="orange">linear model</span>. The blue line is the predictive curve of a <span class="blue">linear model</span> after an <span class="blue">empirical logit variable transformation</span>.</li>
</ul>
</section>
<section id="aids-data" class="slide level2 center">
<h2><code>Aids</code> data</h2>
<ul>
<li>Number of AIDS <code>deaths</code> in Australia in a sequence of three-months periods between 1983 and 1986.</li>
</ul>
<div class="smaller">
<div class="cell">
<div class="cell-output-display">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">1983-1</th>
<th style="text-align: right;">1984-1</th>
<th style="text-align: right;">1985-1</th>
<th style="text-align: right;">1986-1</th>
<th style="text-align: right;">1983-2</th>
<th style="text-align: right;">1984-2</th>
<th style="text-align: right;">1985-2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">deaths</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="even">
<td style="text-align: left;">period</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">7</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output-display">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">1986-2</th>
<th style="text-align: right;">1983-3</th>
<th style="text-align: right;">1984-3</th>
<th style="text-align: right;">1985-3</th>
<th style="text-align: right;">1986-3</th>
<th style="text-align: right;">1983-4</th>
<th style="text-align: right;">1984-4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">deaths</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">23</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">37</td>
<td style="text-align: right;">45</td>
</tr>
<tr class="even">
<td style="text-align: left;">period</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">14</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<ul>
<li><p>We are interested in predicting the number of <code>deaths</code> as a function of the <code>period</code> of time.</p></li>
<li><p>The response variable <span class="math inline">Y_i \in \{0, 1, \dots\}</span> is a non-negative <span class="orange">count</span>.</p></li>
</ul>
</section>
<section id="aids-data-scatter-plot" class="slide level2 center">
<h2><code>Aids</code> data, scatter plot</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-7-1.png" class="quarto-figure quarto-figure-center r-stretch" width="1950"><ul>
<li>There is a clear positive association between period and deaths. However, the increase appears to be <span class="orange">faster</span> than <span class="orange">linear</span>. Note that both the mean and the <span class="blue">variability</span> of <span class="math inline">Y_i</span> increase over time.</li>
</ul>
</section>
<section id="modelling-the-aids-data" class="slide level2 center">
<h2>Modelling the <code>Aids</code> data</h2>
<ul>
<li>Let <span class="math inline">Y_i</span> be the number of deaths, and let <span class="math inline">x_i</span> denote the period. By definition, <span class="math inline">Y_i \in \{0, 1, \dots\}</span> are non-negative counts, for <span class="math inline">i = 1,\dots,14</span>.</li>
</ul>
<div class="fragment">
<ul>
<li><p>We model <span class="math inline">Y_i</span> as <span class="blue">independent Poisson</span> random variables, counting the number of deaths: <span class="math display">
Y_i \overset{\text{ind}}{\sim} \text{Poisson}(\mu_i), \qquad i = 1,\dots,14,
</span> where <span class="math inline">\mu_i</span> is the <span class="orange">mean</span> of <span class="math inline">Y_i</span>, namely <span class="math inline">\mathbb{E}(Y_i) = \mu_i</span>.</p></li>
<li><p>A modeling approach, called <span class="blue">Poisson regression</span>, specifies:<br>
<span class="math display">
g(\mu_i) = \log(\mu_i) = \beta_1 + \beta_2 x_i \quad \Longrightarrow \quad \mu_i = g^{-1}(\beta_1 + \beta_2 x_i) = \exp(\beta_1 + \beta_2 x_i),
</span> for some parameters <span class="math inline">\beta_1, \beta_2 \in \mathbb{R}</span>. <span class="orange">Note</span> that <span class="math inline">\mu_i &gt; 0</span> by construction.</p></li>
<li><p>Under this specification, the <span class="blue">variances</span> of the observations are<br>
<span class="math display">
\text{var}(Y_i) = \mu_i = \exp(\beta_1 + \beta_2 x_i),
</span> which increases with <span class="math inline">x</span>, as desired. This implies that <span class="math inline">Y_1,\dots,Y_n</span> are <span class="orange">heteroschedastic</span>, but this is not an issue in GLMs, as this aspect is <span class="blue">automatically accounted</span> for.</p></li>
</ul>
</div>
</section>
<section id="aids-data-fitted-model" class="slide level2 center">
<h2><code>Aids</code> data, fitted model</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-8-1.png" class="quarto-figure quarto-figure-center r-stretch" width="1950"><ul>
<li>The <span class="orange">maximum likelihood</span> estimates are <span class="math inline">\hat{\beta}_1 = 0.304</span> and <span class="math inline">\hat{\beta}_2 = 0.259</span>. This yields the <span class="blue">predictive curve</span> <span class="math inline">\hat{\mu}(x) = \exp(\hat{\beta}_1 + \hat{\beta}_2 x),</span> which estimates the mean <span class="math inline">\mathbb{E}(Y_i)</span>.</li>
</ul>
</section>
<section id="a-comparison-with-old-tools-i-1" class="slide level2 center">
<h2>A comparison with old tools I</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>We consider the <span class="blue">variance-stabilizing</span> transformation <span class="math inline">S_i = \sqrt{Y_i}</span>, obtaining<br>
<span class="math display">
\sqrt{Y_i} = \beta_1 + \beta_2 x_i + \epsilon_i.
</span> The predictions belong to <span class="math inline">(0, \infty)</span>, since <span class="math display">
\hat{\mu}_i = \mathbb{E}(\sqrt{Y_i})^2 = (\hat{\beta}_1 + \hat{\beta}_2 x_i)^2,</span> in which <span class="math inline">\hat{\beta}_1</span> and <span class="math inline">\hat{\beta}_2</span> are estimated with OLS using <span class="math inline">\sqrt{Y_i}</span> as response.</p>
</div>
</div>
</div>
<ul>
<li><p>The <span class="blue">interpretation</span> of <span class="math inline">\hat{\beta}</span> is less clear, as they refer to the mean of <span class="math inline">\sqrt{Y}_i</span> instead of <span class="math inline">\mathbb{E}(Y_i)</span>.</p></li>
<li><p>This approach is <span class="orange">not compatible</span> with the reasonable assumption <span class="math inline">Y_i \sim \text{Poisson}(\mu_i)</span> and it only valid as an <span class="blue">asymptotic approximation</span>.</p></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>To compare such a model with a similar specification, we also fit another Poisson GLM in which <span class="math display">
Y_i \overset{\text{ind}}{\sim} \text{Poisson}(\mu_i), \qquad \sqrt{\mu_i} = \beta_1 + \beta_2 x_i, \qquad i=1,\dots,14.
</span></p>
</div>
</div>
</div>
</section>
<section id="a-comparison-with-old-tools-ii-1" class="slide level2 center">
<h2>A comparison with old tools II</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-9-1.png" class="quarto-figure quarto-figure-center r-stretch" width="1950"><ul>
<li>The black line is the predicted curve of a <span class="grey">Poisson regression GLM</span> with <span class="gray">logarithmic link</span>. The orange line is the predicted curve of a <span class="orange">linear model</span> with a <span class="orange">square-root transformation</span>. The blue line is the predictive curve of a <span class="blue">Poisson regression GLM</span> with <span class="blue">square-root link</span>.</li>
</ul>
</section>
<section id="the-components-of-a-glm" class="slide level2 center">
<h2>The components of a GLM</h2>
<ul>
<li><span class="orange">Random component</span>. This specifies the probability distribution response variable <span class="math inline">Y_i</span>. The observations <span class="math inline">\bm{y} =(y_1,\dots,y_n)</span> on that distribution are treated as <span class="orange">independent</span>.</li>
</ul>
<div class="fragment">
<ul>
<li><span class="blue">Linear predictor</span>. For a parameter vector <span class="math inline">\bm{\beta} = (\beta_1,\dots,\beta_p)^T</span> and an <span class="math inline">n \times p</span> design matrix <span class="math inline">\bm{X}</span>, the linear predictor is <span class="math inline">\bm{\eta} = \bm{X}\beta</span>. We will also write <span class="math display">
\eta_i = \bm{x}_i^T\beta = \beta_1x_{i1} + \cdots + x_{ip}\beta_p, \qquad i=1,\dots,n.
</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li><span class="grey">Link function</span>. This is an invertible and differentiable function <span class="math inline">g(\cdot)</span> applied to each component of the <span class="grey">mean</span> <span class="math inline">\mu_i = \mathbb{E}(Y_i)</span> that relates it to the linear predictor: <span class="math display">
g(\mu_i) = \eta_i = \bm{x}_i^T\beta, \qquad \Longrightarrow \qquad \mu_i = g^{-1}(\eta_i) = g^{-1}(\bm{x}_i^T\beta).
</span></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Note that, in general, we <span class="orange">cannot</span> express the response in an additive way <span class="math inline">Y_i = g^{-1}(\eta_i) + \epsilon_i</span>.</p>
</div>
</div>
</div>
</div>
</section>
<section id="random-component-of-a-glm" class="slide level2 center">
<h2>Random component of a GLM</h2>
<ul>
<li><p>In GLMs the random variables <span class="math inline">Y_i</span> are <span class="orange">independent</span> and they are distributed according to an <span class="blue">exponential dispersion family</span>, whose definition will be provided in a few slides.</p></li>
<li><p>The <span class="orange">distributions most commonly</span> used in Statistics, such as the normal, binomial, gamma, and Poisson, are exponential family distributions.</p></li>
<li><p>Exponential dispersion families are <span class="orange">characterized</span> by their <span class="blue">mean</span> and <span class="blue">variance</span>. Let <span class="math inline">v(\mu) &gt; 0</span> be a function of the mean, called <span class="blue">variance function</span> and let <span class="math inline">a_i(\phi) &gt;0</span> be functions of an additional unknown parameter <span class="math inline">\phi &gt; 0</span> called <span class="orange">dispersion</span>.</p></li>
</ul>
<div class="fragment">
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>In a GLMs the observations are independent draws from a distribution <span class="math inline">\text{ED}(\mu_i, a_i(\phi)v(\mu_i))</span>: <span class="math display">
Y_i \overset{\text{ind}}{\sim} \text{ED}(\mu_i, a_i(\phi)v(\mu_i)), \qquad \mathbb{E}(Y_i) = \mu_i, \qquad g(\mu_i) = \bm{x}_i^T\beta,
</span> with <span class="math inline">\mu_i \in \mathcal{M}</span>. Moreover, the <span class="orange">variance</span> is connected to the <span class="blue">mean</span> via <span class="math inline">v(\mu)</span>: <span class="math display">
\text{var}(Y_i) = a_i(\phi) v(\mu_i),
</span> where <span class="math inline">a_i(\phi) = \phi / \omega_i</span> and <span class="math inline">\omega_i</span> are <span class="blue">known weights</span>. Special cases are <span class="math inline">a_i(\phi) = \phi</span> and <span class="math inline">a_i(\phi) = 1</span>.</p>
</div>
</div>
</div>
</div>
</section>
<section id="notable-examples" class="slide level2 center">
<h2>Notable examples</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>In a <span class="blue">Gaussian linear model</span> we consider the <span class="blue">identity link</span> <span class="math inline">g(\mu) = \mu</span> and let <span class="math display">
Y_i \overset{\text{ind}}{\sim}\text{N}(\mu_i, \sigma^2), \qquad \mu_i = \bm{x}_i^T\beta.
</span> The unknown variance <span class="math inline">\sigma^2 = \phi</span> is called <span class="orange">dispersion</span> in GLMs. The <span class="orange">parameter space</span> is <span class="math inline">\mathcal{M} = \mathbb{R}</span>, whereas <span class="math inline">a_i(\phi) = \phi</span> and the variance function is <span class="blue">constant</span> <span class="math inline">v(\mu) = 1</span> (homoschedasticity).</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>In a <span class="blue">binomial regression model</span> with <span class="blue">logit link</span> <span class="math inline">g(\mu) = \text{logit}(\mu)</span> we let <span class="math inline">Y_i = S_i/m_i</span> and <span class="math display">
S_i \overset{\text{ind}}{\sim}\text{Binomial}(m_i, \pi_i),\qquad \mathbb{E}\left(Y_i\right) = \pi_i = \mu_i, \qquad \text{logit}(\mu_i) =  \bm{x}_i^T\beta.
</span> We have <span class="math inline">a_i(\phi) = 1/m_i</span> and <span class="math inline">v(\mu) = \mu(1-\mu)</span>. There is <span class="orange">no dispersion</span> parameter.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>In <span class="blue">Poisson regression</span> with <span class="blue">logarithmic link</span> <span class="math inline">g(\mu) = \log(\mu)</span> we let <span class="math display">
Y_i \overset{\text{ind}}{\sim}\text{Poisson}(\mu), \qquad \log(\mu_i) =  \bm{x}_i^T\beta.
</span> We have <span class="math inline">a_i(\phi) = 1</span> and <span class="math inline">v(\mu) = \mu</span>. There is <span class="orange">no dispersion</span> parameter.</p>
</div>
</div>
</div>
</section></section>
<section>
<section id="exponential-dispersion-families" class="title-slide slide level1 center">
<h1>Exponential dispersion families</h1>

</section>
<section id="overview" class="slide level2 center">
<h2>Overview</h2>
<ul>
<li>Figure 1 of <span class="citation" data-cites="Efron2023">Efron (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span>. <span class="grey">Three level</span> of statistical modeling.</li>
</ul>

<img data-src="img/EF.png" class="quarto-figure quarto-figure-center r-stretch" style="width:6in"><ul>
<li><p>The <span class="orange">prime role</span> of <span class="orange">exponential families</span> in the theory of statistical inference was first emphasized by <span class="citation" data-cites="Fisher1934">Fisher (<a href="#/references" role="doc-biblioref" onclick="">1934</a>)</span>.</p></li>
<li><p>Most <span class="blue">well-known</span> <span class="blue">distributions</span>‚Äîsuch as Gaussian, Poisson, Binomial, and Gamma‚Äîare instances of exponential families.</p></li>
</ul>
</section>
<section id="exponential-dispersion-family-definition" class="slide level2 center">
<h2>Exponential dispersion family: definition</h2>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The <span class="orange">density</span> of <span class="math inline">Y_i</span> belongs to an <span class="blue">exponential dispersion family</span> if it can be written as <span class="math display">
p(y_i; \theta_i, \phi) = \exp\left\{\frac{\theta_i y_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi)\right\},
</span> where <span class="math inline">y_i \in \mathcal{Y} \subseteq \mathbb{R}</span>, <span class="math inline">\theta_i \in \Theta \subseteq\mathbb{R}</span> and <span class="math inline">a_i(\phi) = \phi / \omega_i</span> where <span class="math inline">\omega_i</span> are <span class="blue">known positive weights</span>. The parameter <span class="math inline">\theta_i</span> is called <span class="orange">natural parameter</span> while <span class="math inline">\phi</span> is called <span class="blue">dispersion</span> parameter.</p>
</div>
</div>
</div>
<div class="fragment">
<ul>
<li><p>By specifying the functions <span class="math inline">a_i(\cdot), b(\cdot)</span> and <span class="math inline">c(\cdot)</span> one obtain a particular <span class="orange">parametric model</span>.</p></li>
<li><p>The support <span class="math inline">\mathcal{Y}</span> of <span class="math inline">Y_i</span> does not depend on the parameters <span class="math inline">\phi</span> or <span class="math inline">\theta_i</span> and <span class="math inline">b(\cdot)</span> can be <span class="blue">differentiated</span> infinitely many times. In particular, this is a <span class="orange">regular statistical model</span>.</p></li>
<li><p>As mentioned, special cases are <span class="math inline">a_i(\phi) = \phi</span> and <span class="math inline">a_i(\phi) = 1</span>. When <span class="math inline">a_i(\phi) = 1</span> and <span class="math inline">c(y_i, \phi) = c(y_i)</span> we obtain <span class="math display">
p(y_i; \theta_i) = \exp\left\{\theta_i y_i - b(\theta_i) + c(y_i)\right\},
</span> which is called <span class="orange">natural exponential family</span> of order 1.</p></li>
</ul>
</div>
</section>
<section id="mean-and-variance-i" class="slide level2 center">
<h2>Mean and variance I üìñ</h2>
<ul>
<li><p>Let us consider the <span class="orange">log-likelihood</span> contribution of the <span class="math inline">i</span>th observations, which is defined as <span class="math display">
\ell(\theta_i, \phi; y_i) = \log{p(y_i; \theta_i, \phi)} = \frac{\theta_i y_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi).
</span> If you prefer, this is the log-likelihood when the sample size <span class="math inline">n = 1</span> and we only observe <span class="math inline">Y_i</span>.</p></li>
<li><p>The <span class="blue">score</span> and <span class="orange">hessian</span> functions, namely the first and second derivative over <span class="math inline">\theta_i</span> are <span class="math display">
\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; y_i) = \frac{y_i - b'(\theta_i)}{a_i(\phi)}, \qquad \frac{\partial^2}{\partial \theta_i^2}\ell(\theta_i, \phi; y_i) = \frac{-b''(\theta_i)}{a_i(\phi)}.
</span> where <span class="math inline">b'(\cdot)</span> and <span class="math inline">b''(\cdot)</span> denote the <span class="blue">first</span> and <span class="orange">second</span> derivative of <span class="math inline">b(\cdot)</span>.</p></li>
<li><p>Recall the following <span class="grey">Bartlett identities</span>, valid in any <span class="orange">regular</span> statistical model: <span class="math display">
\begin{aligned}
\mathbb{E}\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right) &amp;= 0, \\
\mathbb{E}\left\{\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right)^2\right\} = \text{var}\left(\frac{\partial}{\partial \theta_i} \ell(\theta_i, \phi; Y_i) \right) &amp;= \mathbb{E}\left(-\frac{\partial^2}{\partial \theta_i^2}\ell(\theta_i, \phi; Y_i)\right).
\end{aligned}
</span></p></li>
</ul>
</section>
<section id="mean-and-variance-ii" class="slide level2 center">
<h2>Mean and variance II üìñ</h2>
<ul>
<li>Specializing Bartlett identities in <span class="blue">exponential dispersion families</span>, we obtain <span class="math display">
\mathbb{E}\left(\frac{Y_i - b'(\theta_i)}{a_i(\phi)}\right) = 0, \qquad \text{var}\left(\frac{Y_i - b'(\theta_i)}{a_i(\phi)}\right) = \frac{\text{var}(Y_i)}{a_i(\phi)^2} = \frac{b''(\theta_i)}{a_i(\phi)}.
</span> Re-arranging the terms, we finally get the following key result.</li>
</ul>
<div class="callout callout-warning no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y_i</span> be an exponential dispersion family, identified by the functions <span class="math inline">a_i(\cdot), b(\cdot)</span> and <span class="math inline">c(\cdot)</span>, and with natural parameter <span class="math inline">\theta_i</span>. Then the <span class="orange">mean</span> and the <span class="blue">variance</span> of <span class="math inline">Y_i</span> equal <span class="math display">
\mathbb{E}(Y_i) =  b'(\theta_i), \qquad \text{var}(Y_i) = a_i(\phi) b''(\theta_i).
</span></p>
</div>
</div>
</div>
<ul>
<li><p>The mean <span class="math inline">\mu_i = b'(\theta_i)</span> does <span class="orange">not</span> depend on the <span class="orange">dispersion</span> parameter.</p></li>
<li><p>We have <span class="math inline">b''(\cdot) &gt; 0</span> because <span class="math inline">\text{var}(Y_i)</span>, which means that <span class="math inline">b(\cdot)</span> is a <span class="blue">convex function</span>.</p></li>
<li><p>Moreover, the function <span class="math inline">b'(\theta)</span> is <span class="orange">continuous</span> and <span class="orange">monotone increasing</span> and hence <span class="orange">invertible</span>.</p></li>
</ul>

<aside><div>
<p>The function <span class="math inline">b(\cdot)</span> is related to the moment generating function of <span class="math inline">Y_i</span>. Thus, higher order derivatives of <span class="math inline">b(\cdot)</span> allows the calculations of skewness, kurtosis, etc.</p>
</div></aside></section>
<section id="mean-parametrization-variance-function" class="slide level2 center">
<h2>Mean parametrization, variance function</h2>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y_i</span> be an exponential dispersion family, identified by the functions <span class="math inline">a_i(\cdot), b(\cdot)</span> and <span class="math inline">c(\cdot)</span>, and with <span class="orange">natural parameter</span> <span class="math inline">\theta_i</span>, then <span class="math display">
\mu(\theta_i):= \mu_i = \mathbb{E}(Y_i) = b'(\theta_i).
</span> The function <span class="math inline">\mu(\cdot) : \Theta \to\mathcal{M}</span> is <span class="blue">one-to-one</span> and invertible, that is, a <span class="orange">reparametrization</span> of <span class="math inline">\theta_i</span>. We call <span class="math inline">\mu_i</span> the <span class="blue">mean parametrization</span> of an exponential dispersion family.</p>
</div>
</div>
</div>
<div class="fragment">
<ul>
<li><p>The <span class="orange">inverse</span> relationship, re-obtaining <span class="math inline">\theta_i</span> as a function of <span class="math inline">\mu_i</span>, is denoted with <span class="math display">
\theta_i = \theta(\mu_i) = b'^{-1}(\mu_i).
</span></p></li>
<li><p>Using this notation, we can express the variance of <span class="math inline">Y_i</span> as a function of <span class="math inline">\mu_i</span> as follows <span class="math display">
\text{var}(Y_i) = a_i(\phi)b''(\theta_i) =  a_i(\phi)b''(\theta(\mu_i)) = a_i(\phi)v(\mu_i),
</span> where <span class="math inline">v(\mu_i) := b''(\theta(\mu_i))</span> is the <span class="blue">variance function</span>.</p></li>
<li><p>The domain <span class="math inline">\mathcal{M}</span> and the variance function <span class="math inline">v(\mu)</span> <span class="orange">characterize</span> the function <span class="math inline">b(\cdot)</span> and the entire distribution, for any given <span class="math inline">a_i(\phi)</span>. This justifies the <span class="blue">notation</span> <span class="math inline">Y_i \sim \text{ED}(\mu_i, a_i(\phi)v(\mu_i))</span>.</p></li>
</ul>
</div>
</section>
<section id="gaussian-distribution" class="slide level2 center">
<h2>Gaussian distribution üìñ</h2>
<ul>
<li><p>Let <span class="math inline">Y_i \sim \text{N}(\mu_i, \sigma^2)</span>. The <span class="blue">density</span> function of <span class="math inline">Y_i</span> can be written as <span class="math display">
\begin{aligned}
p(y_i; \mu_i, \sigma^2) &amp;= \frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(y_i - \mu_i)^2\right\}
\\
&amp;=\exp\left\{\frac{y_i \mu_i - \mu_i^2/2}{\sigma^2}- \frac{\log(2\pi\sigma^2)}{2}-\frac{y_i^2}{2\sigma^2}\right\}
\end{aligned}
</span></p></li>
<li><p>Then, we can recognize the following relationships: <span class="math display">
\theta_i = \theta(\mu_i) = \mu_i, \quad a_i(\phi) = \phi = \sigma^2, \quad b(\theta_i) = \frac{\theta_i^2}{2}, \quad c(y_i, \phi) = - \frac{\log(2\pi\phi)}{2}-\frac{y_i^2}{2\phi}.
</span> In the Gaussian case, the <span class="blue">mean parametrization</span> and the <span class="orange">natural parametrization</span> coincide. Moreover, the <span class="orange">dispersion</span> <span class="math inline">\phi</span> coincides with the <span class="blue">variance</span> <span class="math inline">\sigma^2</span>.</p></li>
<li><p>Using the results we previously discussed, we obtain the well-known relationships <span class="math display">
\mathbb{E}(Y_i) = b'(\theta_i) = \theta_i, \qquad \text{var}(Y_i) = a_i(\phi)b''(\theta_i) = \phi.
</span> The <span class="blue">variance function</span> <span class="math inline">v(\mu_i) = 1</span> is <span class="orange">constant</span>. We will write <span class="math inline">Y_i \sim \text{ED}(\mu_i, \phi)</span> with <span class="math inline">\mu_i \in \mathcal{M} = \mathbb{R}</span>.</p></li>
</ul>
</section>
<section id="poisson-distribution" class="slide level2 center">
<h2>Poisson distribution üìñ</h2>
<p>Let <span class="math inline">Y_i \sim \text{Poisson}(\mu_i)</span>. The <span class="blue">pdf</span> function of <span class="math inline">Y_i</span> can be written as <span class="math display">
\begin{aligned}
p(y_i; \mu_i) &amp;= \frac{\mu_i^{y_i} e^{-\mu_i}}{y_i!}=\exp\{y_i \log(\mu_i) - \mu_i - \log(y_i!)\} \\
&amp;=\exp\{y_i \theta_i - e^{\theta_i} - \log(y_i!)\}, \qquad y_i = 0, 1, 2,\dots.
\end{aligned}
</span></p>
<ul>
<li><p>Then, we can recognize the following relationships: <span class="math display">
\begin{aligned}
\theta_i &amp;= \theta(\mu_i) = \log(\mu_i), \quad &amp;&amp;a_i(\phi) = 1, \\
b(\theta_i) &amp;= e^{\theta_i}, \quad &amp;&amp;c(y_i, \phi) = c(y_i) = -\log(y_i!).
\end{aligned}
</span> There is <span class="orange">no dispersion</span> parameter since <span class="math inline">a_i(\phi) = 1</span>.</p></li>
<li><p>Using the results we previously discussed, we obtain the well-known relationships <span class="math display">
\begin{aligned}
\mathbb{E}(Y_i) &amp;= b'(\theta_i) = e^{\theta_i} = \mu_i, \\
\text{var}(Y_i) &amp;= a_i(\phi) b''(\theta_i) = e^{\theta_i} = \mu_i.
\end{aligned}
</span> The <span class="blue">variance function</span> <span class="math inline">v(\mu_i) = \mu_i</span> is <span class="orange">linear</span>. We will write <span class="math inline">Y_i \sim \text{ED}(\mu_i, \mu_i)</span> with <span class="math inline">\mu_i \in (0, \infty)</span>.</p></li>
</ul>
</section>
<section id="gamma-distribution-i" class="slide level2 center">
<h2>Gamma distribution I üìñ</h2>
<ul>
<li><p>Let <span class="math inline">Y_i \sim \text{Gamma}(\alpha, \lambda_i)</span>. The <span class="blue">density</span> function of <span class="math inline">Y_i</span> can be written as <span class="math display">
\begin{aligned}
p(y_i; \alpha, \lambda_i) &amp;= \frac{\lambda_i^\alpha y_i^{\alpha-1}\alpha e^{-\lambda_i y_i}}{\Gamma(\alpha)}
\\
&amp;=\exp\left\{\alpha\log{\lambda_i} - \lambda_i y_i + (\alpha-1)\log{y_i}  - \log{\Gamma}(\alpha)\right\} \\
&amp;=\exp\left\{\alpha\left(\log{\lambda_i} - \frac{\lambda_i}{\alpha} y_i\right) + (\alpha-1)\log{y_i}  - \log{\Gamma}(\alpha)\right\} \\
&amp;=\exp\left\{\frac{\theta_i y_i + \log(-\theta_i)}{\phi} - (1/\phi)\log{\phi}+ (1/\phi - 1)\log{y_i}  - \log{\Gamma}(1/\phi)\right\}, \qquad y &gt; 0,\\
\end{aligned}
</span> having defined the <span class="blue">dispersion</span> <span class="math inline">\phi = 1/\alpha</span> and the <span class="orange">natural parameter</span> <span class="math inline">\theta_i = -\lambda_i/\alpha</span>.</p></li>
<li><p>Then, we can recognize the following relationships: <span class="math display">
\begin{aligned}
\quad a_i(\phi) &amp;= \phi, \qquad b(\theta_i) = - \log(-\theta_i), \\
c(y_i, \phi) &amp;= -  (1/\phi)\log{\phi}+ (1/\phi - 1)\log{y_i}  - \log{\Gamma}(1/\phi).
\end{aligned}
</span></p></li>
</ul>
</section>
<section id="gamma-distribution-ii" class="slide level2 center">
<h2>Gamma distribution II üìñ</h2>
<ul>
<li><p>Using the results we previously discussed, we obtain the well-known relationships <span class="math display">
\mathbb{E}(Y_i) = b'(\theta_i) = - \frac{1}{\theta_i} = \frac{\alpha}{\lambda_i} = \mu_i, \qquad \text{var}(Y_i) = a_i(\phi)b''(\theta_i) = \frac{\phi}{\theta_i^2} = \frac{\alpha}{\lambda_i^2}.
</span></p></li>
<li><p>At the same time, we can write the <span class="orange">inverse</span> relationship linking <span class="math inline">\theta_i</span> to the <span class="blue">mean</span> as <span class="math display">
\theta_i = \theta(\mu_i) = - \frac{1}{\mu_i}
</span> from which we finally obtain the following <span class="blue">quadratic</span> variance function <span class="math display">
v(\mu_i) = \mu_i^2.
</span></p></li>
<li><p>We will write <span class="math inline">Y_i \sim \text{ED}(\mu_i, \phi\mu_i^2)</span> with <span class="math inline">\mu_i \in (0, \infty)</span>.</p></li>
</ul>
</section>
<section id="binomial-distribution-i" class="slide level2 center">
<h2>Binomial distribution I üìñ</h2>
<ul>
<li>Let <span class="math inline">S_i \sim \text{Binomial}(m_i, \pi_i)</span>, with <span class="math inline">\pi_i \in (0, 1)</span>. The random variable <span class="math inline">Y_i = S_i/m_i</span> has <span class="blue">density</span> <span class="math display">
\begin{aligned}
p(y_i; m_i, \pi_i) &amp;= \binom{m_i}{m_i y_i}\pi_i^{m_i y_i}(1 - \pi_i)^{m_i - m_i y_i}\\
&amp;=\binom{m_i}{m_i y_i}\left(\frac{\pi_i}{1 - \pi_i}\right)^{m_i y_i}(1 - \pi_i)^{m_i}\\
&amp;=\exp\left\{m_iy_i\log\left(\frac{\pi_i}{1 - \pi_i}\right) + m_i\log(1 - \pi_i) + \log\binom{m_i}{m_i y_i}\right\},
\end{aligned}
</span> for <span class="math inline">y_i \in \{0, 1/m_i, 2/m_2, \dots, m_i/m_i\}</span>. This can be written as <span class="math display">
p(y_i; m_i, \pi_i) =\exp\left\{\frac{y_i\theta_i - \log\{1 + \exp(\theta_i)\}}{1/m_i}+ \log\binom{m_i}{m_i y_i}\right\},
</span> where the <span class="orange">natural parameter</span> is <span class="math inline">\theta_i = \text{logit}(\pi_i) = \log\{\pi/(1-\pi_i)\}</span>.</li>
</ul>
</section>
<section id="binomial-distribution-ii" class="slide level2 center">
<h2>Binomial distribution II üìñ</h2>
<ul>
<li><p>Note that <span class="math inline">\mathbb{E}(Y_i) = \mathbb{E}(Z_i / m_i) = \pi_i = \mu_i</span>. This means there <span class="orange">no dispersion</span> parameter <span class="math inline">\phi</span> and <span class="math display">
\theta_i = \text{logit}(\mu_i), \quad a_i(\phi) = \frac{1}{m_i}, \quad b(\theta_i) = \log\{1 + \exp(\theta_i)\}, \quad c(y_i) = \log\binom{m_i}{m_i y_i}.
</span></p></li>
<li><p>Using the general formulas therefore we obtain <span class="math display">
\begin{aligned}
\mathbb{E}(Y_i) &amp;= b'(\theta_i) = \frac{\exp(\theta_i)}{1 + \exp(\theta_i)} = \mu_i, \\
\text{var}(Y_i) &amp;= a_i(\phi)b''(\theta_i) = \frac{1}{m_i}\frac{\exp(\theta_i)}{[1 + \exp(\theta_i)]^2} = \frac{\mu_i (1 - \mu_i)}{m_i},
\end{aligned}
</span> from which we obtain that the <span class="blue">variance function</span> is <span class="math inline">v(\mu_i) = \mu_i(1-\mu_i)</span> is quadratic.</p></li>
<li><p>We will write <span class="math inline">Y_i \sim \text{ED}(\mu_i, \mu_i(1-\mu_i))</span> with <span class="math inline">\mu_i \in \mathcal{M} = (0, 1)</span>.</p></li>
</ul>
</section>
<section id="notable-exponential-dispersion-families" class="slide level2 center">
<h2>Notable exponential dispersion families</h2>
<table class="caption-top">
<colgroup>
<col style="width: 25%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th><span class="math inline">\text{N}(\mu_i, \sigma^2)</span></th>
<th><span class="math inline">\text{Gamma}(\alpha, \alpha/\mu_i)</span></th>
<th><span class="math inline">\frac{1}{m_i}\text{Binomial}(m_i, \mu_i)</span></th>
<th><span class="math inline">\text{Poisson}(\mu_i)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="blue">Support</span> <span class="math inline">\mathcal{Y}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">[0, \infty)</span></td>
<td><span class="math inline">\{0, 1/m_i,\dots, 1\}</span></td>
<td><span class="math inline">\mathbb{N}</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\theta_i</span></td>
<td><span class="math inline">\mu_i</span></td>
<td><span class="math inline">- 1/\mu_i</span></td>
<td><span class="math inline">\log\left(\frac{\mu_i}{1 - \mu_i}\right)</span></td>
<td><span class="math inline">\log{\mu_i}</span></td>
</tr>
<tr class="odd">
<td><span class="orange">Parametric space</span> <span class="math inline">\Theta</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">(-\infty, 0)</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
</tr>
<tr class="even">
<td><span class="math inline">b(\theta_i)</span></td>
<td><span class="math inline">\theta_i^2/2</span></td>
<td><span class="math inline">-\log(-\theta_i)</span></td>
<td><span class="math inline">\log\{1 + \exp(\theta_i)\}</span></td>
<td><span class="math inline">\exp(\theta_i)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\phi</span></td>
<td><span class="math inline">\sigma^2</span></td>
<td><span class="math inline">1/\alpha</span></td>
<td><span class="math inline">1</span></td>
<td><span class="math inline">1</span></td>
</tr>
<tr class="even">
<td><span class="math inline">a_i(\phi)</span></td>
<td><span class="math inline">\sigma^2</span></td>
<td><span class="math inline">1/\alpha</span></td>
<td><span class="math inline">1/m_i</span></td>
<td><span class="math inline">1</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\mathcal{M}</span></td>
<td><span class="math inline">\mathbb{R}</span></td>
<td><span class="math inline">(0, \infty)</span></td>
<td><span class="math inline">(0, 1)</span></td>
<td><span class="math inline">(0, \infty)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">v(\mu_i)</span></td>
<td><span class="math inline">1</span></td>
<td><span class="math inline">\mu_i^2</span></td>
<td><span class="math inline">\mu_i(1-\mu_i)</span></td>
<td><span class="math inline">\mu_i</span></td>
</tr>
</tbody>
</table>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The list of exponential dispersion families does not end here. Other examples are the <span class="blue">inverse Gaussian</span>, the <span class="orange">negative binomial</span> and <span class="grey">hyperbolic secant</span> distributions.</p>
</div>
</div>
</div>
</section>
<section id="link-functions-and-canonical-link" class="slide level2 center">
<h2>Link functions and canonical link</h2>
<ul>
<li><p>To complete the GLM specification, we need to choose a <span class="blue">link function</span> <span class="math inline">g(\cdot)</span> such that: <span class="math display">
g(\mu_i) = \bm{x}_i^T\beta, \qquad \theta_i = \theta(\mu_i) \quad \Longrightarrow \quad \theta_i = \theta(g^{-1}(\bm{x}_i^T\beta)).
</span></p></li>
<li><p>It is fairly natural to consider a <span class="blue">monotone</span> and <span class="orange">differentiable</span> link function <span class="math inline">g(\cdot) : \mathcal{M} \to \mathbb{R}</span> so that the inverse <span class="math inline">g^{-1}(\cdot) : \mathbb{R} \to \mathcal{M}</span>. This ensures that the predictions are well-defined. <span class="math display">
\mathbb{E}(Y_i) = g^{-1}(\bm{x}_i^T\beta) \in \mathcal{M}.
</span></p></li>
<li><p>For example, in <span class="blue">binary regression</span> any continuous <span class="orange">cumulative distribution function</span> for <span class="math inline">g^{-1}(\cdot)</span> leads to a good link function, such as <span class="math inline">g(\cdot) = \Phi(\cdot)</span> (probit) or <span class="math inline">g^{-1}(\eta_i) = e^{\eta_i}/(1 + e^{\eta_i})</span> (logistic).</p></li>
</ul>
<div class="fragment">
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The following link is called <span class="orange">canonical link</span> and it is implied by the distribution: <span class="math display">
g(\mu_i) = \theta(\mu_i) \quad \Longrightarrow \quad \theta_i = \bm{x}_i^T\beta.
</span> Such a choice leads to remarkable simplifications in the likelihood function.</p>
</div>
</div>
</div>
<ul>
<li>The <span class="blue">identity</span> link is canonical for the <span class="blue">Gaussian</span>, the <span class="orange">logarithm</span> is canonical for the <span class="orange">Poisson</span>, the <span class="grey">logit</span> is canonical for the Binomial and the <span class="blue">reciprocal</span> is canonical for the <span class="blue">Gamma</span>.</li>
</ul>
<!-- - In a Gamma GLM, the reciprocal canonical link does not map $\bm{x}_i^T\beta$ into positive values, therefore the logarithmic link is sometimes preferred.  -->
</div>
</section></section>
<section>
<section id="likelihood-quantities" class="title-slide slide level1 center">
<h1>Likelihood quantities</h1>

</section>
<section id="likelihood-function" class="slide level2 center">
<h2>Likelihood function</h2>
<ul>
<li><p>Let <span class="math inline">Y_i \overset{\text{ind}}{\sim}\text{ED}(\mu_i, a_i(\phi)v(\mu_i))</span> be the response variable of a GLM, with <span class="math inline">g(\mu_i) = \bm{x}_i^T\beta</span>. The <span class="blue">joint distribution</span> of the responses <span class="math inline">\bm{Y} = (Y_1,\dots,Y_n)</span> is <span class="math display">
p(\bm{y}; \beta, \phi) = \prod_{i=1}^np(y_i; \beta, \phi) = \prod_{i=1}^n \exp\left\{\frac{y_i\theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi)\right\}.
</span> with <span class="math inline">\theta_i = \theta(\mu_i) = \theta(g^{-1}(\bm{x}_i^T\beta))</span>.</p></li>
<li><p>The <span class="orange">log-likelihood</span> function therefore is <span class="math display">
\ell(\beta, \phi) = \sum_{i=1}^n\frac{y_i\theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi).
</span></p></li>
</ul>
<div class="fragment">
<ul>
<li>In general, there is <span class="orange">no sufficient statistic</span> with dimension smaller than <span class="math inline">n</span>.</li>
</ul>
</div>
</section>
<section id="sufficient-statistics-and-canonical-link" class="slide level2 center">
<h2>‚ò†Ô∏è - Sufficient statistics and canonical link</h2>
<ul>
<li>Consider the <span class="blue">canonical link</span>, namely <span class="math inline">\theta(\mu_i) = g(\mu_i)</span> so that <span class="math inline">\theta_i = \bm{x}_i^T\beta</span>. Then the log-likelihood function <span class="orange">simplifies</span>: <span class="math display">
\begin{aligned}
\ell(\beta, \phi) &amp;= \sum_{i=1}^n\frac{y_i\bm{x}_i^T\beta - b(\bm{x}_i^T\beta)}{a_i(\phi)} + c(y_i, \phi) \\
&amp;= \left[\beta_1\sum_{i=1}^n\frac{x_{i1}y_i}{a_i(\phi)} + \cdots + \beta_p\sum_{i=1}^n\frac{x_{ip}y_i}{a_i(\phi)} \right] - \sum_{i=1}^n\frac{b(\bm{x}_i^T\beta)}{a_i(\phi)} + c(y_i, \phi).
\end{aligned}
</span> If <span class="math inline">\phi</span> were <span class="blue">known</span>, say <span class="math inline">a_i(\phi) = 1</span> or <span class="math inline">a_i(\phi) = 1/\omega_i</span>, then <span class="math display">
\left(\sum_{i=1}^n\frac{1}{a_i(\phi)}x_{i1}y_i, \dots, \sum_{i=1}^n\frac{1}{a_i(\phi)}x_{ip}y_i \right),
</span> is (minimal) <span class="blue">sufficient</span> of dimension <span class="math inline">p \le n</span> for inference on <span class="math inline">\beta</span>.</li>
</ul>
<div class="fragment">
<ul>
<li>In <span class="blue">logistic regression</span> for binary observations and <span class="orange">Poisson regression</span> with logarithmic link we have <span class="math inline">a_i(\phi) = 1</span>. The <span class="blue">sufficient</span> statistic is <span class="math inline">\bm{X}^T\bm{y} = \sum_{i=1}^n\bm{x}_iy_i = \left(\sum_{i=1}^nx_{i1}y_i, \dots, \sum_{i=1}^nx_{ip}y_i \right).</span></li>
</ul>
</div>
</section>
<section id="likelihood-equations-i" class="slide level2 center">
<h2>Likelihood equations I üìñ</h2>
<ul>
<li><p>To conduct inference using the <span class="orange">classical theory</span> (as in <em>Statistica II</em>), we need to consider the first and second derivative of the log-likelihood, that is, the <span class="blue">score function</span> <span class="math display">
\ell_*(\beta;\phi) := \frac{\partial}{\partial \beta_r}\ell(\beta; \phi), \qquad r=1,\dots,p,
</span> and the <span class="orange">observed information matrix</span> <span class="math inline">\bm{J}</span>, whose elements are <span class="math display">
j_{rs} = - \frac{\partial}{\partial \beta_r}\frac{\partial}{\partial \beta_s}\ell(\beta; \phi), \qquad r, s=1,\dots,p.
</span></p></li>
<li><p>These quantities have a <span class="blue">simple expression</span> in the end, but getting there requires quite a <span class="orange">bit of calculus</span>.</p></li>
</ul>
<div class="fragment">
<div class="callout callout-warning no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let us focus on the estimation of <span class="math inline">\beta</span>, assuming for now that <span class="math inline">\phi</span> is a <span class="blue">known parameter</span>, as is the case in binomial or Poisson regression.</p>
<p>This assumption is not restrictive, even when <span class="math inline">\phi</span> is actually unknown. In fact, we will show that the maximum likelihood estimate <span class="math inline">\hat{\beta}</span> does not depend on <span class="math inline">\phi</span>, and that <span class="math inline">\beta</span> and <span class="math inline">\phi</span> are <span class="orange">orthogonal</span>.</p>
</div>
</div>
</div>
</div>
</section>
<section id="likelihood-equations-ii" class="slide level2 center">
<h2>Likelihood equations II üìñ</h2>
<ul>
<li><p>Let us begin by noting that <span class="math display">
\ell_*(\beta;\phi) = \frac{\partial}{\partial \beta_r}\ell(\beta; \phi) =  \sum_{i=1}^n\frac{1}{a_i(\phi)} \left(y_i \frac{\partial \theta_i}{\partial \beta_r} - \frac{\partial b(\theta_i)}{\partial \beta_r} \right), \qquad r = 1,\dots,p.
</span> Such an expression can be <span class="blue">simplified</span> because <span class="math display">
\frac{\partial b(\theta_i)}{\partial \beta_r} = b'(\theta_i)\frac{\partial \theta_i}{\partial \beta_r} = \mu_i\frac{\partial \theta_i}{\partial \beta_r},
</span> which implies that the score function will have the following <span class="orange">structure</span>: <span class="math display">
\frac{\partial}{\partial \beta_r}\ell(\beta; \phi) = \sum_{i=1}^n\frac{1}{a_i(\phi)}(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r}, \qquad r=1,\dots,p.
</span></p></li>
<li><p>Recall that <span class="math inline">a_i(\phi) = \phi/\omega_i</span>, hence the <span class="blue">maximum likelihood estimator</span> is obtained by solving: <span class="math display">
\textcolor{red}{\cancel{\frac{1}{\phi}}}\sum_{i=1}^n\omega_i(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r} = 0, \qquad r=1,\dots,p.
</span></p></li>
</ul>
</section>
<section id="likelihood-equations-iii" class="slide level2 center">
<h2>Likelihood equations III üìñ</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">f(x)</span> be a function with <span class="orange">inverse</span> <span class="math inline">g(x) = f^{-1}(x)</span> and <span class="blue">first derivative</span> <span class="math inline">f'(x)</span>. Then <span class="math display">
\frac{\partial g}{\partial{x}} = [f^{-1}]'(x) = \frac{1}{f'(f^{-1}(x))}.
</span></p>
</div>
</div>
</div>
<ul>
<li>Recall that <span class="math inline">g(\mu_i) = \bm{x}_i^T\beta = \eta_i</span> and that <span class="math inline">\theta_i = \theta(\mu_i)</span> is the inverse of <span class="math inline">\mu(\theta_i)</span>. As an <span class="blue">application</span> of the above <span class="blue">lemma</span>: <span class="math display">
\frac{\partial \theta_i}{\partial \mu_i} = \theta'(\mu_i) = \frac{1}{\mu'(\theta(\mu_i))}= \frac{1}{b''(\theta(\mu_i))} = \frac{1}{v(\mu_i)},
</span> Moreover, since we <span class="math inline">\mu_i = g^{-1}(\eta_i)</span> we obtain <span class="math display">
\frac{\partial \mu_i}{\partial \eta_i} = \frac{1}{g'(g^{-1}(\eta_i))} = \frac{1}{g'(\mu_i)}.
</span></li>
<li>Summing up, the <span class="orange">chain rule of derivation</span> for <span class="blue">composite functions</span> gives: <span class="math display">
\frac{\partial \theta_i}{\partial \beta_r} = \frac{\partial \theta_i}{\partial \mu_i} \frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \eta_i}{\partial\beta_r} = \frac{1}{v(\mu_i)}\frac{1}{g'(\mu_i)}x_{ir}, \qquad r=1,\dots,p.
</span></li>
</ul>
</section>
<section id="likelihood-equations-iv" class="slide level2 center">
<h2>Likelihood equations IV üìñ</h2>
<ul>
<li>Combining all the above equations, we obtain an explicit formula for the <span class="blue">score function</span> <span class="math display">
\frac{\partial}{\partial \beta_r}\ell(\beta; \phi)  = \frac{1}{\phi}\sum_{i=1}^n \omega_i \frac{(y_i - \mu_i)}{v(\mu_i)}\frac{x_{ir}}{g'(\mu_i)} = \sum_{i=1}^n \frac{(y_i - \mu_i)}{\text{var}(Y_i)}\frac{x_{ir}}{g'(\mu_i)}, \qquad r=1,\dots,p.
</span></li>
</ul>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The <span class="blue">maximum likelihood estimator</span> solves the <span class="orange">likelihood equations</span>: <span class="math display">
\sum_{i=1}^n \omega_i \frac{(y_i - \mu_i)}{v(\mu_i)}\frac{x_{ir}}{g'(\mu_i)} = 0, \qquad r=1,\dots,p,
</span> which <span class="orange">do not depend</span> on <span class="math inline">\phi</span>. In <span class="blue">matrix notation</span> <span class="math display">
\bm{D}^T \bm{V}^{-1}(\bm{y} - \bm{\mu}) = \bm{0},
</span> where <span class="math inline">\bm{V} = \text{diag}(v(\mu_1)/\omega_1,\dots,v(\mu_n)/\omega_n)</span> and <span class="math inline">\bm{D}</span> is an <span class="math inline">n \times p</span> matrix whose elements are <span class="math display">
d_{ir} = \frac{\partial \mu_i}{\partial \beta_r} =\frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \eta_i}{\partial\beta_r} =\frac{1}{g'(\mu_i)}x_{ir}, \qquad i=1,\dots,n, \quad r=1,\dots,p.
</span></p>
</div>
</div>
</div>
</section>
<section id="canonical-link-simplifications" class="slide level2 center">
<h2>Canonical link: simplifications üìñ</h2>
<ul>
<li>When using the <span class="orange">canonical link</span> <span class="math inline">\theta(\mu_i) = g(\mu_i)</span> significant simplifications arise, because <span class="math display">
\frac{\partial \theta_i}{\partial \mu_i}= \frac{1}{v(\mu_i)} = g'(\mu_i) \quad \Longrightarrow\quad v(\mu_i)g'(\mu_i) = 1.
</span> Thus, plugging-in this equality in the former equations, gives: <span class="math display">
\frac{\partial \theta_i}{\partial \beta_r} = x_{ir}, \qquad r=1,\dots,p,
</span> which is not surprising, because the canonical link implies <span class="math inline">\theta_i = x_{i1}\beta_1 + \cdots + x_{ip}\beta_p</span>.</li>
</ul>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The <span class="orange">likelihood equations</span> under the <span class="blue">canonical link</span> are <span class="math display">
\sum_{i=1}^n \omega_i (y_i - \mu_i)x_{ir} = 0, \qquad r=1,\dots,p.
</span> Let <span class="math inline">\bm{\Omega} = \text{diag}(\omega_1,\dots,\omega_n)</span>, then in <span class="blue">matrix notation</span> we have <span class="math inline">\bm{X}^T\bm{\Omega}(\bm{y} - \bm{\mu}) = \bm{0}</span>. The equations simplify even further when the weights are constant, i.e.&nbsp;<span class="math inline">\bm{\Omega} = I_n</span>, yielding <span class="math inline">\bm{X}^T(\bm{y} - \bm{\mu}) = \bm{0}</span>.</p>
</div>
</div>
</div>
</section>
<section id="examples-of-estimating-equations" class="slide level2 center">
<h2>Examples of estimating equations</h2>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y_i \sim \text{ED}(\mu_i, \phi)</span> with <span class="math inline">g(\mu_i) = \mu_i</span>, namely the <span class="orange">Gaussian</span> linear model with the <span class="grey">identity</span> (canonical) link. The likelihood equations are <span class="math display">
\bm{X}^T(\bm{y} - \bm{X}\beta) = \bm{0},
</span> which are also called <span class="orange">normal equations</span>. Their solution over <span class="math inline">\beta</span> is the OLS <span class="math inline">\hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}</span>.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y_i \sim \text{ED}(\mu_i, \phi/\omega_i)</span> with <span class="math inline">g(\mu_i) = \mu_i</span>, namely the <span class="orange">Gaussian</span> linear model with the <span class="grey">identity</span> (canonical) link and <span class="blue">heteroschedastic errors</span>. The likelihood equations are <span class="math display">
\bm{X}^T\bm{\Omega}(\bm{y} - \bm{X}\beta) = \bm{0},
</span> Their solution over <span class="math inline">\beta</span> is the weighted least square estimator <span class="math inline">\hat{\beta}_\text{wls} = (\bm{X}^T\bm{\Omega}\bm{X})^{-1}\bm{X}^T\bm{\Omega}\bm{y}</span>.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let <span class="math inline">Y_i \sim \text{ED}(\mu_i, \mu_i)</span> with <span class="math inline">g(\mu_i) = \log{\mu_i}</span>, namely a <span class="orange">Poisson</span> regression model with the <span class="grey">logarithmic</span> (canonical) link. The likelihood equations can be solved <span class="orange">numerically</span> <span class="math display">
\bm{X}^T(\bm{y} - \bm{\mu}) = \bm{0}, \qquad \bm{\mu} = (e^{\bm{x}_1^T\beta}, \dots,  e^{\bm{x}_n^T\beta}).
</span></p>
</div>
</div>
</div>
</section>
<section id="example-beetles-data" class="slide level2 center">
<h2>Example: <code>Beetles</code> data</h2>
<ul>
<li><p>Using the <code>Beetles</code> data, we specified a <span class="blue">binomial logistic</span> regression model for the counts <span class="math inline">m_i Y_i \sim \text{Binomial}(m_i, \pi_i)</span> with mean <span class="math inline">\mathbb{E}(Y_i) = \pi_i = \exp(\beta_1 + \beta_2 x_i)/(1 + \exp(\beta_1 + \beta_2 x_i))</span>.</p></li>
<li><p>The <span class="orange">maximum likelihood estimate</span> <span class="math inline">(\hat{\beta}_1, \hat{\beta}_2)</span> is the value solving simultaneously: <span class="math display">
\sum_{i=1}^n m_i y_i =  \sum_{i=1}^n m_i \frac{\exp(\beta_1 + \beta_2x_i)}{1 + \exp(\beta_1 + \beta_2x_i)}, \quad
\text{and}\quad \sum_{i=1}^n m_i x_i y_i =  \sum_{i=1}^n m_i x_i \frac{\exp(\beta_1 + \beta_2x_i)}{1 + \exp(\beta_1 + \beta_2x_i)}.
</span> Unfortunately, there is <span class="orange">no closed form</span> solution.</p></li>
<li><p>In our case, we have that <span class="math display">
\sum_{i=1}^n m_i y_i = 291, \qquad \sum_{i=1}^n m_i x_i y_i = 532.2083.
</span></p></li>
<li><p>With these values, we can use the <span class="orange">numerical</span> algorithm IRLS to solve the above system, obtaining <span class="math display">
\hat{\beta} = (\hat{\beta}_1, \hat{\beta_2}) = (-60.717, 34.270).
</span></p></li>
</ul>
</section>
<section id="example-beetles-data-1" class="slide level2 center">
<h2>Example: <code>Beetles</code> data</h2>
<ul>
<li>The <span class="blue">predicted response</span> can be computed by using the formula <span class="math display">
\hat{\mu}_i = \frac{\exp(\hat{\beta}_1 + \hat{\beta}_2x_i)}{1 + \exp(\hat{\beta}_1 + \hat{\beta}_2x_i)} = \frac{\exp(-60.717 + 34.270 x_i)}{1 + \exp(-60.717 + 34.270 x_i)}, \qquad i=1,\dots, 8.
</span></li>
</ul>
<table class="caption-top" style="width:100%;">
<colgroup>
<col style="width: 7%">
<col style="width: 16%">
<col style="width: 19%">
<col style="width: 28%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;"><span class="math inline">m_i</span></th>
<th style="text-align: right;"><code>deaths</code> (<span class="math inline">S_i</span>)</th>
<th style="text-align: right;"><code>logdose</code> (<span class="math inline">x_i</span>)</th>
<th style="text-align: right;"><span class="math inline">Y_i = S_i / m_i</span></th>
<th style="text-align: right;"><span class="math inline">\hat{\mu}_i</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">59</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">1.691</td>
<td style="text-align: right;">0.102</td>
<td style="text-align: right;">0.059</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">1.724</td>
<td style="text-align: right;">0.217</td>
<td style="text-align: right;">0.164</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">1.755</td>
<td style="text-align: right;">0.290</td>
<td style="text-align: right;">0.362</td>
</tr>
<tr class="even">
<td style="text-align: right;">56</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">1.784</td>
<td style="text-align: right;">0.500</td>
<td style="text-align: right;">0.605</td>
</tr>
<tr class="odd">
<td style="text-align: right;">63</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">1.811</td>
<td style="text-align: right;">0.825</td>
<td style="text-align: right;">0.795</td>
</tr>
<tr class="even">
<td style="text-align: right;">59</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">1.837</td>
<td style="text-align: right;">0.898</td>
<td style="text-align: right;">0.903</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">1.861</td>
<td style="text-align: right;">0.984</td>
<td style="text-align: right;">0.955</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">1.884</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.979</td>
</tr>
</tbody>
</table>
<ul>
<li>The predicted values and the data <span class="math inline">Y_i</span> were also shown in a <span class="blue">plot</span> at the <a href="./un_B_slides.html#/beetles-data-fitted-model">beginning of this unit</a>.</li>
</ul>
</section>
<section id="example-aids-data" class="slide level2 center">
<h2>Example: <code>Aids</code> data</h2>
<ul>
<li><p>In the <code>Aids</code> data, we specified a <span class="blue">Poisson</span> regression model with <span class="math inline">\mathbb{E}(Y_i) = \exp(\beta_1 + \beta_2 x_i)</span>.</p></li>
<li><p>The maximum likelihood estimate <span class="math inline">(\hat{\beta}_1, \hat{\beta}_2)</span> <span class="orange">solve</span> simultaneously: <span class="math display">
\sum_{i=1}^n y_i =  \sum_{i=1}^n \exp(\beta_1 + \beta_2x_i), \quad
\text{and}\quad \sum_{i=1}^n x_i y_i =  \sum_{i=1}^n x_i\exp(\beta_1 + \beta_2 x_i).
</span></p></li>
<li><p>This system does <span class="orange">not</span> always admits a <span class="orange">solution</span>. This happens, for example, in the <span class="blue">extreme case</span> <span class="math inline">\sum_{i=1}^ny_i = 0</span>, occurring when all counts equal zero.</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>Using the <code>Aids</code> data we have <span class="math inline">\sum_{i=1}^ny_i = 217</span> and <span class="math inline">\sum_{i=1}^nx_i y_i = 2387</span>. Via <span class="orange">numerical methods</span> we solve the above system of equations and we obtain <span class="math inline">\hat{\beta}_1 = 0.304</span> and <span class="math inline">\hat{\beta}_2 = 0.259</span>.</p></li>
<li><p>The estimated mean values are <span class="math inline">\hat{\mu}_i = \exp(0.304 + 0.259 x_i)</span> and in particular the mean for the <span class="blue">next period</span> is <span class="math display">
\hat{\mu}_{i+1} = \exp(0.304 + 0.259 (x_i +1)) = \exp(0.259) \hat{\mu}_i = 1.296 \hat{\mu}_i.
</span> In other words, the estimated number of deaths increases by about <span class="math inline">30\%</span> every trimester.</p></li>
</ul>
</div>
</section>
<section id="example-aids-data-1" class="slide level2 center">
<h2>Example: <code>Aids</code> data</h2>
<div class="small-table">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><code>deaths</code> (<span class="math inline">Y_i</span>)</th>
<th style="text-align: right;"><code>period</code> (<span class="math inline">x_i</span>)</th>
<th style="text-align: right;"><span class="math inline">\hat{\mu}_i</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1983-1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1.755</td>
</tr>
<tr class="even">
<td style="text-align: left;">1984-1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">2.274</td>
</tr>
<tr class="odd">
<td style="text-align: left;">1985-1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">2.946</td>
</tr>
<tr class="even">
<td style="text-align: left;">1986-1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">3.817</td>
</tr>
<tr class="odd">
<td style="text-align: left;">1983-2</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">4.945</td>
</tr>
<tr class="even">
<td style="text-align: left;">1984-2</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">6.407</td>
</tr>
<tr class="odd">
<td style="text-align: left;">1985-2</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">8.301</td>
</tr>
<tr class="even">
<td style="text-align: left;">1986-2</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">10.755</td>
</tr>
<tr class="odd">
<td style="text-align: left;">1983-3</td>
<td style="text-align: right;">23</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">13.934</td>
</tr>
<tr class="even">
<td style="text-align: left;">1984-3</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">18.052</td>
</tr>
<tr class="odd">
<td style="text-align: left;">1985-3</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">23.389</td>
</tr>
<tr class="even">
<td style="text-align: left;">1986-3</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">30.302</td>
</tr>
<tr class="odd">
<td style="text-align: left;">1983-4</td>
<td style="text-align: right;">37</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">39.259</td>
</tr>
<tr class="even">
<td style="text-align: left;">1984-4</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">50.863</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>The predicted values and the data <span class="math inline">Y_i</span> were also shown in a <span class="blue">plot</span> at the <a href="./un_B_slides.html#/aids-data-fitted-model">beginning of this unit</a>.</li>
</ul>
</section>
<section id="observed-and-expected-information-i" class="slide level2 center">
<h2>Observed and expected information I üìñ</h2>
<ul>
<li><p>Let us first consider the negative derivative of the score function, that is the <span class="orange">observed information matrix</span> <span class="math inline">\bm{J}</span> with entries: <span class="math display">
\begin{aligned}
j_{rs} &amp;=  -\frac{\partial}{\partial \beta_s}\left[\frac{\partial}{\partial \beta_r}\ell(\beta; \phi)\right] = -\frac{\partial}{\partial \beta_s}\sum_{i=1}^n\frac{1}{a_i(\phi)}(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r} \\
&amp;=\sum_{i=1}^n\frac{1}{a_i(\phi)}\left[\frac{\partial\mu_i}{\partial \beta_s}\frac{\partial\theta_i}{\partial \beta_r} - (y_i - \mu_i) \frac{\partial^2\theta_i}{\partial \beta_r \partial \beta_s}\right], \qquad r,s = 1,\dots,p.
\end{aligned}
</span></p></li>
<li><p>Let <span class="math inline">\bm{I} = \mathbb{E}(\bm{J})</span> be the <span class="math inline">p \times p</span> <span class="blue">Fisher information matrix</span> associated with <span class="math inline">\beta</span>, whose elements are <span class="math display">
i_{rs} = \mathbb{E}(j_{rs}) = \mathbb{E}\left(- \frac{\partial}{\partial \beta_r}\frac{\partial}{\partial \beta_s}\ell(\beta; \phi)\right), \qquad r,s = 1,\dots,p.
</span></p></li>
<li><p>Thus, the Fisher information matrix substantially simplifies because <span class="math inline">\mathbb{E}(Y_i) = \mu_i</span>, obtaining: <span class="math display">
i_{rs} = \sum_{i=1}^n\frac{1}{a_i(\phi)}\frac{\partial\mu_i}{\partial \beta_s}\frac{\partial\theta_i}{\partial \beta_r}, \qquad r,s = 1,\dots,p.
</span></p></li>
</ul>
</section>
<section id="observed-and-expected-information-ii" class="slide level2 center">
<h2>Observed and expected information II üìñ</h2>
<ul>
<li>In the previous slides we already computed the explicit values of these derivatives: <span class="math display">
\frac{\partial\mu_i}{\partial \beta_s} = \frac{x_{is}}{g'(\mu_i)}, \qquad \frac{\partial\theta_i}{\partial \beta_r} = \frac{x_{is}}{v(\mu_i) g'(\mu_i)}.
</span></li>
</ul>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Combining the above equations, we obtain that the <span class="orange">Fisher information</span> <span class="math inline">\bm{I}</span> of a GLM has entries <span class="math display">
i_{rs} = \frac{1}{\phi}\sum_{i=1}^n \omega_i \frac{x_{ir} x_{is}}{v(\mu_i)g'(\mu_i)^2} = \sum_{i=1}^n \frac{x_{ir}x_{is}}{\text{var}(Y_i) g'(\mu_i)^2}, \qquad r,s = 1,\dots,p.
</span> In <span class="blue">matrix notation</span>, we have that <span class="math display">
\bm{I} = \bm{X}^T\bm{W}\bm{X},
</span> where <span class="math inline">\bm{W} = \text{diag}(w_1,\dots,w_n)</span> and <span class="math inline">w_i</span> are <span class="blue">weights</span> such that <span class="math display">
w_i = \frac{1}{\phi}\frac{\omega_i}{v(\mu_i) g'(\mu_i)^2} = \frac{1}{\text{var}(Y_i) g'(\mu_i)^2}, \qquad i=1,\dots,n.
</span></p>
</div>
</div>
</div>
</section>
<section id="canonical-link-simplifications-1" class="slide level2 center">
<h2>Canonical link: simplifications üìñ</h2>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Under the <span class="orange">canonical link</span> we have that <span class="math inline">\theta_i = \beta_1x_{i1} + \cdots \beta_p x_{ip}</span>, which means that <span class="math display">
\frac{\partial^2\theta_i}{\partial \beta_r \partial \beta_s} = 0 \qquad \Longrightarrow \qquad i_{rs} = j_{rs} = \sum_{i=1}^n\frac{1}{a_i(\phi)}\frac{\partial\mu_i}{\partial \beta_s}\frac{\partial\theta_i}{\partial \beta_r}.
</span> The <span class="blue">observed information</span> <span class="math inline">\bm{J}</span> is <span class="orange">non-stochastic</span>, which means that observed information and expected (Fisher) information coincide, that is <span class="math inline">i_{rs} = j_{rs}</span> and <span class="math inline">\bm{I} = \bm{J}</span>.</p>
</div>
</div>
</div>
<div class="fragment">
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Under the <span class="orange">canonical link</span>, we also have the simplifications <span class="math inline">1/v(\mu_i) = g'(\mu_i)</span>, yielding <span class="math display">
i_{rs}  = \frac{1}{\phi}\sum_{i=1}^n \omega_i v(\mu_i)x_{ir} x_{is}, \qquad r,s = 1,\dots,p.
</span> In <span class="blue">matrix notation</span>, we have that <span class="math inline">\bm{I} = \bm{X}^T\bm{W}\bm{X}</span> with weights <span class="math display">
w_i = \frac{1}{\phi} \omega_iv(\mu_i) = \frac{v(\mu_i)}{a_i(\phi)}, \qquad i=1,\dots,n.
</span></p>
</div>
</div>
</div>
</div>
</section>
<section id="further-considerations" class="slide level2 center">
<h2>Further considerations</h2>
<ul>
<li>The <span class="orange">observed</span> and <span class="blue">expected</span> information matrices <span class="math inline">\bm{J}</span> and <span class="math inline">\bm{I}</span>, as well as weights <span class="math inline">\bm{W}</span>, <span class="blue">depend</span> on <span class="math inline">\beta</span> and <span class="math inline">\phi</span>. We write <span class="math inline">\hat{\bm{J}}</span>, <span class="math inline">\hat{\bm{I}}</span> and <span class="math inline">\hat{\bm{W}}</span> to indicate that <span class="math inline">\beta</span> and <span class="math inline">\phi</span> have been estimated with <span class="math inline">\hat{\beta}</span> and <span class="math inline">\hat{\phi}</span>.</li>
</ul>
<div class="fragment">
<ul>
<li>If <span class="math inline">\bm{X}</span> has <span class="blue">full rank</span> and <span class="math inline">g'(\mu) \neq 0</span>, then <span class="math inline">\bm{I}</span> is <span class="orange">positive definite</span> for any value of <span class="math inline">\beta</span> and <span class="math inline">\phi</span>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>Under the <span class="orange">canonical link</span>, we have <span class="math inline">\bm{J} = \bm{I}</span>, and both matrices are <span class="orange">positive definite</span> if <span class="math inline">\text{rk}(\bm{X}) = p</span>.</p></li>
<li><p>This implies that the log-likelihood function is <span class="orange">concave</span> because its second derivative is negative definite, so any <span class="orange">solution</span> to the <span class="orange">estimating equations</span> is also a <span class="blue">global optimum</span>.</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>The Fisher information matrix could be computed exploiting <span class="grey">Bartlett identity</span>, namely <span class="math display">
i_{rs} = \mathbb{E}\left[\left(\frac{\partial}{\partial \beta_r}\ell(\beta; \phi)\right)\left(\frac{\partial}{\partial \beta_s}\ell(\beta; \phi)\right)\right], \qquad r,s = 1,\dots,p.</span> as in <span class="citation" data-cites="Agresti2015">Agresti (<a href="#/references" role="doc-biblioref" onclick="">2015</a>)</span>. Of course, the final result <span class="orange">coincide</span> with ours.</li>
</ul>
</div>
</section>
<section id="orthogonality-of-beta-and-psi" class="slide level2 center">
<h2>‚ò†Ô∏è - Orthogonality of <span class="math inline">\beta</span> and <span class="math inline">\psi</span></h2>
<ul>
<li>Let us now consider the case in which <span class="math inline">\phi</span> is <span class="blue">unknown</span> so that <span class="math inline">a_i(\phi) = \phi/\omega_i</span>. We obtain: <span class="math display">
j_{r \phi} = - \frac{\partial}{\partial \beta_r}\frac{\partial}{\partial \phi}\ell(\beta; \phi) = \frac{1}{\phi^2}\sum_{i=1}^n \omega_i(y_i - \mu_i)\frac{\partial \theta_i}{\partial \beta_r}, \qquad r = 1,\dots,p.
</span> whose <span class="orange">expected value</span> is <span class="math inline">i_{r\phi} = \mathbb{E}(j_{r\phi}) = 0</span> since <span class="math inline">\mathbb{E}(Y_i) = \mu_i</span>.</li>
</ul>
<div class="fragment">
<ul>
<li>This means the Fisher information matrix accounting for <span class="math inline">\phi</span> takes the form: <span class="math display">
\begin{pmatrix}
\bm{I} &amp; \bm{0} \\
\bm{0} &amp; i_{\phi \phi}
\end{pmatrix} \qquad\Longrightarrow\qquad  \begin{pmatrix}
\bm{I} &amp; \bm{0} \\
\bm{0} &amp; i_{\phi \phi}
\end{pmatrix}^{-1} = \begin{pmatrix}
\bm{I}^{-1} &amp; \bm{0} \\
\bm{0} &amp; 1 /i_{\phi \phi}
\end{pmatrix}
</span> where <span class="math inline">[\bm{I}]_{rs} = i_{rs}</span> are the elements associated to <span class="math inline">\beta</span> as before.</li>
</ul>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The parameters <span class="math inline">\beta</span> and <span class="math inline">\phi</span> are <span class="orange">orthogonal</span> and their maximum likelihood estimates are <span class="blue">asymptotically independent</span>.</p>
<p>Moreover, the matrices <span class="math inline">\bm{I}</span> and <span class="math inline">\bm{I}^{-1}</span> are sufficient for inference on <span class="math inline">\beta</span> and there is no need to compute <span class="math inline">i_{\phi \phi}</span>. Note that the maximum likelihood <span class="math inline">\hat{\beta}</span> can also be computed without knowing <span class="math inline">\phi</span>.</p>
</div>
</div>
</div>
<!-- - In a GLM with $p = 2$ the Fisher information matrix, evaluated on a given $\beta,\phi$, looks like: -->
<!-- $$ -->
<!-- \begin{pmatrix} -->
<!-- i_{11} & i_{12} &  i_{1\phi} \\ -->
<!-- i_{21} & i_{22} &  i_{2\phi} \\ -->
<!-- i_{\phi 1} & i_{\phi2 } &  i_{\phi\phi} \\ -->
<!-- \end{pmatrix}= \begin{pmatrix} -->
<!-- 10.39 & -1.22 & 0 \\  -->
<!-- -1.22 & 10.39 & 0 \\  -->
<!-- 0 & 0 & 10 \\  -->
<!--   \end{pmatrix} \Longrightarrow \begin{pmatrix} -->
<!-- i_{11} & i_{12} &  i_{1\phi} \\ -->
<!-- i_{21} & i_{22} &  i_{2\phi} \\ -->
<!-- i_{\phi 1} & i_{\phi2 } &  i_{\phi\phi} \\ -->
<!-- \end{pmatrix}^{-1}= \begin{pmatrix} -->
<!-- 0.098 & 0.011 & 0 \\  -->
<!-- 0.011 & 0.098 & 0 \\  -->
<!-- 0 & 0 & 0.1 \\  -->
<!--   \end{pmatrix} -->
<!-- $$ -->
<!-- ```{r} -->
<!-- set.seed(1234) -->
<!-- XtX <- crossprod(matrix(rnorm(20), 10, 2)) -->
<!-- XtX <- rbind(cbind(XtX, 0),0) -->
<!-- XtX[3, 3] <- 10 -->
<!-- # xtable::xtable(XtX, digits = 2) -->
<!-- # xtable::xtable(solve(XtX), digits = 3) -->
<!-- ``` -->
</div>
</section></section>
<section>
<section id="irls-algorithm" class="title-slide slide level1 center">
<h1>IRLS algorithm</h1>

</section>
<section id="numerical-methods-for-maximum-likelihood-estimation" class="slide level2 center">
<h2>Numerical methods for maximum likelihood estimation</h2>
<ul>
<li><p>In general, the estimating equations of a GLM <span class="math display">
\bm{D}^T \bm{V}^{-1}(\bm{y} - \bm{\mu}) = \bm{0}
</span> cannot be solved in <span class="blue">closed form</span> and we need to rely on <span class="orange">numerical methods</span>.</p></li>
<li><p>An <span class="orange">iterative</span> method means that we start the <span class="blue">algorithm</span> with a candidate value <span class="math inline">\beta^{(1)}</span> (<span class="grey">initialization</span>). Then, at the step <span class="math inline">t</span> we update <span class="math display">
\beta^{(t+1)} = \texttt{update}(\beta^{(t)}), \qquad t=1,2,\dots
</span></p></li>
<li><p>The algorithm <span class="orange">stops</span> whenever a certain criteria is met, e.g.&nbsp;when <span class="math inline">||\beta^{(t+1)} - \beta^{(t)}|| &lt; \epsilon</span>, where <span class="math inline">\epsilon</span> is sometimes called <span class="grey">tolerance</span>. We say it reached <span class="blue">convergence</span>.</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>The <span class="blue">iteratively re-weighted least squares</span> (IRLS) algorithm became very popular after being proposed by <span class="citation" data-cites="Nelder1972">Nelder and Wedderburn (<a href="#/references" role="doc-biblioref" onclick="">1972</a>)</span> and is currently implemented in <strong>R</strong>.</p></li>
<li><p>The IRLS algorithm can be used for any GLM, has a clear geometric interpretation, and often delivers good performance. It can be seen as a <span class="orange">variant</span> of <span class="blue">Newton-Raphson</span>.</p></li>
</ul>
</div>
</section>
<section id="newton-raphson-algorithm-i" class="slide level2 center">
<h2>Newton-Raphson algorithm I</h2>
<ul>
<li><p>In the Newton-Raphson algorithm, we consider a second-order <span class="blue">Taylor expansion</span> of the log-likelihood <span class="math inline">\ell(\beta) = \ell(\beta,\phi)</span> centered in <span class="math inline">\beta^{(t)}</span>, namely: <span class="math display">
\ell(\beta) \approx \ell(\beta^{(t)}) + \ell_*(\beta^{(t)})^T(\beta - \beta^{(t)}) - \frac{1}{2}(\beta -
\beta^{(t)})^T\bm{J}^{(t)}
</span> where <span class="math inline">\ell_*(\beta^{(t)})</span> is the <span class="orange">score function</span> and <span class="math inline">\bm{J}^{(t)}</span> is the <span class="blue">observed information</span>, evaluated at <span class="math inline">\beta^{(t)}</span>.</p></li>
<li><p>In other words, we <span class="orange">approximate</span> the log-likelihood <span class="math inline">\ell(\beta)</span> with a <span class="blue">parabola</span>. This gives the <span class="grey">approximate likelihood equations</span>: <span class="math display">
\ell_*(\beta^{(t)}) - \bm{J}^{(t)}(\beta - \beta^{(t)}) = \bm{0}.
</span></p></li>
<li><p>Solving the equation above gives the following <span class="orange">updates</span>: <span class="math display">
\beta^{(t+1)} = \hat{\beta}^{(t)} + (\bm{J}^{(t)})^{-1}\ell_*(\beta^{(t)}), \qquad t=1,2,\dots
</span></p></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The Netwon-Raphson method essentially considers a series of <span class="orange">parabolic approximations</span> to the log-likelihood, each time evaluating the <span class="blue">point of maximum</span>.</p>
</div>
</div>
</div>
</section>
<section id="newton-raphson-algorithm-ii" class="slide level2 center">
<h2>Newton-Raphson algorithm II</h2>

<img data-src="img/NR.png" class="quarto-figure quarto-figure-center r-stretch" style="width:5in"><ul>
<li>Figure taken from <span class="citation" data-cites="Agresti2015">Agresti (<a href="#/references" role="doc-biblioref" onclick="">2015</a>)</span>.</li>
</ul>
</section>
<section id="iteratively-re-weighted-least-squares-i" class="slide level2 center">
<h2>Iteratively re-weighted least squares I üìñ</h2>
<ul>
<li>The matrix <span class="math inline">\bm{J}^(t)</span> is not always invertible, therefore the algorithm may crash. To remedy this, we replace it with the <span class="blue">expected information</span> <span class="math inline">\bm{I}^{(t)}</span>.</li>
</ul>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>In the <span class="blue">iteratively re-weighted least squares</span> (IRLS) algorithm, we consider the updates: <span class="math display">
\beta^{(t+1)} = \hat{\beta}^{(t)} + (\bm{I}^{(t)})^{-1}\ell_*(\beta^{(t)}), \qquad t=1,2,\dots
</span> This method is also called <span class="orange">Fisher scoring</span>.</p>
</div>
</div>
</div>
<ul>
<li>The above formula can <span class="blue">simplified</span> a bit. First, we rewrite the <span class="orange">score</span> as <span class="math display">
\frac{\partial}{\partial \beta_r}\ell(\beta; \phi)  = \frac{1}{\phi}\sum_{i=1}^n \omega_i \frac{(y_i - \mu_i)}{v(\mu_i)}\frac{x_{ir}}{g'(\mu_i)} = \sum_{i=1}^nx_{ir} w_i (y_i - \mu_i)g'(\mu_i),
</span> where the <span class="orange">weights</span> were defined as <span class="math inline">w_i = \omega_i / (\phi v(\mu_i) g'(\mu_i)^2)</span>. In <span class="blue">matrix notation</span> we will write: <span class="math display">
\ell_*(\beta^{(t)}) = \bm{X}^T\bm{W}^{(t)}\bm{u}^{(t)}, \qquad \bm{I}^{(t)} = \bm{X}^T\bm{W}^{(t)}\bm{X},
</span> where <span class="math inline">\bm{u}^{(t)} =(u_1^{t},\dots,u_n^{(t)})^T</span> and <span class="math inline">u_i^{(t)} = (y_i - \mu_i^{(t)})g'(\mu_i^{(t)})</span> for <span class="math inline">i=1,\dots,n</span>.</li>
</ul>
</section>
<section id="iteratively-re-weighted-least-squares-ii" class="slide level2 center">
<h2>Iteratively re-weighted least squares II üìñ</h2>
<ul>
<li>Exploiting the former formulas, we can write the IRLS update as follows <span class="math display">
\beta^{(t+1)} = \beta^{(t)} + (\bm{X}^T\bm{W}^{(t)}\bm{X})^{-1}\bm{X}^T\bm{W}^{(t)}\bm{u}^{(t)}.
</span> Now <span class="orange">multiply</span> both sides by <span class="math inline">(\bm{X}^T\bm{W}^{(t)}\bm{X})</span>, <span class="blue">simplify</span> and <span class="grey">re-arrange</span> the resulting terms. This gives the following formula.</li>
</ul>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>In the <span class="blue">iteratively re-weighted least squares</span> (IRLS) algorithm, we consider the updates: <span class="math display">
\beta^{(t+1)} = (\bm{X}^T\bm{W}^{(t)}\bm{X})^{-1}\bm{X}^T\bm{W}^{(t)}\bm{z}^{(t)}, \qquad t=1,2,\dots,
</span> where <span class="math inline">\bm{z}^{(t)} = (z_1^{(t)}, \dots, z_n^{(t)})</span> is called <span class="orange">pseudo-response</span> whose elements are defined as <span class="math display">
z_i^{(t)} = \bm{x}_i^T\hat{\beta}^{(t)} + u_i^{(t)}  =  \bm{x}_i^T\hat{\beta}^{(t)} + (y_i - \mu_i^{(t)})g'(\mu_i^{(t)}),\qquad i=1,\dots,n.
</span> Hence, each update can be interpreted as the solution of a <span class="grey">weighted least square</span> problem: <span class="math display">
\hat{\beta}^{(t+1)} = \arg\min_{\beta \in \mathbb{R}^p} \: (\bm{z}^{(t)} - \bm{X}\beta)^T\bm{W}^{(t)}(\bm{z}^{(t)} - \bm{X}\beta).
</span></p>
</div>
</div>
</div>
</section>
<section id="iteratively-re-weighted-least-squares-iii" class="slide level2 center">
<h2>Iteratively re-weighted least squares III üìñ</h2>
<ul>
<li>The IRLS updates does not depend on the choice of <span class="math inline">\phi</span>, because it <span class="orange">cancels</span> in the multiplications, as we would expect.</li>
</ul>
<div class="fragment">
<ul>
<li><p>The pseudo-responses have a <span class="blue">nice interpretation</span>, because they can be interpreted as a linear approximation of the transformed responses: <span class="math display">
g(y_i) \approx g(\mu_i) + (y_i - \mu_i)g'(\mu_i) = \eta_i + (y_i - \mu_i)g'(\mu_i) = z_i.
</span></p></li>
<li><p>Based on this approximation, a good <span class="blue">initialization</span> is <span class="math display">
\bm{W}^{(1)} = I_n, \qquad z_i^{(1)} = g(y_i), \qquad \Longrightarrow \qquad \hat{\beta}^{(2)} = (\bm{X}^T\bm{X})^{-1}\bm{X}^Tg(\bm{y}),
</span> the <span class="orange">least square</span> solution for the transformed data. To avoid boundary issues, sometimes the data are perturbed, as we did in Binomial regression.</p></li>
</ul>
</div>
</section>
<section id="example-irls-for-logistic-regression" class="slide level2 center">
<h2>Example: IRLS for logistic regression</h2>
<ul>
<li>Consider a <span class="blue">logistic regression</span> model for <span class="orange">proportions</span> <span class="math inline">Y_i \in \{0, 1/m_i, \dots,1\}</span> with probability of success <span class="math inline">\pi_i = \mu_i</span> and trials <span class="math inline">m_i</span>.</li>
</ul>
<div class="callout callout-tip no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>IRLS algorithm for logistic regression</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p><span class="grey">Initialize</span> <span class="math inline">\bm{\beta}^{(1)} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\text{logit}(\tilde{\bm{y}})</span> where <span class="math inline">\text{logit}(\tilde{\bm{y}})</span> is the <span class="orange">empirical logit</span> transform.</p></li>
<li><p>For <span class="math inline">t=1,2, \dots</span> until <span class="orange">convergence</span> compute: <span class="math display">
\beta^{(t+1)} = (\bm{X}^T\bm{W}^{(t)}\bm{X})^{-1}\bm{X}^T\bm{W}^{(t)}\bm{z}^{(t)},
</span> where the weights in <span class="math inline">\bm{W}^{(t)}</span> equals <span class="math inline">w_i^{(t)} = m_i \pi_i^{(t)} (1 - \pi_i^{(t)})</span> and the <span class="blue">pseudo-responses</span> <span class="math inline">\bm{z}^{(t)}</span> are <span class="math display">
z_i^{(t)} = \bm{x}_i^T\hat{\beta}^{(t)} + \frac{y_i - \pi_i^{(t)}}{ \pi_i^{(t)}(1-\pi_i^{(t)})}, \qquad i=1,\dots,n,
</span> with probabilities <span class="math inline">\pi_i^{(t)} = \exp(\bm{x}_i^T\beta^{(t)})/(1 + \exp(\bm{x}_i^T\beta^{(t)}))</span> for <span class="math inline">i=1,\dots,n</span>.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="estimation-of-the-dispersion-phi" class="slide level2 center">
<h2>Estimation of the dispersion <span class="math inline">\phi</span></h2>
<ul>
<li><p>In some GLMs, such as the Gaussian and the Gamma, there is a <span class="orange">dispersion parameter</span> <span class="math inline">\phi</span> that we need to estimate.</p></li>
<li><p>Instead of the maximum likelihood, because of <span class="orange">numerical instabilities</span> and <span class="blue">lack of robustness</span> it is typically preferred a <span class="orange">method of moments</span> estimator. If <span class="math inline">\mu_i</span> were known, the estimator <span class="math display">
\frac{1}{n}\sum_{i=1}^n\omega_i\frac{(y_i - \mu_i)^2}{v(\mu_i)}
</span> would be <span class="orange">unbiased</span> for <span class="math inline">\phi</span>, because <span class="math inline">\mathbb{E}\{(Y_i - \mu_i)^2\} = (\phi/\omega_i) v(\mu_i)</span>. This motivates the estimator <span class="math display">
\hat{\phi} = \frac{1}{n - p}\sum_{i=1}^n \omega_i\frac{(y_i - \hat{\mu}_i)^2}{v(\hat{\mu}_i)}, \qquad \hat{\mu}_i = g^{-1}(\bm{x}_i^T\hat{\beta}).
</span></p></li>
<li><p>This is a <span class="blue">consistent</span> estimator of <span class="math inline">\phi</span> as long as <span class="math inline">\hat{\beta}</span> is consistent.</p></li>
</ul>
<div class="fragment">
<ul>
<li>When <span class="math inline">g(\mu_i) = \mu_i</span> is the <span class="blue">identity link</span> and <span class="math inline">v(\mu_i) = \omega_i = 1</span>, this coincides with the usual unbiased estimator <span class="math inline">s^2</span> of <span class="math inline">\sigma^2</span> for a Gaussian linear model.</li>
</ul>
</div>
</section></section>
<section>
<section id="inference-and-hypothesis-testing" class="title-slide slide level1 center">
<h1>Inference and hypothesis testing</h1>

</section>
<section id="asymptotic-distribution-of-hatbeta" class="slide level2 center">
<h2>Asymptotic distribution of <span class="math inline">\hat{\beta}</span></h2>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The <span class="orange">asymptotic distribution</span> of the <span class="blue">maximum likelihood estimator</span> is <span class="math display">
\hat{\beta} \, \dot{\sim} \, \text{N}_p\left(\beta, (\bm{X}^T\bm{W}\bm{X})^{-1}\right),
</span> for large values of <span class="math inline">n</span> and under mild regularity conditions on <span class="math inline">\bm{X}</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>Under correct specification and mild conditions on <span class="math inline">\bm{X}</span>, the maximum likelihood estimator is <span class="orange">asymptotically unbiased</span> and with known asymptotic <span class="blue">variance</span> <span class="math display">
\mathbb{E}(\hat{\beta}) \approx 0, \qquad \text{var}(\hat{\beta}) \approx (\bm{X}^T\bm{W}\bm{X})^{-1}.
</span></p></li>
<li><p>In practice, since <span class="math inline">\bm{W}</span> depends on <span class="math inline">\beta</span> and <span class="math inline">\phi</span>, we rely on the following approximation <span class="math display">
\widehat{\text{var}}(\hat{\beta}) = (\bm{X}^T\hat{\bm{W}}\bm{X})^{-1},
</span> where we plugged in the estimates <span class="math inline">\hat{\beta}</span> and <span class="math inline">\hat{\phi}</span> into <span class="math inline">\bm{W}</span> obtaining <span class="math inline">\hat{\bm{W}}</span>. The <span class="orange">standard errors</span> are: <span class="math display">
\texttt{Std. Error} = [\widehat{\text{se}}(\hat{\beta})]_j = \sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}}
</span></p></li>
</ul>
<!-- - Note that the asymptotic variance implicitly takes into account [heteroschedasticity]{.orange}.  -->
</section>
<section id="example-beetles-data-2" class="slide level2 center">
<h2>Example: <code>Beetles</code> data</h2>
<ul>
<li><p>Using the <code>Beetles</code> data, we specified a <span class="blue">binomial logistic</span> regression model for the counts <span class="math inline">m_i Y_i \sim \text{Binomial}(m_i, \pi_i)</span> with mean <span class="math inline">\mu_i = \exp(\beta_1 + \beta_2 x_i)/(1 + \exp(\beta_1 + \beta_2 x_i))</span>.</p></li>
<li><p>We previously <span class="orange">estimated</span> <span class="math inline">\hat{\beta} = (-60.717, 34.270)</span>. This means that the <span class="blue">weights</span> are estimated as <span class="math display">
\hat{\bm{W}} =\text{diag}(m_1\hat{\mu}_1(1 - \hat{\mu}_1),\dots,m_n \hat{\mu}_n(1 - \hat{\mu}_n)) = \text{diag}(3.255, 8.227, \dots, 1.231).
</span> from which we obtain the <span class="orange">estimated</span> <span class="blue">Fisher information matrix</span>: <span class="math display">
\bm{X}^T\hat{\bm{W}}\bm{X} = \begin{pmatrix}
\sum_{i=1}^nm_i\hat{\mu}_i(1 - \hat{\mu}_i) &amp; \sum_{i=1}^n x_im_i\hat{\mu}_i(1 - \hat{\mu}_i)\\
\sum_{i=1}^n x_im_i\hat{\mu}_i(1 - \hat{\mu}_i) &amp; \sum_{i=1}^n x_i^2m_i\hat{\mu}_i(1 - \hat{\mu}_i)
\end{pmatrix} = \begin{pmatrix}
58.484 &amp; 104.011\\
104.011 &amp; 185.095
\end{pmatrix}.
</span></p></li>
<li><p>Hence, the <span class="blue">estimated covariance</span> matrix of the maximum likelihood estimator is <span class="math display">
\widehat{\text{var}}(\hat{\beta}) = (\bm{X}^T\hat{\bm{W}}\bm{X})^{-1} = \begin{pmatrix}
26.840 &amp; -15.082 \\
-15.082 &amp; 8.481
\end{pmatrix}.
</span></p></li>
<li><p>Therefore the <span class="orange">estimated standard errors</span> are <span class="math display">
[\widehat{\text{se}}(\hat{\beta})]_j = \sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}} \quad\Longrightarrow \quad \widehat{\text{se}}(\hat{\beta}) = (5.181, 2.912).
</span></p></li>
</ul>
</section>
<section id="example-aids-data-2" class="slide level2 center">
<h2>Example: <code>Aids</code> data</h2>
<ul>
<li><p>In the <code>Aids</code> data, we specified a <span class="blue">Poisson</span> regression model with <span class="math inline">\mathbb{E}(Y_i) = \exp(\beta_1 + \beta_2 x_i)</span> and estimated <span class="math inline">\hat{\beta} = (0.304, 0.259)</span>.</p></li>
<li><p>This means that the weights are estimated as <span class="math display">
\hat{\bm{W}} = \text{diag}(\hat{\mu}_1,\dots,\hat{\mu}_n) = \text{diag}(1.755, \dots, 50.863).
</span> from which we obtain the <span class="orange">estimated</span> <span class="blue">Fisher information matrix</span>: <span class="math display">
\bm{X}^T\hat{\bm{W}}\bm{X} = \begin{pmatrix}
\sum_{i=1}^n\hat{\mu}_i &amp; \sum_{i=1}^n x_i\hat{\mu}_i\\
\sum_{i=1}^n x_i\hat{\mu}_i &amp; \sum_{i=1}^n x_i^2\hat{\mu}_i
\end{pmatrix} = \begin{pmatrix}
217 &amp; 2387\\
2387 &amp; 28279.05
\end{pmatrix}.
</span></p></li>
<li><p>Hence, the <span class="blue">estimated covariance</span> matrix of the maximum likelihood estimator is <span class="math display">
\widehat{\text{var}}(\hat{\beta}) = (\bm{X}^T\hat{\bm{W}}\bm{X})^{-1} = \begin{pmatrix}
0.06445 &amp; -0.00544 \\
-0.00544 &amp; 0.00049
\end{pmatrix}.
</span></p></li>
<li><p>Therefore the <span class="orange">estimated standard errors</span> are <span class="math display">
[\widehat{\text{se}}(\hat{\beta})]_j = \sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}} \quad\Longrightarrow \quad \widehat{\text{se}}(\hat{\beta}) = (0.254, 0.022).
</span></p></li>
</ul>
</section>
<section id="wald-test-and-confidence-intervals" class="slide level2 center">
<h2>Wald test and confidence intervals</h2>
<ul>
<li><p>Consider the <span class="blue">hypothesis</span> <span class="math inline">H_0: \beta_j = \beta_0</span> against the <span class="orange">alternative</span> <span class="math inline">H_1: \beta_j \neq \beta_0</span>. The <span class="blue">Wald test statistic</span> <span class="math inline">z_j</span>, rejecting the hypothesis for large values of <span class="math inline">|z_j|</span> is: <span class="math display">
\texttt{z value} = z_j = \frac{\hat{\beta}_j - \beta_0}{[\widehat{\text{se}}(\hat{\beta})]_j} = \frac{\hat{\beta}_j - \beta_0}{\sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}}} \, \dot{\sim}\,\text{N}(0, 1).
</span> which is <span class="orange">approximately</span> distributed as a <span class="blue">standard normal</span> under <span class="math inline">H_0</span>.</p></li>
<li><p>The <span class="blue">p-value</span> is defined in the usual way, namely <span class="math display">
\alpha_\text{obs} = \mathbb{P}(Z \ge |z_j|) = 2 (1 - \Phi(|z_j|)), \qquad Z \sim \text{N}(0, 1).
</span></p></li>
</ul>
<div class="fragment">
<ul>
<li>By inverting the the Wald test, we obtain the associated <span class="blue">confidence interval</span> <span class="math display">
\hat{\beta}_j \pm z_{1 - \alpha/2} \sqrt{[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{jj}}.
</span> of <span class="orange">approximate</span> level <span class="math inline">1-\alpha</span>, where <span class="math inline">z_{1-\alpha/2}</span> is the quantile of a standard Gaussian.</li>
</ul>
</div>
</section>
<section id="comparison-with-the-gaussian-linear-model" class="slide level2 center">
<h2>Comparison with the Gaussian linear model</h2>
<ul>
<li><p>In a classical <span class="blue">Gaussian linear model</span> the weight matrix is <span class="math inline">\bm{W} = \sigma^2 I_n</span>, therefore the distribution of the maximum likelihood is <span class="math display">
\hat{\beta} \sim \text{N}_p\left(\beta, \sigma^2(\bm{X}^T\bm{X})^{-1}\right),
</span> an important result we have encountered multiple times in this course.</p></li>
<li><p>The <span class="blue">Wald statistic</span> <span class="math inline">z_j</span> <span class="orange">specializes</span> to <span class="math display">
z_j = \frac{\hat{\beta}_j - \beta_0}{[\widehat{\text{se}}(\hat{\beta})]_j} = \frac{\hat{\beta}_j - \beta_0}{s \sqrt{[(\bm{X}^T\bm{X})^{-1}]_{jj}}},
</span> which is the usual test statistic considered, e.g., in the output of <code>lm</code> in <strong>R</strong>.</p></li>
<li><p>However, in the Gaussian case there is <span class="orange">no need</span> of <span class="orange">approximations</span>. The distribution of <span class="math inline">z_j</span> is a <span class="grey">Student</span>‚Äô <span class="math inline">t_{n-p}</span> under <span class="math inline">H_0</span>, which indeed converges to a <span class="math inline">\text{N}(0, 1)</span> for large values of <span class="math inline">n</span>.</p></li>
</ul>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>In GLMs we use procedures that are <span class="orange">approximate</span> rather than <span class="blue">exact</span>. Of course, whenever an exact result is known, we should use it.</p>
</div>
</div>
</div>
</section>
<section id="example-beetles-data-3" class="slide level2 center">
<h2>Example: <code>Beetles</code> data</h2>
<ul>
<li>The Wald test is the <span class="orange">default</span> choice in <strong>R</strong> for checking the hypotheses <span class="math inline">H_0 : \beta_j = 0</span>. In the <code>Beetles</code> data we get the following familiar summary:</li>
</ul>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>
z test of coefficients:

            Estimate Std. Error z value  Pr(&gt;|z|)    
(Intercept) -60.7175     5.1807 -11.720 &lt; 2.2e-16 ***
logdose      34.2703     2.9121  11.768 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<ul>
<li><p>Many of the above quantities (estimates and standard errors) have been obtained before.</p></li>
<li><p>In this case, we <span class="orange">reject</span> the null hypothesis that <span class="math inline">\beta_2 = 0</span>. Indeed, even from the scatterplot there was evidence of a relationship between the <code>deaths</code> proportion and the <code>logdose</code>.</p></li>
</ul>
<div class="fragment">
<ul>
<li>For completeness, we also compute the associated <span class="blue">Wald confidence intervals</span>, which are:</li>
</ul>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>                2.5 %    97.5 %
(Intercept) -70.87144 -50.56347
logdose      28.56265  39.97800</code></pre>
</div>
</div>
</div>
</section>
<section id="example-aids-data-3" class="slide level2 center">
<h2>Example: <code>Aids</code> data</h2>
<ul>
<li>The Wald tests for checking the hypotheses <span class="math inline">H_0 : \beta_j = 0</span> in the <code>Aids</code> data are provided below.</li>
</ul>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>
z test of coefficients:

            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 0.303655   0.253867  1.1961   0.2317    
period      0.258963   0.022238 11.6448   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<ul>
<li>In this case, we <span class="orange">reject</span> the null hypothesis that <span class="math inline">\beta_2 = 0</span> because the <span class="blue">p-value</span> <span class="math inline">\texttt{Pr(&gt;|z|)} \approx 0</span>. Again, this is not very surprising: the number of <code>deaths</code> was clearly increasing over time.</li>
</ul>
<div class="fragment">
<ul>
<li>The associated <span class="blue">Wald confidence intervals</span> are:</li>
</ul>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>                 2.5 %    97.5 %
(Intercept) -0.1939158 0.8012249
period       0.2153764 0.3025494</code></pre>
</div>
</div>
</div>
</section>
<section id="general-hypothesis-testing" class="slide level2 center">
<h2>General hypothesis testing</h2>
<ul>
<li>Suppose we wish to test <span class="blue">multiple parameters</span> at the same time. Let us organize the parameters into <span class="blue">two blocks</span>: <span class="math display">
\beta = \begin{pmatrix}\beta_A \\ \beta_B \end{pmatrix}, \qquad \beta_A = \begin{pmatrix}\beta_1 \\ \vdots \\ \beta_{p_0} \end{pmatrix}, \quad \beta_B = \begin{pmatrix}\beta_{p_0+1} \\ \vdots \\ \beta_p \end{pmatrix},
</span> where <span class="math inline">q = p - p_0</span> is the number of <span class="orange">constrained parameters</span>. We want to <span class="blue">test the hypothesis</span>: <span class="math display">
H_0: \beta_B = \beta_0 \qquad \text{against}\qquad H_1: \beta_B \neq \beta_0.</span></li>
</ul>
<div class="fragment">
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>A common case is <span class="math inline">H_0: \beta_B = 0</span> (<span class="grey">nested models</span>), where we compare the <span class="orange">reduced model</span> <span class="math inline">M_0</span> against the <span class="blue">full model</span> <span class="math inline">M_1</span>. We verify if <span class="orange">all</span> the <span class="math inline">q</span> variables associated with <span class="math inline">\beta_B</span> can be omitted.</p>
<p>The case <span class="math inline">q = 1</span>, that is <span class="math inline">\beta_B = \beta_p</span> with <span class="math inline">H_0: \beta_p = 0</span> corresponds to the previously considered situation where we test if a <span class="orange">specific coefficient</span>, say <span class="math inline">\beta_p</span>, is non-zero.</p>
</div>
</div>
</div>
</div>
</section>
<section id="testing-hypothesis-in-glms-i" class="slide level2 center">
<h2>Testing hypothesis in GLMs I</h2>
<ul>
<li><p>There are <span class="orange">three classical tests</span> that we could consider for such a testing problem: the <span class="orange">Wald test</span> <span class="math inline">W_e</span>, the <span class="grey">Rao-score test</span> <span class="math inline">W_u</span>, and the <span class="blue">log-likelihood ratio test</span> <span class="math inline">W</span>.</p></li>
<li><p>All these tests <span class="orange">reject</span> the null hypothesis for <span class="blue">large values</span> of the <span class="blue">statistic</span>.</p></li>
</ul>
<div class="callout callout-note no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Wald test (general case)</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\hat{\beta} = (\hat{\beta}_A, \hat{\beta}_B)</span> be the <span class="blue">unrestricted</span> maximum likelihood, the quantity <span class="math display">
W_e = (\hat{\beta}_B - \beta_0)^T \:\widehat{\text{var}}(\hat{\beta}_B)^{-1} \:(\hat{\beta}_B - \beta_0),
</span> is called <span class="blue">Wald test</span>. Here <span class="math inline">\widehat{\text{var}}(\hat{\beta}_B)</span> is the appropriate <span class="grey">block</span> of <span class="math inline">(\bm{X}\hat{\bm{W}}\bm{X})^{-1}</span> and <span class="math inline">\hat{\bm{W}}</span> is estimated using <span class="math inline">\hat{\beta}</span> and <span class="math inline">\hat{\phi}</span>. Under <span class="math inline">H_0</span> this quantity is <span class="orange">approximately</span> distributed as <span class="math display">
W_e \: \dot{\sim} \; \chi^2_q,
</span> a <span class="math inline">\chi^2</span> distribution with <span class="math inline">q</span> <span class="blue">degrees of freedom</span>. The <span class="blue">p-value</span> is <span class="math inline">\texttt{Pr(&gt;Chi)} = \mathbb{P}(W_e &gt; w_\text{e, obs})</span>.</p>
</div>
</div>
</div>
<ul>
<li>Clearly, in the <span class="math inline">q = 1</span> case we recover the Wald statistic with <span class="math inline">z_j^2 = W_e</span>.</li>
</ul>
<!-- - The Wald statistic is the [simplest]{.blue}, but it has some [drawbacks]{.orange}. For example, it depends on the parametrization and it is sometimes problematic for small $n$. -->
</section>
<section id="log-likelihood-ratio-test" class="slide level2 center">
<h2>Log-likelihood ratio test</h2>
<div class="callout callout-note no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Log-likelihood ratio test (LRT)</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\hat{\beta} = (\hat{\beta}_A, \hat{\beta}_B)</span> be the <span class="blue">unrestricted</span> maximum likelihood and let <span class="math inline">\hat{\beta}_0 = (\hat{\beta}_{A,0}, \textcolor{red}{\beta_0})</span> the <span class="orange">restricted</span> maximum likelihood estimate. The quantity <span class="math display">
W = 2 [\ell(\hat{\beta}; \hat{\phi}) - \ell(\hat{\beta}_0; \hat{\phi})],
</span> is called <span class="blue">log-likelihood ratio</span> test (LRT). Under <span class="math inline">H_0</span> this quantity is <span class="orange">approximately</span> distributed as <span class="math display">
W \: \dot{\sim} \; \chi^2_q,
</span> a <span class="math inline">\chi^2</span> distribution with <span class="math inline">q</span> <span class="blue">degrees of freedom</span>. The <span class="blue">p-value</span> is <span class="math inline">\texttt{Pr(&gt;Chi)} =\mathbb{P}(W &gt; w_\text{obs})</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>When testing <span class="math inline">H_0 : \beta_B = 0</span>, we separately fit the <span class="blue">full model</span>, obtaining <span class="math inline">\hat{\beta}</span>, and the <span class="orange">reduced model</span>, obtaining <span class="math inline">\hat{\beta}_0 = (\hat{\beta}_{A,0}, 0)</span>. Then, we compare their log-likelihoods: <span class="math inline">\ell(\hat{\beta}; \hat{\phi}) - \ell(\hat{\beta}_0; \hat{\phi})</span>.</p></li>
<li><p>The LRT is the <span class="orange">default</span> in <strong>R</strong> for comparing <span class="blue">nested models</span>.</p></li>
</ul>

<aside><div>
<p>When the dispersion parameter <span class="math inline">\phi</span> is unknown, a <span class="orange">variant</span> uses <span class="blue">separate estimates</span> <span class="math inline">\hat{\phi}</span>, based on <span class="math inline">\hat{\beta}</span>, and <span class="math inline">\hat{\phi}_0</span>, based on <span class="math inline">\hat{\beta}_0</span>. The <code>anova</code> <strong>R</strong> command uses a single <span class="math inline">\hat{\phi}</span>, as described above.</p>
</div></aside></section>
<section id="score-or-rao-test" class="slide level2 center">
<h2>Score or Rao test</h2>
<div class="callout callout-note no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Rao-score test</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\hat{\beta} = (\hat{\beta}_A, \hat{\beta}_B)</span> be the <span class="blue">unrestricted</span> maximum likelihood and let <span class="math inline">\hat{\beta}_0 = (\hat{\beta}_{A,0}, \textcolor{red}{\beta_0})</span> the <span class="orange">restricted</span> maximum likelihood estimate. Moreover, let <span class="math display">
\ell_B(\beta;\phi) = \frac{\partial}{\partial \beta_B}\ell(\beta, \phi),
</span> namely the <span class="grey">block</span> of the score function associated with <span class="math inline">\beta_B</span>. The quantity <span class="math display">
W_u = \ell_B(\hat{\beta}_0; \hat{\phi})^T\: \widetilde{\text{var}}(\hat{\beta}_B) \: \ell_B(\hat{\beta}_0; \hat{\phi}),
</span> is called <span class="blue">Rao-score test</span>. Here <span class="math inline">\widetilde{\text{var}}(\hat{\beta}_B)</span> is the appropriate <span class="grey">block</span> of <span class="math inline">(\bm{X}\tilde{\bm{W}}\bm{X})^{-1}</span> where <span class="math inline">\tilde{\bm{W}}</span> is estimated using the <span class="orange">restricted</span> <span class="math inline">\hat{\beta}_0</span>. Under <span class="math inline">H_0</span> this quantity is <span class="orange">approximately</span> distributed as <span class="math display">
W_u \: \dot{\sim} \; \chi^2_q,
</span> a <span class="math inline">\chi^2</span> distribution with <span class="math inline">q</span> <span class="blue">degrees of freedom</span>. The <span class="blue">p-value</span> is <span class="math inline">\texttt{Pr(&gt;Chi)} =\mathbb{P}(W_u &gt; w_\text{u, obs})</span>.</p>
</div>
</div>
</div>

<aside><div>
<p>The Rao-score test arguably the <span class="orange">less common</span>. However, it is linked to the <span class="math inline">X^2</span> Pearson statistic. When <span class="math inline">\phi</span> is unknown, there are several variants depending on how it is estimated.</p>
</div></aside></section>
<section id="a-graphical-representation-when-p-1" class="slide level2 center">
<h2>A graphical representation when <span class="math inline">p = 1</span></h2>

<img data-src="img/tests.png" class="quarto-figure quarto-figure-center r-stretch" style="width:4in"><ul>
<li>Figure taken from Azzalini (1996). This is also the <span class="grey">cover</span> of the <span class="orange">book</span>!</li>
</ul>
</section>
<section id="three-asymptotically-equivalent-tests" class="slide level2 center">
<h2>Three asymptotically equivalent tests</h2>
<div class="callout callout-warning no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The Wald test, the Score test and the log-likelihood ration test are <span class="blue">asymptotically equivalent</span>, that is, these tests give the same <span class="blue">same number</span> for large values of <span class="math inline">n</span>. We have that <span class="math display">
W_e = W + o_p(1), \qquad W_u = W + o_p(1),
</span> where <span class="math inline">o_p(1)</span> is a quantity that goes to <span class="math inline">0</span> in probability as <span class="math inline">n \to \infty</span>.</p>
</div>
</div>
</div>
<div class="fragment">
<ul>
<li>When <span class="math inline">q = 1</span>, we can also <span class="blue">invert</span> <span class="math inline">W_e</span>, <span class="math inline">W_u</span> and <span class="math inline">W</span> tests over <span class="math inline">\beta_0</span> to obtain the corresponding <span class="orange">confidence interval</span>. This is often done <span class="orange">numerically</span> for <span class="math inline">W_u</span> and <span class="math inline">W</span>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>The Wald test depends on the parametrization. When considering a transformation of <span class="math inline">\beta</span>, the variance must be adjusted using the derivative of the transformation (<span class="blue">delta method</span>).<sup>1</sup></p></li>
<li><p>On the other hand, both the LRT and the score are <span class="blue">invariant</span>, and therefore we can simply <span class="orange">transform</span> the <span class="orange">extremes</span> of the <span class="orange">original interval</span> without further corrections.</p></li>
</ul>
</div>
<aside><ol class="aside-footnotes"><li id="fn1"><p>Transforming the extremes of Wald confidence interval ‚Äúworks‚Äù in the sense that it produces a valid confidence interval, but it is <span class="orange">not</span> the Wald interval in the trasformed scale.</p></li></ol></aside></section>
<section id="comparison-with-the-gaussian-linear-model-1" class="slide level2 center">
<h2>Comparison with the Gaussian linear model</h2>
<div class="callout callout-warning no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>In the Gaussian linear model all tests are <span class="blue">equivalent</span> if <span class="math inline">\phi = \sigma^2</span> is <span class="orange">known</span>. We have <span class="math display">
W = W_e = W_u =  \frac{||\bm{Y} - \bm{X}\hat{\beta}_0||^2 - ||\bm{Y} - \bm{X}\hat{\beta}||^2}{\sigma^2} \sim \chi^2_q.
</span> The <span class="math inline">\chi^2_q</span> distribution is <span class="orange">exact</span> and not an approximation thanks to Cochran theorem.</p>
</div>
</div>
</div>
<div class="fragment">
<ul>
<li>Consider the <span class="blue">log-likelihood ratio</span> for testing <span class="math inline">H_0: \beta_B = \beta_0</span>. Suppose <span class="math inline">\sigma^2</span> is <span class="orange">unknown</span>, then: <span class="math display">
\begin{aligned}
W &amp;= 2 [\ell(\hat{\beta}; \hat{\phi}) - \ell(\hat{\beta}_0; \hat{\phi})] = \frac{||\bm{Y} - \bm{X}\hat{\beta}_0||^2 - ||\bm{Y} - \bm{X}\hat{\beta}||^2}{\hat{\phi}} = q \frac{(||\bm{Y} - \bm{X}\hat{\beta}_0||^2 - ||\bm{y} - \bm{Y}\hat{\beta}||^2)/q}{||\bm{y} - \bm{X}\hat{\beta}||^2/(n - p)} \\
&amp; = q F,
\end{aligned}
</span> where <span class="math inline">F \sim F_{q, n - p}</span> is the usual <span class="orange">Snedecor</span>‚Äôs F. Indeed <span class="math inline">qF</span> is <span class="orange">approximately</span> distributed as <span class="math inline">\chi^2_q</span> for large values of <span class="math inline">n</span>.</li>
</ul>
<!-- - In Gaussian linear models we should not use the asymptotic approximation, because the [exact distribution]{.orange} is known! -->
</div>
<div class="fragment">
<ul>
<li>The quantities <span class="math inline">W_e, W_u</span>, and <span class="math inline">W</span> are the natural extension of the F-statistic for GLMs. They are <span class="orange">approximately</span> distributed as <span class="math inline">\chi^2_q</span> with <span class="math inline">q</span> <span class="blue">degrees of freedom</span>.</li>
</ul>
</div>
</section>
<section id="example-beetles-data-4" class="slide level2 center">
<h2>Example: <code>Beetles</code> data</h2>
<ul>
<li>We would like to use the <span class="blue">Wald</span>, the <span class="orange">Rao-score</span> and the <span class="grey">log-likelihood ratio</span> tests to verify the hypothesis <span class="math inline">H_0: \beta_2 = 0</span>, that is the relevance of <code>logdose</code> in predicting the response.</li>
</ul>
<div class="fragment">
<ul>
<li>In this case, we have <span class="math inline">q = 1</span> (<span class="math inline">\texttt{Df}</span>) because there is only one parameter under scrutiny.</li>
</ul>
<table class="caption-top" style="width:100%;">
<colgroup>
<col style="width: 61%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Test for the hypothesis <span class="math inline">H_0 : \beta_2 = 0</span></th>
<th style="text-align: right;"><span class="math inline">\texttt{Chi}</span></th>
<th style="text-align: right;"><span class="math inline">\texttt{Df}</span></th>
<th style="text-align: right;"><span class="math inline">\texttt{Pr(&gt;Chi)}</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W_e</span> - <span class="blue">Wald test</span></td>
<td style="text-align: right;">138.488</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;"><span class="math inline">\approx</span> 0</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">W_u</span> - <span class="orange">Rao-score test</span></td>
<td style="text-align: right;">227.580</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;"><span class="math inline">\approx</span> 0</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W</span> - <span class="grey">Log-likelihood ratio test</span></td>
<td style="text-align: right;">272.970</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;"><span class="math inline">\approx</span> 0</td>
</tr>
</tbody>
</table>
<ul>
<li><p>As one may expect, the test values are not identical. Here the sample size is <span class="math inline">n = 8</span>, which is definitely not a big number, therefore we are far from the <span class="blue">asymptotic regime</span>.</p></li>
<li><p>However, the practical conclusions are identical. All tests strongly <span class="orange">reject</span> the null hypothesis.</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>We previously obtained the <span class="blue">Wald statistic</span> <span class="math inline">z_j</span> and indeed <span class="math inline">z_j^2 = 11.76811^2 = 138.488 = W_e</span>.</li>
</ul>
</div>
</section>
<section id="example-beetles-data-5" class="slide level2 center">
<h2>Example: <code>Beetles</code> data</h2>
<ul>
<li>Any statistical test can be <span class="orange">inverted</span>, namely we find all the values <span class="math inline">\beta_0</span> such that we do <span class="blue">not reject the null hypothesis</span>. This generates a <span class="blue">confidence interval</span>.</li>
</ul>
<div class="fragment">
<ul>
<li><p>For the Wald test, the inversion is done analytically, producing the ‚Äúusual‚Äù confidence interval.</p></li>
<li><p>For the Rao-score and the log-likelihood ratio we need numerical procedures.</p></li>
<li><p>In the <code>Beetles</code> data, the three tests produce the following confidence intervals for <span class="math inline">\beta_2</span>, associated to <code>logdose</code>.</p></li>
</ul>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;">Confidence intervals for <span class="math inline">\beta_2</span> at a <span class="math inline">95\%</span> level</th>
<th style="text-align: right;">2.5%</th>
<th style="text-align: right;">97.5%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W_e</span> - <span class="blue">Wald test</span></td>
<td style="text-align: right;">28.563</td>
<td style="text-align: right;">39.978</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">W_u</span> - <span class="orange">Rao-score test</span></td>
<td style="text-align: right;">28.588</td>
<td style="text-align: right;">39.957</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W</span> - <span class="grey">Log-likelihood ratio test</span></td>
<td style="text-align: right;">28.854</td>
<td style="text-align: right;">40.301</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Wald interval was also computed before. The three tests produce <span class="blue">nearly identical</span> intervals.</p></li>
<li><p>Wald is always symmetric around <span class="math inline">\hat{\beta}_j</span>, whereas Rao and the log-likelihood ratio are typically <span class="orange">asymmetric</span>, depending on the shape of the likelihood function.</p></li>
</ul>
</div>
</section>
<section id="example-aids-data-4" class="slide level2 center">
<h2>Example: <code>Aids</code> data</h2>
<ul>
<li>Let us know perform the same analysis for the <code>Aids</code> data. Again, we test the null hypothesis <span class="math inline">H_0 : \beta_2 = 0</span>, which is the relevance of <code>period</code> in predicting the response.</li>
</ul>
<table class="caption-top" style="width:100%;">
<colgroup>
<col style="width: 61%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Test for the hypothesis <span class="math inline">H_0 : \beta_2 = 0</span></th>
<th style="text-align: right;"><span class="math inline">\texttt{Chi}</span></th>
<th style="text-align: right;"><span class="math inline">\texttt{Df}</span></th>
<th style="text-align: right;"><span class="math inline">\texttt{Pr(&gt;Chi)}</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W_e</span> - <span class="blue">Wald test</span></td>
<td style="text-align: right;">135.602</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;"><span class="math inline">\approx</span> 0</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">W_u</span> - <span class="orange">Rao-score test</span></td>
<td style="text-align: right;">163.586</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;"><span class="math inline">\approx</span> 0</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W</span> - <span class="grey">Log-likelihood ratio test</span></td>
<td style="text-align: right;">163.586</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;"><span class="math inline">\approx</span> 0</td>
</tr>
</tbody>
</table>
<ul>
<li>As before, despite their numerical differences, all the tests <span class="orange">reject</span> the null hypothesis. We previously obtained the <span class="blue">Wald statistic</span> <span class="math inline">z_j = 11.645</span> and indeed <span class="math inline">z_j^2 = 11.645^2 = 135.6 = W_e</span>.</li>
</ul>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;">Confidence intervals for <span class="math inline">\beta_2</span> at a <span class="math inline">95\%</span> level</th>
<th style="text-align: right;">2.5%</th>
<th style="text-align: right;">97.5%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W_e</span> - <span class="blue">Wald test</span></td>
<td style="text-align: right;">0.2154</td>
<td style="text-align: right;">0.3025</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">W_u</span> - <span class="orange">Rao-score test</span></td>
<td style="text-align: right;">0.2155</td>
<td style="text-align: right;">0.3025</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W</span> - <span class="grey">Log-likelihood ratio test</span></td>
<td style="text-align: right;">0.2165</td>
<td style="text-align: right;">0.3037</td>
</tr>
</tbody>
</table>
</section>
<section id="example-aids-data-5" class="slide level2 center">
<h2>Example: <code>Aids</code> data</h2>
<ul>
<li><p>We are actually interested in a confidence interval for the quantity <span class="math inline">100\times(\exp(\beta_2) - 1)</span>, which is the <span class="blue">percentage increase</span> of <code>deaths</code> after each period.</p></li>
<li><p>Thanks to <span class="blue">invariance property</span> of the Rao-score and the log-likelihood ratio tests, we can simply <span class="orange">transform</span> the <span class="blue">original intervals</span> for <span class="math inline">\beta_2</span>.</p></li>
<li><p>If the extremes of the log-likelihood ration interval are <span class="math inline">C_\text{low}, C_\text{high}</span>, then the new interval is <span class="math display">
[100\times(\exp(C_\text{low}) - 1), \:100\times(\exp(C_\text{high}) - 1)].
</span> and similarly for the Rao-score case. These are reported below.</p></li>
</ul>
<table class="caption-top">
<colgroup>
<col style="width: 61%">
<col style="width: 19%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Confidence intervals for <span class="math inline">100[\exp(\beta_2)-1]</span> at a <span class="math inline">95\%</span> level</th>
<th style="text-align: right;">2.5%</th>
<th style="text-align: right;">97.5%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W_u</span> - <span class="orange">Rao-score test</span></td>
<td style="text-align: right;">24.04</td>
<td style="text-align: right;">35.32</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">W</span> - <span class="grey">Log-likelihood ratio test</span></td>
<td style="text-align: right;">24.17</td>
<td style="text-align: right;">35.49</td>
</tr>
</tbody>
</table>
<div class="fragment">
<ul>
<li><p>Hence, the expected <span class="orange">average</span> percentage increase is between <span class="math inline">24\%</span> and <span class="math inline">35\%</span> each <code>period</code> with a <span class="math inline">95\%</span> <span class="blue">confidence</span>.</p></li>
<li><p>By construction, these confidence intervals are <span class="orange">positive</span>, which is desirable because they represent a percentage.</p></li>
</ul>
</div>
</section>
<section id="example-aids-data-6" class="slide level2 center">
<h2>Example: <code>Aids</code> data</h2>
<ul>
<li><p>In the Wald case, we cannot simply transform the extremes of the intervals. Indeed, that would lead to a <span class="blue">valid</span> confidence interval that is not anymore of Wald type (<em>Lo sbagliato</em> ü•É).</p></li>
<li><p>Instead, we first need to <span class="orange">adjust</span> the <span class="orange">variance</span> according to the <span class="blue">delta method</span>, obtaining <span class="math display">
\widehat{\text{var}}\{100[\exp(\hat{\beta}_2)-1]\} = 100^2\exp(2 \hat{\beta}_2) \text{var}(\hat{\beta}_2) = 8.301184.
</span></p></li>
<li><p>The Wald confidence interval for <span class="math inline">100[\exp(\hat{\beta}_2)-1]</span> therefore is <span class="math display">
100[\exp(\hat{\beta}_2)-1] \pm z_{1-\alpha/2}\widehat{\text{se}}\{100[\exp(\hat{\beta}_2)-1]\}.
</span></p></li>
</ul>
<div class="cell styled-output">

</div>
<table class="caption-top">
<colgroup>
<col style="width: 61%">
<col style="width: 19%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Confidence intervals for <span class="math inline">100[\exp(\beta_2)-1]</span> at a <span class="math inline">95\%</span> level</th>
<th style="text-align: right;">2.5%</th>
<th style="text-align: right;">97.5%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">W_e</span> - <span class="blue">Wald test</span></td>
<td style="text-align: right;">23.91</td>
<td style="text-align: right;">35.21</td>
</tr>
<tr class="even">
<td style="text-align: left;">‚Äú<em>Lo sbagliato</em>‚Äù - transformed Wald</td>
<td style="text-align: right;">24.03</td>
<td style="text-align: right;">35.33</td>
</tr>
</tbody>
</table>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Whenever there are restrictions on the parametric space, as in this case, Wald is typically <span class="orange">problematic</span>. Here, it <span class="blue">could</span> lead to <span class="orange">negative values</span>, which is absurd.</p>
</div>
</div>
</div>
</section></section>
<section>
<section id="deviance-model-checking-residuals" class="title-slide slide level1 center">
<h1>Deviance, model checking, residuals</h1>

</section>
<section id="deviance-some-intuitions" class="slide level2 center">
<h2>Deviance: some intuitions</h2>
<ul>
<li><p>In a Gaussian linear model, we called <span class="orange">deviance</span> the residual sum of squares, that is <span class="math display">
D(\bm{y}; \hat{\bm{\mu}}) = \sum_{i=1}^n(y_i - \bm{x}_i^T\hat{\beta})^2 = \sum_{i=1}^n(y_i - \hat{\mu}_i)^2.
</span></p></li>
<li><p>The residual sum of squares <span class="math inline">D(\bm{y}; \hat{\bm{\mu}})</span> is a <span class="blue">goodness of fit</span> measure. The lower the deviance, the higher the quality of the predictions.</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>When <span class="math inline">\sigma^2</span> is <span class="orange">known</span>, the distribution of the <span class="blue">scaled deviance</span> is <span class="math display">
\frac{D(\bm{Y}; \hat{\bm{\mu}})}{\sigma^2} = \frac{1}{\sigma^2}\sum_{i=1}^n(Y_i - \bm{x}_i^T\hat{\beta})^2  \sim \chi^2_{n - p}.
</span></p></li>
<li><p>When <span class="math inline">\sigma^2</span> is known, the difference of scaled deviances of two nested models is: <span class="math display">
W = \frac{D(\bm{Y}; \hat{\bm{\mu}}_0) - D(\bm{Y}; \hat{\bm{\mu}})}{\sigma^2} = \frac{||\bm{Y} - \bm{X}\hat{\beta}_0||^2 - ||\bm{Y} - \bm{X}\hat{\beta}||^2}{\sigma^2} \sim \chi^2_q.
</span></p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>The natural question is: what is a natural <span class="blue">generalization</span> of the deviance for GLMs?</li>
</ul>
</div>
</section>
<section id="example-beetles-data-saturated-model" class="slide level2 center">
<h2>Example: <code>Beetles</code> data, saturated model</h2>
<ul>
<li>Let us consider again the <code>Beetles</code> data and the predictions <span class="math inline">\hat{\mu}_i</span>, based on <span class="math inline">p = 2</span> parameters. These predictions are not perfect but that may be due to chance.</li>
</ul>
<table class="caption-top" style="width:100%;">
<colgroup>
<col style="width: 7%">
<col style="width: 16%">
<col style="width: 19%">
<col style="width: 28%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;"><span class="math inline">m_i</span></th>
<th style="text-align: right;"><code>deaths</code> (<span class="math inline">S_i</span>)</th>
<th style="text-align: right;"><code>logdose</code> (<span class="math inline">x_i</span>)</th>
<th style="text-align: right;"><span class="math inline">Y_i = S_i / m_i</span></th>
<th style="text-align: right;"><span class="math inline">\hat{\mu}_i</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">59</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">1.691</td>
<td style="text-align: right;">0.102</td>
<td style="text-align: right;">0.059</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">1.724</td>
<td style="text-align: right;">0.217</td>
<td style="text-align: right;">0.164</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">1.755</td>
<td style="text-align: right;">0.290</td>
<td style="text-align: right;">0.362</td>
</tr>
<tr class="even">
<td style="text-align: right;">56</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">1.784</td>
<td style="text-align: right;">0.500</td>
<td style="text-align: right;">0.605</td>
</tr>
<tr class="odd">
<td style="text-align: right;">63</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">1.811</td>
<td style="text-align: right;">0.825</td>
<td style="text-align: right;">0.795</td>
</tr>
<tr class="even">
<td style="text-align: right;">59</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">1.837</td>
<td style="text-align: right;">0.898</td>
<td style="text-align: right;">0.903</td>
</tr>
<tr class="odd">
<td style="text-align: right;">62</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">1.861</td>
<td style="text-align: right;">0.984</td>
<td style="text-align: right;">0.955</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">1.884</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.979</td>
</tr>
</tbody>
</table>
<ul>
<li>The <span class="orange">empirical proportions</span> <span class="math inline">s_i / m_i</span> can be seen as <span class="blue">estimates</span> of the <span class="blue">most flexible model</span>, in which every observation <span class="math inline">Y_i</span> has its own mean <span class="math inline">\mu_i</span>. We call it <span class="orange">saturated model</span> because <span class="math inline">p = n</span>.</li>
</ul>
</section>
<section id="saturated-model" class="slide level2 center">
<h2>Saturated model</h2>
<ul>
<li><p>Let us express the <span class="orange">log-likelihood</span> of a GLM as a function of the mean <span class="math inline">\bm{\mu} = (\mu_1,\dots,\mu_n)</span>.</p></li>
<li><p>When evaluated in the maximum likelihood, this gives: <span class="math display">
\ell_\mathcal{M}(\hat{\bm{\mu}},\phi) = \sum_{i=1}^n\omega_i\frac{y_i\theta(\hat{\mu}_i) - b(\theta(\hat{\mu}_i))}{\phi} + c(y_i, \phi).
</span> The maximum likelihood for each <span class="math inline">\mu_i</span> is <span class="orange">restricted</span>, in the sense that depends on the <span class="math inline">p</span> parameters of the <span class="blue">linear predictor</span> <span class="math inline">\bm{x}_i^T\beta</span> through the link function <span class="math inline">g(\mu_i) = \bm{x}_i^T\beta</span>.</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>In the <span class="blue">saturated model</span> the means <span class="math inline">\mu_i</span> are <span class="orange">unrestricted</span>: each parameter is estimated separately, giving the maximum likelihood estimate <span class="math inline">\hat{\mu}_{i, \text{sat}} = y_i</span>. This happens whenever <span class="math inline">p = n</span>.</p></li>
<li><p>When evaluated in the maximum, the log-likelihood of the saturated model is <span class="math display">
\ell_\mathcal{M}(\bm{y},\phi) = \sum_{i=1}^n\omega_i\frac{y_i\theta(y_i) - b(\theta(y_i))}{\phi} + c(y_i, \phi).
</span></p></li>
<li><p>The saturated model is the <span class="blue">most complex model</span> we can think of.</p></li>
</ul>
</div>
</section>
<section id="deviance" class="slide level2 center">
<h2>Deviance</h2>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>The <span class="blue">deviance</span> of a GLM is defined as <span class="math display">
\begin{aligned}
D(\bm{y}; \hat{\bm{\mu}}) &amp;:= \phi \: 2[\ell_\mathcal{M}(\bm{y},\phi)  - \ell_\mathcal{M}(\hat{\bm{\mu}},\phi)] \\
&amp;=\sum_{i=1}^n\omega_i\left\{y_i [\theta(y_i) - \theta(\hat{\mu}_i)] - [b(\theta(y_i)) - b(\theta(\hat{\mu}_i))]\right\}.
\end{aligned}
</span> The quantity <span class="math inline">D(\bm{y}; \hat{\bm{\mu}})/\phi</span> is called <span class="blue">scaled deviance</span> and it corresponds to a <span class="orange">log-likelihood ratio test</span> in which the current model is tested against the saturated model.</p>
</div>
</div>
</div>
<ul>
<li><p>By definition, the deviance is <span class="orange">positive</span>: <span class="math inline">D(\bm{y}; \hat{\bm{\mu}}) \ge 0</span>, because <span class="math inline">\ell_\mathcal{M}(\bm{y},\phi) \ge \ell_\mathcal{M}(\hat{\bm{\mu}},\phi)</span>.</p></li>
<li><p>The deviance of the saturated model is <span class="math inline">D(\bm{y}; \bm{y}) = 0</span>.</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>The deviance describes a <span class="blue">lack of fit</span>: the higher the deviance, the poorer the fit.</p></li>
<li><p>The deviance measures the discrepancy between the saturated model and a model using <span class="math inline">p &lt; n</span> parameters.</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>In the Gaussian linear model, this definition gives the <span class="orange">residual sum of squares</span>.</li>
</ul>
</div>
</section>
<section id="deviance-and-log-likelihood-ratio-test" class="slide level2 center">
<h2>Deviance and log-likelihood ratio test</h2>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let us consider two <span class="grey">nested models</span> <span class="math inline">M_0 \subset M_1</span>. The <span class="orange">reduced model</span> <span class="math inline">M_0</span> has <span class="math inline">p_0</span> parameters and predictions <span class="math inline">\hat{\bm{\mu}}_0</span>. The <span class="blue">full model</span> <span class="math inline">M_1</span> has <span class="math inline">p</span> parameters <span class="math inline">\hat{\bm{\mu}}_1</span>.</p>
<p>The <span class="blue">log-likelihood ratio test</span> <span class="math inline">W</span> for testing model <span class="math inline">M_0</span> against model <span class="math inline">M_1</span><sup>1</sup> can be written as <span class="math display">
W = 2 [\ell_\mathcal{M}(\hat{\bm{\mu}}, \hat{\phi})  - \ell_\mathcal{M}(\hat{\bm{\mu}}_0, \hat{\phi})] = \frac{D(\bm{Y}; \hat{\bm{\mu}}) - D(\bm{Y}; \hat{\bm{\mu}}_0)}{\hat{\phi}} \: \dot{\sim} \: \chi^2_q.
</span> where <span class="math inline">q = p - p_0</span> are the degrees of freedom.</p>
</div>
</div>
</div>
<ul>
<li><p>The log-likelihood ratio can be interpreted as a difference of scaled deviances. This explains why it is popular in GLMs for comparing nested models.</p></li>
<li><p>This is also strong parallelism with the Gaussian linear model.</p></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn2"><p>More formally, we should say that we are testing the hypothesis <span class="math inline">H_0: \beta_B = \bm{0}</span> against the alternative <span class="math inline">H_1: \beta_B \neq \bm{0}</span>. I hope you can tolerate this slight linguistic abuse.</p></li></ol></aside></section>
<section id="the-null-model" class="slide level2 center">
<h2>The null model</h2>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Let us consider a model <span class="math inline">M_\text{null}</span> with <span class="blue">no covariates</span> and one parameter (<span class="math inline">p =1</span>), i.e.&nbsp;the <span class="blue">intercept</span>. The predicted values are all equals to <span class="math display">
\bm{\hat{\mu}}_\text{null} = (g^{-1}(\hat{\beta}_1), \dots, g^{-1}(\hat{\beta}_1)).
</span> We call <span class="math inline">M_\text{null}</span> the <span class="blue">null model</span> and <span class="math inline">D(\bm{y}; \hat{\bm{\mu}}_\text{null})</span> the <span class="orange">null deviance</span>.</p>
</div>
</div>
</div>
<ul>
<li><p>The null model is the ‚Äúopposite‚Äù of the saturated model. It is the <span class="orange">simplest</span> among all models and the one having the <span class="blue">highest deviance</span>.</p></li>
<li><p>Indeed, the following inequalities hold: <span class="math display">
0 = D(\bm{y}; \bm{y}) \le D(\bm{y}; \hat{\bm{\mu}}) \le D(\bm{y}; \hat{\bm{\mu}}_\text{null}).
</span></p></li>
<li><p>It is sometimes useful to test the current model against the null model: <span class="math display">
W = \frac{D(\bm{Y}; \hat{\bm{\mu}}) - D(\bm{Y}; \hat{\bm{\mu}}_\text{null})}{\hat{\phi}} \: \dot{\sim} \: \chi^2_{p-1}.
</span> If the <span class="math inline">H_0</span> is not rejected, it means all the <span class="orange">covariates</span> are regarded as <span class="orange">irrelevant</span>.</p></li>
</ul>
</section>
<section id="deviance-as-goodness-of-fit-measure" class="slide level2 center">
<h2>Deviance as goodness of fit measure</h2>
</section>
<section id="relationship-between-deviance-and-pearson-x2" class="slide level2 center">
<h2>Relationship between deviance and Pearson <span class="math inline">X^2</span></h2>
</section>
<section id="deviance-of-a-gaussian-linear-model" class="slide level2 center">
<h2>Deviance of a Gaussian linear model</h2>
</section>
<section id="deviance-of-a-poisson-model" class="slide level2 center">
<h2>Deviance of a Poisson model</h2>
</section>
<section id="deviance-of-a-binomial-model" class="slide level2 center">
<h2>Deviance of a binomial model</h2>
</section>
<section id="deviance-of-a-gamma-model" class="slide level2 center">
<h2>Deviance of a Gamma model</h2>
</section>
<section id="example-beetles-data-output-of-summary" class="slide level2 center">
<h2>Example: <code>Beetles</code> data, output of <code>summary</code></h2>
<ul>
<li>This is how the <code>summary</code> of a GLM looks like. It is very similar to the <code>summary</code> of <code>lm</code>. At this stage of the course, you should be able to understand almost everything.</li>
</ul>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = cbind(deaths, n - deaths) ~ logdose, family = "binomial", 
    data = Beetles)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -60.717      5.181  -11.72   &lt;2e-16 ***
logdose       34.270      2.912   11.77   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 284.202  on 7  degrees of freedom
Residual deviance:  11.232  on 6  degrees of freedom
AIC: 41.43

Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<ul>
<li><p><span class="math inline">\texttt{Null deviance}</span> corresponds to the <span class="blue">null deviance</span> <span class="math inline">D(\bm{y}; \hat{\bm{\mu}}_\text{null})</span>.</p></li>
<li><p><span class="math inline">\texttt{Residual deviance}</span> corresponds to the <span class="orange">deviance</span> <span class="math inline">D(\bm{y}; \hat{\bm{\mu}})</span> of the current model.</p></li>
</ul>
</section>
<section id="example-beetles-data-output-of-anova" class="slide level2 center">
<h2>Example: <code>Beetles</code> data, output of <code>anova</code></h2>
<ul>
<li><code>anova(model0, model1)</code> computes <span class="blue">log-likelihood ratio test</span> comparing two nested models: the <span class="orange">reduced</span> model <span class="math inline">M_0</span> with <span class="math inline">p_0</span> parameters and the <span class="blue">full</span> model <span class="math inline">M_1</span> with <span class="math inline">p</span> parameters.</li>
</ul>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Deviance Table

Model 1: cbind(deaths, n - deaths) ~ 1
Model 2: cbind(deaths, n - deaths) ~ logdose
  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
1         7    284.202                          
2         6     11.232  1   272.97 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<div class="fragment">
<ul>
<li><p><span class="math inline">\texttt{Resid Df}</span> are the <span class="blue">degrees of freedom</span> of the <span class="orange">deviances</span>, that is <span class="math inline">n - p_0</span> and <span class="math inline">n-p</span>, respectively.</p></li>
<li><p><span class="math inline">\texttt{Resid. Dev}</span> are the <span class="blue">deviances</span> of the <span class="orange">reduced</span> model <span class="math inline">D(\bm{y}; \hat{\bm{\mu}}_0)</span> and the <span class="blue">full</span> model <span class="math inline">D(\bm{y}; \hat{\bm{\mu}})</span>. In this example, the reduced model is also the <span class="orange">null model</span>.</p></li>
<li><p><span class="math inline">\texttt{Df}</span> refers to the <span class="blue">degrees of freedom</span> <span class="math inline">q = p- p_0</span> of the test, which is <span class="math inline">q = 1</span> in this case.</p></li>
<li><p><span class="math inline">\texttt{Deviance}</span> indicates the <span class="blue">change in deviance</span>, that is <span class="math inline">\phi W = D(\bm{y}; \hat{\bm{\mu}}) - D(\bm{y}; \hat{\bm{\mu}}_0)</span>.</p></li>
<li><p><span class="math inline">\texttt{Pr(&gt;Chi)}</span> is the <span class="blue">p-value</span> of the log-likelihood ratio test <span class="math inline">W</span>.</p></li>
</ul>
</div>
</section>
<section id="example-aids-data-output-of-summary" class="slide level2 center">
<h2>Example: <code>Aids</code> data, output of <code>summary</code></h2>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = deaths ~ period, family = "poisson", data = Aids)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.30365    0.25387   1.196    0.232    
period       0.25896    0.02224  11.645   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 208.754  on 13  degrees of freedom
Residual deviance:  30.203  on 12  degrees of freedom
AIC: 86.949

Number of Fisher Scoring iterations: 5</code></pre>
</div>
</div>
</section>
<section id="example-aids-data-output-of-anova" class="slide level2 center">
<h2>Example: <code>Aids</code> data, output of <code>anova</code></h2>
<div class="cell styled-output">
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Deviance Table

Model 1: deaths ~ 1
Model 2: deaths ~ period
  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
1        13    208.754                          
2        12     30.203  1   178.55 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
</section>
<section id="residuals" class="slide level2 center">
<h2>Residuals</h2>
</section>
<section id="pearson-residuals" class="slide level2 center">
<h2>Pearson residuals</h2>
</section>
<section id="deviance-residuals" class="slide level2 center">
<h2>Deviance residuals</h2>
</section>
<section id="quantile-residuals" class="slide level2 center">
<h2>Quantile residuals</h2>
</section>
<section id="standardized-residuals" class="slide level2 center">
<h2>Standardized residuals</h2>
</section>
<section id="approximate-cooks-distance" class="slide level2 center">
<h2>Approximate Cook‚Äôs distance</h2>
</section></section>
<section>
<section id="model-selection" class="title-slide slide level1 center">
<h1>Model selection</h1>

</section>
<section id="stepwise-procedures-forward-and-backward-selection" class="slide level2 center">
<h2>Stepwise procedures: forward and backward selection</h2>
</section>
<section id="aic-and-bic" class="slide level2 center">
<h2>AIC and BIC</h2>
</section>
<section id="references" class="slide level2 unnumbered smaller scrollable">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Agresti2015" class="csl-entry" role="listitem">
Agresti, A. (2015), <em><span class="nocase">Foundations of Linear and Generalized Linear Models</span></em>, Wiley.
</div>
<div id="ref-Azzalini2008" class="csl-entry" role="listitem">
Azzalini, A. (2008), <em>Inferenza statistica</em>, Springer Verlag.
</div>
<div id="ref-Efron2023" class="csl-entry" role="listitem">
Efron, B. (2023), <em><span class="nocase">Exponential Families in Theory and Practice</span></em>, Cambridge University Press.
</div>
<div id="ref-Fisher1934" class="csl-entry" role="listitem">
Fisher, R. A. (1934), <span>‚Äú<span class="nocase">Two new properties of mathematical likelihood</span>,‚Äù</span> <em>Proceedings of the Royal Society of London. Series A</em>, 144, 285‚Äì307.
</div>
<div id="ref-Nelder1972" class="csl-entry" role="listitem">
Nelder, J. A., and Wedderburn, R. W. M. (1972), <span>‚Äú<span class="nocase">Generalized linear models</span>,‚Äù</span> <em>Journal of the Royal Statistical Society. Series A: Statistics in Society</em>, 135, 370‚Äì384.
</div>
<div id="ref-Salvan2020" class="csl-entry" role="listitem">
Salvan, A., Sartori, N., and Pace, L. (2020), <em>Modelli lineari generalizzati</em>, Springer.
</div>
</div>

</section></section>

    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="img/logoB.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://tommasorigon.github.io/StatIII">Home page</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="un_B_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="un_B_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="un_B_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>