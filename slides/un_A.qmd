---
title: "Linear models and misspecification"
subtitle: "Statistics III - CdL SSE"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
page-layout: full
bibliography: ../biblio.bib
citeproc: true
csl: https://www.zotero.org/styles/journal-of-the-american-statistical-association
reference-location: margin
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_A_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 250
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [simplex, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    smooth-scroll: true
    fig-dpi: 250
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

# Please ignore this chunk
knitr::purl("un_A.qmd", output = "../code/un_A.R", documentation = 0)
styler:::style_file("../code/un_A.R")
```

::: columns
::: {.column width="40%"}
![](img/ABC.png) *"Everything should be made as simple as possible, but
not simpler"*

Attributed to [Albert Einstein]{.grey}
:::

::: {.column width="60%"}
-   This unit will cover the following [topics]{.orange}:

    - Recap: linear models and the modeling process
    - Box-Cox transform, variance stabilizing transformations
    - Robustness of OLS estimates, sandwich estimators
    - Weighted least squares
    
- The main theme is: what should we do when the [assumptions]{.blue} of linear models are [violated]{.orange}?

- We will push the linear model to its limit, using it even when is not supposed to work. 

:::
:::

# Old friends: linear models

## Car data ([diesel]{.blue} or [gas]{.orange})

::: columns
::: {.column width="50%"}
```{r}
#| warning: false
#| message: false
#| fig-width: 5
#| fig-height: 4.5
library(tidyverse)
library(broom)
library(knitr)
library(ggplot2)
library(GGally)
library(ggthemes)

rm(list = ls())
# The dataset can also be downloaded here: https://tommasorigon.github.io/datamining/data/auto.txt
auto <- read.table("../data/auto.txt", header = TRUE) %>% select(city.distance, engine.size, n.cylinders, curb.weight, fuel)

p0 <- ggpairs(auto,
  columns = 1:4, aes(colour = fuel),
  lower = list(continuous = wrap("points", size = 0.5)),
  upper = list(continuous = wrap("points", size = 0.5)),
  diag = "blank"
) +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("") +
  ylab("")
p0
```
:::

::: {.column width="50%"}
-   We consider data for $n = 203$ models of cars in circulation in 1985
    in the USA.
-   We want to [predict]{.blue} the distance per unit of fuel as a
    function of the vehicle features.
-   We consider the following [variables]{.orange}:
    -   The city distance per unit of fuel (km/L, `city.distance`)
    -   The engine size (L, `engine.size`)
    -   The number of cylinders (`n.cylinders`)
    -   The curb weight (kg, `curb.weight`)
    -   The fuel type (gasoline or diesel, `fuel`).
:::
:::

## Linear regression

::: columns
::: {.column width="40%"}
```{r}
#| fig-width: 4
#| fig-height: 3.7
ggplot(data = auto, aes(x = engine.size, y = city.distance, col = fuel)) +
  geom_point() +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  theme(legend.position = "top") +
  xlab("Engine size (L)") +
  ylab("Urban distance (km/L)")
```
:::

::: {.column width="60%"}
-   Let us consider the variables `city.distance` ($y$), `engine.size`
    ($x$) and `fuel` ($z$).

-   A [simple linear regression]{.blue} $$
    Y_i = \beta_1 + \beta_2 x_i + \epsilon_i, \qquad i=1,\dots,n,
    $$ could be easily fit by least squares...

-   ... but the plot suggests that the relationship between
    `city.distance` and `engine.size` is [not]{.orange} well
    approximated by a [linear]{.orange} function.

-   ... and also that `fuel` has a non-negligible effect on the
    response.
:::
:::

## Regression models

::: incremental
-   A [general]{.orange} and [more flexible formulation]{.orange} for
    modeling the relationship between a vector of [fixed
    covariates]{.blue}
    $\bm{x}_i = (x_{i1},\dots,x_{ip})^T \in \mathbb{R}^p$ and a random
    variable $Y_i \in \mathbb{R}$ is $$
    Y_i = f(\bm{x}_i; \beta) + \epsilon_i, \qquad i=1,\dots,n,
    $$ where the "errors" $\epsilon_i$ are iid random variables, having
    zero mean and variance $\sigma^2$.

-   To estimate the unknown parameters $\beta$, a possibility is to rely
    on the [least squares criterion]{.blue}: we seek the
    [minimum]{.orange} of the objective function $$
    D(\beta) = \sum_{i=1}^n\{y_i - f(\bm{x}_i; \beta)\}^2,
    $$ using $n$ pairs of covariates
    $\bm{x}_i = (x_{i1},\dots,x_{ip})^T$ and the observed realizations
    $y_i$ of the random variables $Y_i$, for $i = 1,\dots,n$. The
    [optimal value]{.orange} is denoted by $\hat{\beta}$.

-   The [predicted values]{.blue} are
    $\hat{y}_i = \mathbb{E}(Y_i) = f(\bm{x}_i; \hat{\beta})$, for
    $i=1,\dots,n.$
:::

## Linear models

-   Let us consider again the variables `city.distance` ($y$),
    `engine.size` ($x$) and `fuel` ($z$).

-   Which function $f(x,z;\beta)$ should we choose?

. . .

-   A first attempt is to consider a [polynomial term]{.orange} combined
    with a [dummy variable]{.blue} $$
    f(x, z; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \beta_4 x^3 + \beta_5 I(z = \texttt{gas}),
    $$ which is a special instance of [linear model]{.orange}.

. . .

::: callout-note
#### Definition (Linear model)

In a [linear model]{.blue} the response variable $Y_i$ is related to the
covariates through the function$$
    \mathbb{E}(Y_i) =f(\bm{x}_i; \beta) = \beta_1 x_{i1} + \cdots + \beta_p x_{ip} =\bm{x}_i^T\beta,
    $$ where $\bm{x}_i = (x_{i1},\dots,x_{ip})^T$ is a vector of
[covariates]{.orange} and $\beta = (\beta_1,\dots,\beta_p)^T$ is the
corresponding vector of [coefficients]{.orange}.
:::

## Matrix notation

-   The [response random variables]{.orange} are collected in the random
    vector $\bm{Y} = (Y_1,\dots,Y_n)^T$, whose [observed
    realization]{.blue} is $\bm{y} = (y_1,\dots,y_n)^T$.

-   The [design matrix]{.blue} is a $n \times p$ matrix, comprising the
    covariate's values, defined by $$
    \bm{X} = 
    \begin{bmatrix} 
    x_{11} & \cdots & x_{1p}\\
    \vdots & \ddots & \vdots \\
    x_{n1} & \cdots & x_{np}
    \end{bmatrix}.
    $$

. . .

-   The $j$th variable (column) is denoted with $\tilde{\bm{x}}_j$,
    whereas the $i$th observation (row) is $\bm{x}_i$: $$
    \bm{X} = (\tilde{\bm{x}}_1,\dots,\tilde{\bm{x}}_p) = (\bm{x}_1, \dots,\bm{x}_n)^T.
    $$

. . .

-   Then, a [linear model]{.blue} can be written using the [compact
    notation]{.orange}: $$
    \bm{Y} = \bm{X}\beta + \bm{\epsilon},
    $$ where $\bm{\epsilon} = (\epsilon_1,\dots,\epsilon_n)^T$ is a
    vector of iid error terms with zero mean and variance $\sigma^2$.

## Linear regression: estimation I

-   The optimal set of coefficients $\hat{\beta}$ is the minimizer of
    the [least squared criterion]{.orange} $$
    D(\beta) = (\bm{y} - \bm{X}\beta)^T(\bm{y} - \bm{X}\beta) = ||\bm{y} - \bm{X}\beta||^2,
    $$ also known as [residual sum of squares (RSS)]{.orange}, where $$
    ||\bm{y}|| = \sqrt{y_1^2 + \cdots + y_n^2},$$ denotes the [Euclidean
    norm]{.blue}.

. . .

::: callout-note
#### Least square estimate (OLS)

If the design matrix has [full rank]{.blue}, that is, if
$\text{rk}(\bm{X}^T\bm{X}) = p$, then the [least square
estimate]{.orange} has an explicit solution: $$
    \hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T \bm{y}.
    $$
:::

## Linear regression: estimation II

-   In matrix notation, the predicted values can be obtained as $$
    \hat{\bm{y}} = \bm{X}\hat{\beta} = \bm{H}\bm{y}, \qquad \bm{H} = \bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T.
    $$

-   $\bm{H}$ is a $n \times n$ [projection matrix]{.orange} matrix
    sometimes called [hat matrix]{.blue}.

-   It can be shown that $\text{tr}(\bm{H}) = \text{rk}(\bm{H}) = p$.
    Moreover, it holds $\bm{H} = \bm{H}^T$ and $\bm{H}^2 = \bm{H}$.

. . .

-   The quantity $D(\hat{\beta})$ is the so-called [deviance]{.blue},
    which is equal to $$
    D(\hat{\beta}) = ||\bm{y} - \hat{\bm{y}}||^2 = \bm{y}^T(I_n - \bm{H})\bm{y}.
    $$

-   Moreover, a typical estimate for the [residual variance]{.orange}
    $\sigma^2$ is obtained as follows: $$
    s^2 = \frac{D(\hat{\beta})}{n - p} = \frac{1}{n-p}\sum_{i=1}^n(y_i - \bm{x}_i^T\hat{\beta})^2.
    $$

## Linear regression: inference

-   Let us additionally assume that the errors follow a Gaussian
    distribution:
    $\epsilon_i \overset{\text{iid}}{\sim} N(0, \sigma^2)$.

-   This implies that the [distribution]{.orange} of the
    [estimator]{.orange} $\hat{\beta}$ is $$
    \hat{\beta} \sim \text{N}_p(\beta, \sigma^2 (\bm{X}^T\bm{X})^{-1}).
    $$

-   Hence, the estimator $\hat{\beta}$ is [unbiased]{.orange} and its
    [variance]{.blue} can be estimated by $$
    \widehat{\text{var}}(\hat{\beta}) = s^2 (\bm{X}^T\bm{X})^{-1}.
    $$

-   The [standard errors]{.orange} of the components of $\hat{\beta}$
    correspond to the square root of the diagonal of the above
    covariance matrix.

. . .

-   Confidence interval and Wald's tests can be obtained through
    classical inferential theory.

. . .

-   Ok, we are ready to get back to the original problem...

## Car data, a first model

-   Our first attempt for predicting `city.distance` ($y$) via
    `engine.size` ($x$) and `fuel` ($z$) is: $$
      f(x, z; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \beta_4 x^3 + \beta_5 I(z = \texttt{gas}).
      $$

-   Indeed, by looking at the plot of the data, it is plausible that we
    need a [polynomial]{.orange} of degree $3$ or $4$

-   It is also clear from the plot that `fuel` is a relevant variable.
    Categorical variables are [encoded]{.orange} using [indicator
    variables]{.blue}.

. . .

-   To evaluate the goodness of fit, we can calculate the [coefficient
    of determination]{.orange}: $$
    R^2 = 1 - \frac{\text{(``Residual deviance'')}}{\text{(``Total deviance'')}} = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n(y_i - \bar{y})^2}.
    $$

## A first model: estimated coefficients

-   We obtain the following [summary]{.orange} for the regression
    coefficients $\hat{\beta}$.

```{r}
#| output: false
m1 <- lm(city.distance ~ engine.size + I(engine.size^2) + I(engine.size^3) + fuel, data = auto)
#kable(tidy(m1, conf.int = FALSE), digits = 3)
```

| term            | estimate | std.error | statistic | p.value |
|:----------------|---------:|----------:|----------:|--------:|
| `(Intercept)`   |   28.045 |     3.076 |     9.119 |   0.000 |
| `engine.size`   |  -10.980 |     3.531 |    -3.109 |   0.002 |
| `engine.size^2` |    2.098 |     1.271 |     1.651 |   0.100 |
| `engine.size^3` |   -0.131 |     0.139 |    -0.939 |   0.349 |
| `fuel_gas`      |   -3.214 |     0.427 |    -7.523 |   0.000 |

-   Moreover, the coefficient $R^2$ and the residual standard deviation
    $s$ are:

```{r}
kable(glance(m1)[c(1, 3, 10)])
```

## A first model: fitted values

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
augmented_m1 <- augment(m1)
ggplot(data = augmented_m1, aes(x = engine.size, y = city.distance, col = fuel)) +
  geom_point() +
  geom_line(aes(y = .fitted)) +
  theme_light() +
  theme(legend.position = "right") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Engine size (L)") +
  ylab("Urban distance (km/L)")
```

## A first model: graphical diagnostics

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
ggplot(data = augmented_m1, aes(x = .fitted, y = .resid, col = fuel)) +
  geom_point() +
  geom_hline(aes(yintercept = 0), linetype = "dotted") +
  theme_light() +
  theme(legend.position = "right") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Fitted values") +
  ylab("Residuals")
```

## Comments and criticisms

-   Is this a good model?

-   The overall fit [seems satisfactory]{.blue} at first glance,
    especially if we aim at predicting the urban distance of cars when
    average engine size (i.e., between $1.5L$ and $3L$).

. . .

-   However, the plot of the [residuals]{.orange}
    $r_i = y_i - \hat{y}_i$ suggests that the homoscedasticity
    assumption, i.e. $\text{var}(\epsilon_i) = \sigma^2$, might be
    violated.

. . .

-   Also, this model is unsuitable for [extrapolation]{.orange}. Indeed:

    -   It has no grounding in physics or engineering, leading to
        difficulties when interpreting the trend and to paradoxical
        situations.
    -   For example, the curve of the set of gasoline cars shows a local
        minimum around $4.6 L$ and then rises again!

-   It is plausible that we can find a better one, so what's next?

## Linear models and non-linear patterns

-   A significant advantage of linear models is that they can describe
    non-linear relationships via [variable transformations]{.blue} such
    as polynomials, logarithms, etc.

. . .

-   This gives the statistician a lot of modeling flexibility. For
    instance, we could let: $$
    \log{Y_i} = \beta_1 + \beta_2 \log{x_i} + \beta_3 I(z_i = \texttt{gas}) + \epsilon_i, \qquad i=1,\dots,n.
    $$

. . .

-   This specification is [linear in the parameters]{.orange}, it fixes
    the domain issues, and it imposes a monotone relationship between
    engine size and consumption.

. . .

```{r}
#| output: false
m2 <- lm(log(city.distance) ~ I(log(engine.size)) + fuel, data = auto)
kable(tidy(m2, conf.int = FALSE), digits = 3)
```

| term               | estimate | std.error | statistic | p.value |
|:-------------------|---------:|----------:|----------:|--------:|
| `(Intercept)`      |    3.060 |     0.047 |    64.865 |       0 |
| `log(engine.size)` |   -0.682 |     0.040 |   -17.129 |       0 |
| `fuel_gas`         |   -0.278 |     0.038 |    -7.344 |       0 |

## Second model: fitted values

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
augmented_m2 <- augment(m2, data = auto)
ggplot(data = augmented_m2, aes(x = engine.size, y = city.distance, col = fuel)) +
  geom_point() +
  geom_line(aes(y = exp(.fitted))) +
  theme_light() +
  theme(legend.position = "right") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Engine size (L)") +
  ylab("Urban distance (km/L)")
```

## Second model: graphical diagnostics

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
ggplot(data = augmented_m2, aes(x = .fitted, y = .resid, col = fuel)) +
  geom_point() +
  geom_hline(aes(yintercept = 0), linetype = "dotted") +
  theme_light() +
  theme(legend.position = "right") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Fitted values") +
  ylab("Residuals")
```

## Comments and criticisms

-   The [goodness of fit]{.blue} indices are the following:

```{r}
r.squared.original <- 1 - sum(mean((auto$city.distance - exp(predict(m2)))^2)) / sum(mean((auto$city.distance - mean(auto$city.distance))^2))
kable(data.frame(r.squared.original = r.squared.original, glance(m2)[c(1, 3, 10)]))
```

-   Do not mix [apple]{.blue} and [oranges]{.orange}! Compare $R^2$s
    only if they refer to the same scale!

. . .

-   This second model is [more parsimonious]{.blue}, and yet it reaches
    satisfactory predictive performance.

-   It is also more coherent with the nature of the data: the
    predictions cannot be negative, and the relationship between engine
    size and the consumption is monotone.

-   Yet, there is still some heteroscedasticity in the residuals --- is
    this is due to a missing covariate that has not been included in the
    model?

## A third model: additional variables

-   Let us consider [two additional variables]{.blue}: `curb.weight`
    ($w$) and `n.cylinders` ($v$).

-   A richer model, therefore, could be: $$
    \log{Y_i} = \beta_1 + \beta_2 \log{x_i} +  \beta_3 \log{w_i} + \beta_4 I(z_i = \texttt{gas}) + \beta_5 I(v_i = 2) + \epsilon_i,
      $$ for $i=1,\dots,n$. The estimates are:

. . .

```{r}
#| output: false
auto$cylinders2 <- factor(auto$n.cylinders == 2)
m3 <- lm(log(city.distance) ~ I(log(engine.size)) + I(log(curb.weight)) + fuel + cylinders2, data = auto)
kable(tidy(m3, conf.int = FALSE), digits = 3)
```

| term               | estimate | std.error | statistic | p.value |
|:-------------------|---------:|----------:|----------:|--------:|
| `(Intercept)`      |    9.423 |     0.482 |    19.549 |   0.000 |
| `log(engine.size)` |   -0.180 |     0.051 |    -3.504 |   0.001 |
| `log(curb.weight)` |   -0.943 |     0.072 |   -13.066 |   0.000 |
| `fuel_gas`         |   -0.353 |     0.022 |   -15.934 |   0.000 |
| `cylinders2_TRUE`  |   -0.481 |     0.052 |    -9.301 |   0.000 |

## A third model: graphical diagnostics

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
augmented_m3 <- augment(m3, data = auto)
ggplot(data = augmented_m3, aes(x = .fitted, y = .resid, col = fuel)) +
  geom_point() +
  geom_hline(aes(yintercept = 0), linetype = "dotted") +
  theme_light() +
  theme(legend.position = "right") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Fitted values") +
  ylab("Residuals")
```

## Comments and criticisms

-   The goodness of fit greatly [improved]{.blue}:

```{r}
r.squared.original <- 1 - sum(mean((auto$city.distance - exp(predict(m3)))^2)) / sum(mean((auto$city.distance - mean(auto$city.distance))^2))
kable(data.frame(r.squared.original = r.squared.original, glance(m3)[c(1, 3, 10)]))
```

-   In this third model, we handled the [outliers]{.orange} appearing in
    the residual plots, which it turns out are identified by the group
    of cars having 2 cylinders.

-   The diagnostic plots are also very much improved, although still not
    perfect.

-   The estimates are coherent with our expectations, based on common
    knowledge. Have a look at the textbook (A&S) for a detailed
    explanation of $\beta_4$!

-   The car dataset is available from the textbook (A&S) website:

    -   Dataset <http://azzalini.stat.unipd.it/Book-DM/auto.dat>
    -   Variable description
        <http://azzalini.stat.unipd.it/Book-DM/auto.names>
        
# Misspecification and remedies

## Assumptions and misspecification

:::callout-note
#### Classical assumptions of a linear model

  - **(A.1)** [Linear structure]{.blue}, namely $\bm{Y} = \bm{X}\beta + \bm{\epsilon}$ with $\mathbb{E}(\bm{\epsilon}) = 0$, implying $\mathbb{E}(\bm{Y}) = \bm{X}\beta$. [^err1] 
  
  - **(A.2)** [Homoschedasticity]{.orange} and [uncorrelation]{.orange} of the errors, namely $\text{var}(\bm{\epsilon}) = \sigma^2 I_n$.
  
  - **(A.3)** [Gaussianity]{.grey}, namely $\bm{\epsilon} \sim \text{N}_n(0, \sigma^2 I_n)$. In other words, the errors $\epsilon_i \overset{\text{iid}}{\sim}N(0, \sigma^2)$ are iid Gaussian random variables with zero mean and variance $\sigma^2$.

It is also commonly asked that $\text{rk}(\bm{X}) = p$, otherwise the model is not identifiable.
:::

- If one of the above assumptions is violated, it is not necessarily a huge problem, because
  - the OLS estimator $\hat{\beta}$ is fairly [robust]{.orange} to misspecification;
  - simple [fixes]{.blue} (variable transformations, standard error corrections) are available.

<!-- - Here we review the [implications]{.blue} of each [assumption]{.orange}, and a few  [solutions]{.blue}. -->


<!-- - Even if the data are [heroschedastic]{.orange} the OLS estimator is still a very reasonable choice, but we need to "correct" the standard errors to account for that.  -->

[^err1]: If the intercept is included in $\bm{X}$, the errors automatically satisfy the property $\mathbb{E}(\bm{\epsilon}) = 0$.


## Robust estimation and assumptions

![](img/plane.png){.nostretch fig-align="center" width="600px"}

- A plane can still fly with one of its [engines on fire]{.orange}, but this is hardly an appealing situation.

- Similarly, robust estimators may work under [model misspecification]{.orange}, but this does not mean we should neglect [checking]{.blue} whether the original [assumptions]{.blue} hold.



## Variable transformations

- The first remedy for [misspecification]{.orange} was implicitly applied in the analysis of the car dataset, namely through [variable transformation]{.blue}.

:::callout-note
While the model may have been incorrectly specified for the original data, it could become [appropriate]{.blue} once the [transformations]{.orange} are considered, namely
$$
g(Y_i) = h_1(\bm{x}_i)\beta_1 + \cdots + h_p(\bm{x}_i)\beta_p + \epsilon_i, \qquad i=1,\dots,n,
$$
where $g(\cdot)$ and $h_j(\cdot)$ for $j=1,\dots,p$ are [non-linear]{.orange} and [known]{.blue} functions. 
:::

. . .

- This idea is conceptually [simple]{.blue} and [powerful]{.blue}. It also shows that linear models are capable of capturing non-linear relationships, as long as they remain [linear in the parameters]{.orange}.

- However, choosing $g(\cdot)$ and $h_j(\cdot)$ in practice is [not simple]{.orange}. In our case study, we proceeded by trial and error and used [contextual information]{.blue} to guide our final choice.

. . .

- Regarding the functions $h_j(\cdot)$, [polynomial]{.blue} terms are a simple and common option. More advanced approaches based on [splines]{.orange} will be discussed in [Data Mining](https://tommasorigon.github.io/datamining/). 

<!-- - Here we consider two general approaches for selecting the function $g(\cdot)$: variance stabilizing transformations and the Box-Cox transform.   -->

## Variance stabilizing transformations I

- Let $Y_i \sim \text{Poisson}(\mu_i)$ with mean $\mathbb{E}(Y_i) = \mu_i = f(\bm{x}_i;\beta) = \text{var}(Y_i)$. Note that $$
Y_i \,\dot{\sim}\, \text{N}(\mu_i, \mu_i),
$$
is [asymptotically Gaussian]{.blue} for large values of $\mu_i$. However, data are [heteroschedastic]{.orange}. 

- In modeling count data, we could transform the counts so that, at
least [approximately]{.orange}, the [variance]{.blue} of $g(Y_i)$ is [constant]{.blue} and ordinary least squares methods can be used.

. . .

- As an application of the [delta method]{.blue}, the following linearization holds
$$
g(Y_i) - g(\mu_i) \approx (Y_i - \mu_i)g'(\mu_i), \quad \text{ which implies }\quad \text{var}\{g(Y_i)\} \approx g'(\mu_i)^2\text{var}(Y_i).
$$
In the Poisson case $\text{var}\{g(Y_i)\} \approx \mu_i \,g'(\mu_i)^2$ and we would like this to be [constant]{.orange}.

. . .

- The choice $g(y) = \sqrt{y}$, called [variance stabilizing]{.orange} transformation, gives
$$
\text{var}(\sqrt{Y_i}) \approx \left(\frac{1}{2\sqrt{\mu_i}}\right)^2\mu_i = \frac{1}{4}.
$$

## Variance stabilizing transformations II

<!-- - The variance stabilizing transformation is [broadly applicable]{.blue} to several distributions.  -->

- Let $Y_i \sim \text{Binomial}(\pi_i, n_i)$, with [success probability]{.blue} $\pi_i = f(\bm{x}_i; \beta)$ and [trials]{.blue} $n_i$. For large values of $n_i$, the [Gaussian approximation]{.orange} holds 
$$
Y_i \,\dot{\sim}\, \text{N}(n_i \pi_i, n_i\pi_i(1 - \pi_i)).
$$
However, the data are  [heteroschedastic]{.orange}, because $\text{var}(Y_i) = n_i \pi_i(1- \pi_i)$.

- Thus, a [variance stabilizing]{.blue} transformation in this case is 
$$
g_{n_i}(y) = \sqrt{n_i}\arcsin\left(\frac{2 y}{n_i} - 1\right),
$$
because in fact we have that
$$
\text{var}(g_{n_i}(Y_i)) \approx \left(\frac{\sqrt{n_i}}{\sqrt{1 - (2\pi_i-1)^2}} \frac{2}{n_i}\right)^2 n_i \pi_i(1- \pi_i) = 1.
$$

. . .

- If the data are [gamma distributed]{.blue}, the [variance stabilizing]{.orange} transform is $g(y) = \log{y}$. 

## Box-Cox transform

:::callout-note
If the data are $y_i$ are [positive]{.orange}, we may consider a [parametric class]{.blue} of transformations:
$$
g_\lambda(y) = \frac{y^\lambda - 1}{\lambda}, \qquad \lambda \neq 0.
$$
and $g_\lambda(y) = \log{y}$ when $\lambda = 0$. This is the celebrated [Box-Cox transform]{.blue}.

The case $\lambda = 1$ corresponds to no transformation, $\lambda= 1/2$ to the square root, $\lambda = 0$ to the logarithm, and $\lambda= −1$ to the reciprocal.
:::

. . .

- The main idea is to estimate $\lambda$ from the data using [maximum likelihood]{.orange}, so that the data themselves can inform us about the best transformation. We assume
$$
g_\lambda(Y_i) = \bm{x}_i^T\beta + \epsilon_i, \qquad \epsilon_i \sim \text{N}(0, \sigma^2), \qquad i=1,\dots,n.
$$

- The aim of the transformation is to produce a response for which the [variance]{.blue} of $\epsilon_i$ is [constant]{.blue} with an [approximately normal]{.orange} distribution. 

## Profile likelihood for $\lambda$: derivation I

- The distribution of the [transformed data]{.blue} $\bm{Y}(\lambda) = (g_\lambda(Y_1), \dots,g_\lambda(Y_n))^T$ is Gaussian:
$$
f(\bm{y}(\lambda)) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left\{-\frac{1}{2\sigma^2}(\bm{y}(\lambda) - \bm{X}\beta)^T(\bm{y}(\lambda) - \bm{X}\beta)\right\}.
$$

- The density of the [original data]{.orange} is $f_Y(\bm{y}) = f(\bm{y}(\lambda))\prod_{i=1}^ny_i^{\lambda - 1}$ after accounting for the [Jacobian]{.orange} of the transformation. 

. . .

- The [log-likelihood]{.blue} therefore is
$$
\ell(\beta, \sigma^2, \lambda) = -\frac{n}{2}\log{\sigma^2} - \frac{1}{2\sigma^2}(\bm{y}(\lambda) - \bm{X}\beta)^T(\bm{y}(\lambda) - \bm{X}\beta) + (\lambda - 1)\sum_{i=1}^n\log{y_i}.
$$

- Note that, for any given value of $\lambda$, the maximum likelihood estimates are 
$$
\hat{\beta}(\lambda) = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}(\lambda), \qquad \hat{\sigma}^2(\lambda) = \frac{1}{n}(\bm{y}(\lambda) - \bm{X}\hat{\beta}(\lambda))^T(\bm{y}(\lambda) - \bm{X}\hat{\beta}(\lambda)),
$$

## Profile likelihood for $\lambda$: derivation II


- The [profile log-likelihood]{.blue} for $\lambda$ admits a very simple expression, namely
$$
\ell_P(\lambda) = \ell(\hat{\beta}(\lambda), \hat{\sigma}^2(\lambda), \lambda) = -\frac{n}{2}\log{\hat{\sigma}^2(\lambda)} + (\lambda -1)\sum_{i=1}^n\log{y_i},
$$
which must be [numerically maximized]{.orange} over $\lambda$, e.g. using `optim`.

- The optimal value $\hat{\lambda} = \arg\max\ell_P(\lambda)$, as well as a confidence interval for it, may offer guidance in choosing the right transformation.

. . .

- Note that Box and Cox themselves suggested using this approach as an [exploratory tool]{.blue}, rather than a formal inferential procedure.

- For instance,the optimal value $\hat{\lambda} = 0.4210283$ may be [hard to interpret]{.orange} but it could be an indication that a square root transformation is appropriate.

## Box-Cox transform for the auto dataset

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 7.8
#| fig-height: 6.55
m_box <- lm(city.distance ~ engine.size + curb.weight + fuel + cylinders2, data = auto)
MASS::boxcox(m_box)
```

- The [Box-Cox transform]{.blue} in the auto dataset suggests a [reciprocal]{.orange} transformation:
$$
\frac{1}{Y_i} = \beta_1 + \beta_2 x_i +  \beta_3 w_i + \beta_4 I(z_i = \texttt{gas}) + \beta_5 I(v_i = 2) + \epsilon_i,
$$
which is a good alternative to our model based on logarithms of $y_i, x_i$, and $w_i$ ([but]{.orange}...). 

## A fourth model: graphical diagnostics

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
m_box <- lm(I(1/city.distance) ~ engine.size + curb.weight + fuel + cylinders2, data = auto)

augmented_m_box <- augment(m_box, data = auto)
ggplot(data = augmented_m_box, aes(x = .fitted, y = .resid, col = fuel)) +
  geom_point() +
  geom_hline(aes(yintercept = 0), linetype = "dotted") +
  theme_light() +
  theme(legend.position = "right") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Fitted values") +
  ylab("Residuals")
```

## Variable transformation: a caveat

<!-- - Variable transformations are appealing for their simplicity and have a long history in statistics. However, they also have some [drawbacks]{.orange}.  -->

:::callout-tip
In the case of transformations applied only to the [explanatory variables]{.blue}, the model is
$$
Y_i = h_1(\bm{x}_i)\beta_1 + \cdots + h_p(\bm{x}_i)\beta_p + \epsilon_i, \qquad i=1,\dots,n,
$$
Thus, the coefficients can [no longer]{.orange} be [interpreted]{.orange} as the change in the mean of $Y_i$ corresponding to a [one-unit increase]{.blue} in $\tilde{\bm{x}}_j$, independently of the starting level.
:::


:::callout-tip
In the case of transformations of the [response]{.orange} variable we let $\mathbb{E}(g(Y_i)) = \bm{x}_i^T\beta$. However:  
$$
   E(g(Y_i)) \neq g(\mathbb{E}(Y_i)) \quad \Longrightarrow \quad \mathbb{E}(Y_i) \neq g^{-1}(\bm{x}_i^T\beta).
$$
Thus $\hat{y}_i = g^{-1}(\bm{x}_i^T\hat{\beta})$ is a [reasonable prediction]{.blue} for $Y_i$ and not an estimate for its mean.

When $g(y) = \log{y}$ this difference can be made explicit, because we have 
$$
g^{-1}(\mathbb{E}\{g(Y_i)\}) = g^{-1}(\bm{x}_i^T\beta) = \exp(\bm{x}_i^T\beta), \qquad \mathbb{E}(Y_i) = \exp(\bm{x}_i^T\beta + \sigma^2/2),
$$
the former being the [geometric mean]{.orange} of $Y_i$, whereas the latter is the usual [mean]{.blue}.
:::

# Robustness of the OLS estimator

## Non-normality of the errors I

- Let us consider the case in which assumptions **(A.1)**-**(A.2)** are [valid]{.blue} but **(A.3)** [is not]{.orange}, that is $\mathbb{E}(\bm{\epsilon}) = 0$ and $\text{var}(\bm{\epsilon}) = \sigma^2 I_n$, but $\epsilon$ does [not]{.orange} follow [a Gaussian]{.orange} distribution. 

- For example, $\epsilon_i$ may follow a Laplace distribution, a skew-Normal, a logistic distribution, a Student's t distribution, etc. 

. . .

- The OLS estimate $\hat{\beta}$ is [not]{.orange} anymore the [maximum likelihood]{.orange} estimator, but it [preserves]{.blue} most of its [properties]{.blue} and a [geometric interpretation]{.blue}. 

- In particular, even without normality of the errors **(A.3)** we obtain the usual formulas:
$$
\mathbb{E}(\hat{\beta}) = \beta, \qquad \text{var}(\hat{\beta}) = \sigma^2(\bm{X}^T\bm{X})^{-1}.
$$

. . .

- Moreover, the OLS $\hat{\beta}$ remains the most [efficient]{.orange} estimator within the class of linear and unbiased estimators ([BLUE]{.blue}) for any distribution of the errors $\bm{\epsilon}$. 

- In fact, the proof of the [Gauss-Markov theorem]{.blue} requires **(A.1)**-**(A.2)** but not **(A.3)**.


## Non-normality of the errors II

- When the errors are non Gaussian the [exact inferential results]{.blue} are not valid. In particular $\hat{\beta}$ does not follow anymore a Gaussian distribution.

- However, a [central limit theorem]{.orange} can be invoked under very mild conditions on the design matrix $\bm{X}$.

- Thus, when the sample size $n$ is large enough, then the following [approximation]{.orange} holds
$$
\hat{\beta} \:\dot{\sim}\: \text{N}_p(\beta, \sigma^2(\bm{X}^T\bm{X})^{-1}),
$$
from which [confidence intervals]{.blue} and [test statistics]{.blue} can be obtained as usual. The approximation is [excellent]{.orange} if the errors are [symmetric]{.blue} around $0$. 

:::callout-tip
[Non-normality]{.blue} of the errors is [not a major concern]{.orange}: the OLS estimator preserves most of its properties, including approximate normality for sufficiently large $n$.

There is often an [over-emphasis]{.orange} on testing whether the residuals are Gaussian. However, even if normality is rejected, the practical implications are minimal.
:::

## Heteroschedasticity of the errors



# Weighted least squares

## A final comment

