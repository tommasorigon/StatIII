---
title: "Exercises A"
subtitle: "Statistics III - CdL SSE"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
page-layout: full
bibliography: ../biblio.bib
citeproc: true
csl: https://www.zotero.org/styles/journal-of-the-american-statistical-association
reference-location: margin
execute:
  cache: false
format:
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [simplex, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 150
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

# [Homepage](../index.html)

The theoretical exercises described below are quite [difficult]{.orange}. At the [exam]{.blue}, you can expect a [simplified version]{.blue} of them; otherwise, they would represent a formidable challenge for most of you.

On the other hand, the data analyses are more or less aligned with what you may encounter in the final examination.

:::callout-tip
The vast majority of these exercises are taken from the textbooks @Salvan2020 and @Agresti2015, possibly with a few minor modifications. You can consult these textbooks if you need additional exercises.
:::


## Data analysis

:::{.callout-note collapse=true}
#### `Abrasion` dataset

The dataset `Abrasion` (Hand et al., 1994), available in the `MLGdata` R package, comes from an experiment designed to evaluate how the abrasion resistance of a certain type of rubber is affected by its hardness and tensile strength. Thirty rubber samples were subjected to constant abrasion for a fixed duration, and the resulting weight loss (`perdita`, in grams per hour) was recorded. For each sample, the following variables were also measured:  

- `D`: hardness (in Shore degrees),  
- `Re`: tensile strength (kg/cm²).  

(a) Identify the statistical units, the response variable, and the explanatory variables, specifying the type of each variable (continuous quantitative, discrete quantitative, nominal qualitative, ordinal qualitative).

(b) Perform a graphical analysis to assess whether a linear regression model may be appropriate.

(c) Write the main equation of a [simple linear regression]{.blue} model, choosing the [most correlated]{.orange} explanatory variable.

(d) Fit the simple linear regression model specified in point (c) in **R**.

(e) Report the estimates and 95% confidence intervals for the coefficients. Interpret the estimated values.

(f) For each element of the R  `summary` output of the `lm` object, indicate what quantity is being calculated and match it to the formulas in the slides/textbooks.

(g) Evaluate the goodness of fit of the model from (c). In particular, check if the variable omitted in the model specification is correlated with the residuals of the estimated model; comment the results. See also [Exercise D]{.orange} (below) for a refined and alternative diagnostic plot. 

(h) Fit a multiple regression model in **R** including [both explanatory variables]{.blue}. Interpret the estimated coefficients.

(i) Evaluate the goodness of fit of the multiple regression model, using all the diagnostic tools at your disposal.

(j) Obtain a 95% [confidence interval]{.orange} for the mean weight loss when hardness is 70 Shore degrees and tensile strength is 180 kg/cm². For the same values, compute a 95% [prediction interval]{.blue} for the response.
:::

::: {.callout-note collapse=true}
#### `FEV` dataset

Littell et al. (2000) reported a pharmaceutical clinical trial involving $n = 72$ patients, randomly assigned to three treatment groups (drug A, drug B, and placebo), with 24 patients per group. The outcome of interest was respiratory function, measured as FEV1 (forced expiratory volume in one second, in liters). The dataset `FEV.dat` is [available here](../data/FEV.dat); the first six observations are shown below.

In this analysis, let $y$ denote the FEV1 value after one hour of treatment (variable `fev1` in the dataset). Define 

- $x_1$: baseline FEV1 measurement prior to treatment (`base`),  
- $x_2$: treatment group (categorical variable `drug` with labels `a`, `b`, `p`).  

[Download the data](../data/FEV.dat) and then:

(a) Fit linear models for $y$ using the following sets of predictors: (i) $x_1$ only, (ii) $x_2$ only, (iii) both $x_1$ and $x_2$

(b) Test to see whether the interaction terms are needed. 

(c) Check the underlying assumptions of the fitted models. If they are violated, propose a solution. 

(d) For each model, [interpret]{.blue} the estimated parameters. 


```{r}
FAV <- read.table("../data/FEV.dat", header = TRUE)
knitr::kable(head(FAV))
```

:::


:::{.callout-note collapse=true}
#### `Anorexia` dataset

For $n = 72$ young girls diagnosed with anorexia, the dataset `Anorexia.dat` [available here](../data/Anorexia.dat) records their body weights (in lbs) before and after an experimental treatment period. The data format is illustrated below. During the study, the participants were randomly assigned to one of three `therapy` groups:  

- Control group: received the standard therapy (label `c`),  
- Family therapy (label `f`),  
- Cognitive behavioral therapy (label `b`).  

[Download the data](../data/Anorexia.dat) and 

(a) Fit a linear model where the outcome is the [post-treatment weight]{.blue} and the predictors are the [baseline weight]{.orange} and the [therapy group]{.orange}

(b) Check the underlying assumptions of the fitted models. If they are violated, propose a solution. 

(c) [Interpret]{.blue} the estimated parameters of the model. 

(d) Convert the weights to kilograms ($1\ \text{lb} = 0.453592\ \text{kg}$), repeat the analysis, and discuss whether the results change and why.  

```{r}
Anorexia <- read.table("../data/Anorexia.dat", header = TRUE)
knitr::kable(head(Anorexia))
```

:::

## Theoretical

:::{.callout-note collapse=true}
#### Exercise A

Prove that the variance-stabilizing transform for gamma random variables $Y_i\sim \text{Gamma}(\alpha, \beta_i)$ with mean $\mu_i = \alpha/\beta_i$ and variance $\alpha/\beta_i^2$, is the logarithm $g(Y_i) = \log(Y_i)$. 

[Hint]{.blue}: use the fact that if $Y_i \sim \text{Gamma}(\alpha, \beta_i)$ with mean $\mu_i = \alpha/\beta_i$ and variance $\alpha/\beta_i^2$, then for large values of $\alpha$ we have the approximation $Y_i \; \dot{\sim}\;\text{N}(\alpha/\beta_i, \alpha/\beta_i^2)$.
:::

:::{.callout-note collapse=true}
#### Exercise B

In the linear model $\mathbb{E}(Y_i) = \beta_1 + \beta_2 x_i$, suppose that instead of observing $x_i$ we observe $x_i^* = x_i + u_i$, where $u_i$ is independent of $x_i$ for all $i$, with $\mathbb{E}(u_i) = \mu_u$ and $\text{var}(u_i) = \sigma^2_u$. Analyze the expected impact of this [measurement error]{.orange} on $\hat{\beta}_1$ and the residuals $\bm{r}$. What does it happen if $\mu_u = 0$?

:::

:::{.callout-note collapse=true}
#### Exercise C

In some applications, such as regressing annual income on the number of years of education, the variance of $Y_i$ tends to be larger at higher values of $x_i > 0$. Consider the model $\mathbb{E}(Y_i) = \beta_1 x_i$, assuming $\text{var}(Y_i) = \sigma^2 x_i$ for unknown $\sigma^2$. 

(a) Show that the weighted least squares estimator minimizes $\sum_{i=1}^n(y_i - \beta x_i)^2/x_i$ (i.e., giving more weight to observation with smaller $x_i$) and has $\hat{\beta}_\text{wls} = \bar{y}/\bar{x}$, with $\text{var}(\hat{\beta}_\text{wls}) = \sigma^2 / \sum_{i=1}^n x_i$.

(b) Show that the ordinary least squares estimator is $\hat{\beta} = (\sum_{i=1}^n x_i y_i) / \sum_{i=1}^n x_i^2$ and has $\text{var}(\hat{\beta}) = \sigma^2 (\sum_{i=1}^n x_i^3) /(\sum_{i=1}^n x_i^2)^2$. 

(c) Show that $\text{var}(\hat{\beta}) \ge \text{var}(\hat{\beta}_\text{wls})$. 

[Hint]{.blue}: part (c) is not easy, but it can be solved in two ways: (i) by applying the Cauchy–Schwarz inequality to a suitable transformation of the $x_i$, or (ii) by relying on the Gauss–Markov theorem.
:::

:::{.callout-note collapse=true}
#### Exercise D

Suppose the normal linear model $\bm{Y} = \bm{X}\beta + z\gamma + \bm{\epsilon}$,  with $\bm{X}$ an $n \times p$ matrix (rank $p<n$), $\bm{z}$ an $n \times 1$ vector, and $\bm{\epsilon} \sim \text{N}_n(0,\sigma^2 I_n)$. We instead fit $\bm{Y} = \bm{X}\beta + \bm{\varepsilon}$, with 
$$
\hat{\beta} = (\bm{X}^T \bm{X})^{-1}\bm{X}^T \bm{y}, \qquad \bm{r} = (I_n - \bm{H})\bm{y}.
$$

(a) Show that $$
\bm{r} = (I_n - \bm{H})\bm{z}\gamma + (I_n - \bm{H})\bm{\epsilon}$$ 
so $\mathbb{E}(\bm{r}) = (I_n - \bm{H})\bm{z}\gamma$. What if $\bm{z}$ lies in the column space of $\bm{X}$? What if $\bm{z}$ is orthogonal to the column space of $\bm{X}$?

(b) Explain why the [added-variable plot]{.blue} (residuals from regressing $\bm{z}$ on $\bm{X}$ vs. residuals $\bm{r}$) helps assess whether to include $\bm{z}$.  

(c) Using the plots in the figure below, referring to the same set of data, comment on the results and suggest how to modify the model.

```{r}
#| fig-align: center
#| fig-width: 10
#| fig-height: 5
set.seed(123)
x1 <- runif(100, 0, 1)
x2 <- runif(100, 0, 1)
z <- 3 * x1 + rnorm(100, 0, 0.2)
y <- 1 + x1 + x2 - 5 * z + rnorm(100, 0, 0.3)

par(mfrow=c(1, 2))
plot(z, residuals(lm(y ~ x1 + x2)), pch = 16)
plot(residuals(lm(z ~ x1 + x2)), residuals(lm(y ~ x1 + x2)), pch = 16)
```
:::

