---
title: "Exercises A"
subtitle: "Statistical Inference - Ph.D. in Economics, Statistics, and Data Science"
author: "[Tommaso Rigon]{.orange}"
institute: "_UniversitÃ  degli Studi di Milano-Bicocca_"
page-layout: full
bibliography: ../biblio.bib
citeproc: true
csl: https://www.zotero.org/styles/journal-of-the-american-statistical-association
reference-location: margin
execute:
  cache: false
format:
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [simplex, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 150
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

# [Homepage](../index.html)

## Theoretical

:::{.callout-note collapse=true}
#### Exercise A - ðŸ¤”ðŸ¤”

Prove that the variance-stabilizing transform for gamma random variables $Y_i\sim \text{Gamma}(\alpha, \beta_i)$ with mean $\mu_i = \alpha/\beta_i$ and variance $\alpha/\beta_i^2$, is the logarithm $g(Y_i) = \log(Y_i)$. 

[Hint]{.blue}: use the fact that if $Y_i \sim \text{Gamma}(\alpha, \beta_i)$ with mean $\mu_i = \alpha/\beta_i$ and variance $\alpha/\beta_i^2$, then for large values of $\alpha$ we have the approximation $Y_i \; \dot{\sim}\;\text{N}(\alpha/\beta_i, \alpha/\beta_i^2)$.
:::

:::{.callout-note collapse=true}
#### Exercise B - ðŸ¤”ðŸ¤”

In the linear model $\mathbb{E}(Y_i) = \beta_1 + \beta_2 x_i$, suppose that instead of observing $x_i$ we observe $x_i^* = x_i + u_i$, where $u_i$ is independent of $x_i$ for all $i$ and $\text{var}(u_i) = \sigma^2_u$. Analyze the expected impact of this [measurement error]{.orange} on $\hat{\beta}_1$ and the residuals $\bm{r}$.

:::

:::{.callout-note collapse=true}
#### Exercise C - ðŸ¤”ðŸ¤”ðŸ¤”

In some applications, such as regressing annual income on the number of years of education, the variance of $Y_i$ tends to be larger at higher values of $x_i > 0$. Consider the model $\mathbb{E}(Y_i) = \beta_1 x_i$, assuming $\text{var}(Y_i) = \sigma^2 x_i$ for unknown $\sigma^2$. 

(a) Show that the weighted least squares estimator minimizes $\sum_{i=1}^n(y_i - \beta x_i)^2/x_i$ (i.e., giving more weight to observation with smaller $x_i$) and has $\hat{\beta}_\text{wls} = \bar{y}/\bar{x}$, with $\text{var}(\hat{\beta}_\text{wls}) = \sigma^2 / \sum_{i=1}^n x_i$.

(b) Show that the ordinary least squares estimator is $\hat{\beta} = (\sum_{i=1}^n x_i y_i) / \sum_{i=1}^n x_i^2$ and has $\text{var}(\hat{\beta}) = \sigma^2 (\sum_{i=1}^n x_i^3) /(\sum_{i=1}^n x_i^2)^2$. 

(c) Show that $\text{var}(\hat{\beta}) \ge \text{var}(\hat{\beta}_\text{wls})$. 
:::

:::{.callout-note collapse=true}
#### Exercise D - ðŸ¤”ðŸ¤”

Suppose the normal linear model $\bm{Y} = \bm{X}\beta + z\gamma + \bm{\epsilon}$,  with $\bm{X}$ an $n \times p$ matrix (rank $p<n$), $\bm{z}$ an $n \times 1$ vector, and $\bm{\epsilon} \sim \text{N}_n(0,\sigma^2 I_n)$. We instead fit $\bm{Y} = \bm{X}\beta + \bm{\varepsilon}$, with 
$$
\hat{\beta} = (\bm{X}^T \bm{X})^{-1}\bm{X}^T \bm{y}, \qquad \bm{r} = (I_n - \bm{H})\bm{y}.
$$

(a) Show that $$
\bm{r} = (I_n - \bm{H})\bm{z}\gamma + (I_n - \bm{H})\bm{\epsilon}$$ 
so $\mathbb{E}(\bm{r}) = (I_n - \bm{H})\bm{z}\gamma$. What if $\bm{z}$ lies in the column space of $\bm{X}$?  

(b) Explain why the [added-variable plot]{.blue} (residuals $\bm{r}$ vs. residuals from regressing $\bm{z}$ on $\bm{X}$) helps assess whether to include $\bm{z}$.  

(c) Using the plots in the figure below, referring to the same set of data, comment on the results and suggest how to modify the model.

```{r}
set.seed(123)
x1 <- runif(100, 0, 1)
x2 <- runif(100, 0, 1)
z <- 3 * x1 + rnorm(100, 0, 0.2)
y <- 1 + x1 + x2 - 5 * z + rnorm(100, 0, 0.3)

plot(z, residuals(lm(y ~ x1 + x2)), pch = 16)
plot(residuals(lm(z ~ x1 + x2)), residuals(lm(y ~ x1 + x2)), pch = 16)
```
:::


## Data analysis

:::{.callout-note collapse=true}
#### `Abrasion` dataset

The dataset `Abrasion` (Hand et al., 1994), available in the `MLGdata` R package, comes from an experiment designed to evaluate how the abrasion resistance of a certain type of rubber is affected by its hardness and tensile strength. Thirty rubber samples were subjected to constant abrasion for a fixed duration, and the resulting weight loss (`perdita`, in grams per hour) was recorded. For each sample, the following variables were also measured:  

- `D`: hardness (in Shore degrees),  
- `Re`: tensile strength (kg/cmÂ²).  

(a) Identify the statistical units, the response variable, and the explanatory variables, specifying the type of each variable (continuous quantitative, discrete quantitative, nominal qualitative, ordinal qualitative).

(b) Perform a graphical analysis to assess whether a linear regression model may be appropriate.

(c) Specify a simple linear regression model, choosing an appropriate explanatory variable.

(d) Fit the simple linear regression model from (c) in **R**.

(e) Report the estimates and 95% confidence intervals for the coefficients. Interpret the estimated values.

(f) For each element of the R  `summary` output of the `lm` object, indicate what quantity is being calculated and match it to the formulas in the slides/textbooks.

(g) Evaluate the goodness of fit of the model.

(h) Fit a multiple regression model in **R** including both explanatory variables. Interpret the estimated coefficients.

(i) Evaluate the goodness of fit of the multiple regression model.

(j) Obtain a 95% confidence interval for the mean weight loss when hardness is 70 Shore degrees and tensile strength is 180 kg/cmÂ². For the same values, compute a 95% prediction interval for the response.
:::

::: {.callout-note collapse=true}
#### `FEV` dataset

Littell et al. (2000) reported a pharmaceutical clinical trial involving $n = 72$ patients, randomly assigned to three treatment groups (drug A, drug B, and placebo), with 24 patients per group. The outcome of interest was respiratory function, measured as FEV1 (forced expiratory volume in one second, in liters). The dataset `FEV.dat` is [available here](../data/FEV.dat); the first six observations are shown below.

In this analysis, let $y$ denote the FEV1 value after one hour of treatment (variable `fev1` in the dataset). Define 

- $x_1$: baseline FEV1 measurement prior to treatment (`base`),  
- $x_2$: treatment group (categorical variable `drug` with labels `a`, `b`, `p`).  

[Download the data](../data/FEV.dat) and then:

(a) Fit linear models for $y$ using the following sets of predictors: (i) $x_1$ only, (ii) $x_2$ only, (iii) both $x_1$ and $x_2$

(b) Test to see whether the interaction terms are needed. 

(c) Check the underlying assumptions of the fitted models. If they are violated, propose a solution. 

(d) For each model, [interpret]{.blue} the estimated parameters. 


```{r}
FAV <- read.table("../data/FEV.dat", header = TRUE)
knitr::kable(head(FAV))
```

:::


:::{.callout-note collapse=true}
#### `Anorexia` dataset

For $n = 72$ young girls diagnosed with anorexia, the dataset `Anorexia.dat` [available here](../data/Anorexia.dat) records their body weights (in lbs) before and after an experimental treatment period. The data format is illustrated below. During the study, the participants were randomly assigned to one of three `therapy` groups:  

- Control group: received the standard therapy (label `c`),  
- Family therapy (label `f`),  
- Cognitive behavioral therapy (label `b`).  

[Download the data](../data/Anorexia.dat) and 

(a) Fit a linear model where the outcome is the [post-treatment weight]{.blue} and the predictors are the [baseline weight]{.orange} and the [therapy group]{.orange}

(b) Check the underlying assumptions of the fitted models. If they are violated, propose a solution. 

(c) [Interpret]{.blue} the estimated parameters of the model. 

(d) Convert the weights to kilograms ($1\ \text{lb} = 0.453592\ \text{kg}$), repeat the analysis, and discuss whether the results change and why.  

```{r}
Anorexia <- read.table("../data/Anorexia.dat", header = TRUE)
knitr::kable(head(Anorexia))
```

:::